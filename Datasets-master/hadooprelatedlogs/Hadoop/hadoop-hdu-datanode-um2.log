2019-08-01 15:07:15,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-01 15:07:15,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-01 15:07:15,692 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 15:07:16,303 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-01 15:07:16,461 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-01 15:07:16,461 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-01 15:07:16,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-01 15:07:16,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-01 15:07:16,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-01 15:07:16,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-01 15:07:16,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-01 15:07:16,617 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-01 15:07:16,625 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-01 15:07:16,642 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-01 15:07:16,649 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-01 15:07:16,650 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-01 15:07:16,650 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-01 15:07:16,667 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-01 15:07:16,674 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-01 15:07:16,675 INFO org.mortbay.log: jetty-6.1.26
2019-08-01 15:07:17,060 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-01 15:07:17,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-01 15:07:17,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-01 15:07:17,605 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-01 15:07:17,623 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-01 15:07:17,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-01 15:07:17,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-01 15:07:17,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-01 15:07:17,707 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 15:07:17,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-01 15:07:17,721 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-01 15:07:17,728 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-01 15:07:18,009 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 5852@um2
2019-08-01 15:07:18,010 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /abc/datanode is not formatted for BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:07:18,010 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-01 15:07:18,085 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:07:18,085 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:07:18,086 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252 is not formatted for BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:07:18,086 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-01 15:07:18,086 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1619971864-192.168.56.101-1564664778252 directory /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current
2019-08-01 15:07:18,087 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-01 15:07:18,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=null
2019-08-01 15:07:18,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-01 15:07:18,101 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 15:07:18,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-01 15:07:18,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-01 15:07:18,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-01 15:07:18,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:07:18,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 15:07:18,166 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 17ms
2019-08-01 15:07:18,166 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 18ms
2019-08-01 15:07:18,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 15:07:18,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 0ms
2019-08-01 15:07:18,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-08-01 15:07:18,170 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564682505170 with interval 21600000
2019-08-01 15:07:18,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-01 15:07:18,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-01 15:07:18,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-01 15:07:18,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=1
2019-08-01 15:07:18,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-01 15:07:18,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdbcbc19719d,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 27 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-01 15:07:18,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:07:18,302 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-01 15:07:18,302 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-01 15:07:18,306 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-01 15:07:18,306 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-01 15:07:18,306 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:07:18,315 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-01 15:09:45,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741825_1001 src: /192.168.56.101:51970 dest: /192.168.56.102:50010
2019-08-01 15:09:45,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51970, dest: /192.168.56.102:50010, bytes: 6162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1476027984_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741825_1001, duration: 87889478
2019-08-01 15:09:45,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:09:53,234 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741825_1001
2019-08-01 15:10:09,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741826_1002 src: /192.168.56.101:51978 dest: /192.168.56.102:50010
2019-08-01 15:10:09,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51978, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_993062177_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741826_1002, duration: 40864049
2019-08-01 15:10:09,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:10:09,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741827_1003 src: /192.168.56.101:51982 dest: /192.168.56.102:50010
2019-08-01 15:10:09,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51982, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_993062177_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741827_1003, duration: 5113188
2019-08-01 15:10:09,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:10:09,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741828_1004 src: /192.168.56.101:51986 dest: /192.168.56.102:50010
2019-08-01 15:10:09,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51986, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_993062177_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741828_1004, duration: 7671229
2019-08-01 15:10:09,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:10:09,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741829_1005 src: /192.168.56.101:51990 dest: /192.168.56.102:50010
2019-08-01 15:10:09,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51990, dest: /192.168.56.102:50010, bytes: 88714, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_993062177_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741829_1005, duration: 13164944
2019-08-01 15:10:09,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:10:18,267 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741827_1003
2019-08-01 15:10:18,287 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741829_1005
2019-08-01 15:10:18,295 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741826_1002
2019-08-01 15:10:18,296 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741828_1004
2019-08-01 15:13:45,280 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-01 15:13:49,089 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-01 15:13:49,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-01 15:15:41,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-01 15:15:41,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-01 15:15:42,061 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 15:15:42,632 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-01 15:15:42,787 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-01 15:15:42,787 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-01 15:15:42,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-01 15:15:42,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-01 15:15:42,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-01 15:15:42,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-01 15:15:42,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-01 15:15:42,931 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-01 15:15:42,946 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-01 15:15:42,958 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-01 15:15:42,960 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-01 15:15:42,960 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-01 15:15:42,960 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-01 15:15:42,987 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-01 15:15:42,993 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-01 15:15:42,994 INFO org.mortbay.log: jetty-6.1.26
2019-08-01 15:15:43,405 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-01 15:15:43,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-01 15:15:43,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-01 15:15:43,931 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-01 15:15:43,953 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-01 15:15:43,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-01 15:15:43,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-01 15:15:44,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-01 15:15:44,012 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 15:15:44,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-01 15:15:44,021 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-01 15:15:44,029 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-01 15:15:44,277 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 6493@um2
2019-08-01 15:15:44,421 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:15:44,421 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:15:44,421 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-01 15:15:44,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-01 15:15:44,433 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 15:15:44,462 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-01 15:15:44,463 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-01 15:15:44,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-01 15:15:44,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:15:44,539 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 15:15:44,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 415367
2019-08-01 15:15:44,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 59ms
2019-08-01 15:15:44,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 59ms
2019-08-01 15:15:44,601 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 15:15:44,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 3ms
2019-08-01 15:15:44,604 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2019-08-01 15:15:44,609 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564679847609 with interval 21600000
2019-08-01 15:15:44,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-01 15:15:44,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-01 15:15:44,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-01 15:15:44,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=44
2019-08-01 15:15:44,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-01 15:15:44,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe32a75253e4,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 1 msec to generate and 66 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-01 15:15:44,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:15:44,787 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-01 15:15:44,787 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-01 15:15:44,793 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-01 15:15:44,793 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-01 15:15:44,793 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 15:15:44,801 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-01 15:16:23,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741830_1006 src: /192.168.56.101:52046 dest: /192.168.56.102:50010
2019-08-01 15:16:23,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52046, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-304989738_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741830_1006, duration: 122804603
2019-08-01 15:16:23,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:16:23,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741831_1007 src: /192.168.56.101:52050 dest: /192.168.56.102:50010
2019-08-01 15:16:23,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52050, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-304989738_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741831_1007, duration: 6565405
2019-08-01 15:16:23,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:16:23,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741832_1008 src: /192.168.56.101:52054 dest: /192.168.56.102:50010
2019-08-01 15:16:23,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52054, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-304989738_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741832_1008, duration: 7361954
2019-08-01 15:16:23,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:16:24,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741833_1009 src: /192.168.56.101:52058 dest: /192.168.56.102:50010
2019-08-01 15:16:24,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52058, dest: /192.168.56.102:50010, bytes: 88718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-304989738_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741833_1009, duration: 11747435
2019-08-01 15:16:24,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 15:16:29,673 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741833_1009
2019-08-01 15:16:29,681 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741830_1006
2019-08-01 15:16:29,685 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741832_1008
2019-08-01 15:16:29,687 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741831_1007
2019-08-01 15:22:32,725 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-01 15:22:36,710 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-01 15:22:36,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-01 19:51:41,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-01 19:51:42,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-01 19:51:42,545 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 19:51:43,136 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-01 19:51:43,306 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-01 19:51:43,306 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-01 19:51:43,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-01 19:51:43,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-01 19:51:43,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-01 19:51:43,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-01 19:51:43,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-01 19:51:43,469 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-01 19:51:43,476 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-01 19:51:43,494 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-01 19:51:43,500 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-01 19:51:43,500 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-01 19:51:43,501 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-01 19:51:43,522 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-01 19:51:43,524 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-01 19:51:43,524 INFO org.mortbay.log: jetty-6.1.26
2019-08-01 19:51:43,836 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-01 19:51:44,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-01 19:51:44,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-01 19:51:44,508 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-01 19:51:44,541 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-01 19:51:44,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-01 19:51:44,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-01 19:51:44,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-01 19:51:44,657 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 19:51:44,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-01 19:51:44,691 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-01 19:51:44,694 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-01 19:51:45,035 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 2351@um2
2019-08-01 19:51:45,150 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:51:45,151 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:51:45,152 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-01 19:51:45,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-01 19:51:45,175 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 19:51:45,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-01 19:51:45,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-01 19:51:45,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-01 19:51:45,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:51:45,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 19:51:45,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 37ms
2019-08-01 19:51:45,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 40ms
2019-08-01 19:51:45,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 19:51:45,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 8ms
2019-08-01 19:51:45,364 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 9ms
2019-08-01 19:51:45,369 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564703233369 with interval 21600000
2019-08-01 19:51:45,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-01 19:51:45,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-01 19:51:45,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-01 19:51:45,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=75
2019-08-01 19:51:45,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-01 19:51:45,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x14f7057ae3,  containing 1 storage report(s), of which we sent 1. The reports had 9 total blocks and used 1 RPC(s). This took 1 msec to generate and 26 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-01 19:51:45,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:51:45,507 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-01 19:51:45,507 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-01 19:51:45,509 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-01 19:51:45,509 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-01 19:51:45,509 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:51:45,520 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-01 19:53:34,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741834_1010 src: /192.168.56.101:51050 dest: /192.168.56.102:50010
2019-08-01 19:53:34,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51050, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2058158447_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741834_1010, duration: 94563280
2019-08-01 19:53:35,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:53:35,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741835_1011 src: /192.168.56.101:51054 dest: /192.168.56.102:50010
2019-08-01 19:53:35,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51054, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2058158447_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741835_1011, duration: 10330901
2019-08-01 19:53:35,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:53:35,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741836_1012 src: /192.168.56.101:51058 dest: /192.168.56.102:50010
2019-08-01 19:53:35,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51058, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2058158447_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741836_1012, duration: 3624237
2019-08-01 19:53:35,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:53:35,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741837_1013 src: /192.168.56.101:51062 dest: /192.168.56.102:50010
2019-08-01 19:53:35,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51062, dest: /192.168.56.102:50010, bytes: 88718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2058158447_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741837_1013, duration: 13663815
2019-08-01 19:53:35,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:56:33,816 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "um2/192.168.56.102"; destination host is: "um1":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-01 19:56:37,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-01 19:56:38,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-01 19:56:38,836 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-01 19:56:38,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-01 19:57:17,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-01 19:57:17,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-01 19:57:18,182 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 19:57:18,708 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-01 19:57:18,830 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-01 19:57:18,830 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-01 19:57:18,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-01 19:57:18,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-01 19:57:18,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-01 19:57:18,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-01 19:57:18,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-01 19:57:18,957 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-01 19:57:18,966 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-01 19:57:18,984 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-01 19:57:18,985 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-01 19:57:18,986 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-01 19:57:18,986 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-01 19:57:19,008 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-01 19:57:19,014 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-01 19:57:19,014 INFO org.mortbay.log: jetty-6.1.26
2019-08-01 19:57:19,320 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-01 19:57:19,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-01 19:57:19,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-01 19:57:19,781 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-01 19:57:19,804 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-01 19:57:19,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-01 19:57:19,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-01 19:57:19,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-01 19:57:19,899 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 19:57:19,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-01 19:57:19,909 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-01 19:57:19,916 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-01 19:57:20,138 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3304@um2
2019-08-01 19:57:20,220 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:57:20,220 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:57:20,220 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-01 19:57:20,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-01 19:57:20,232 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 19:57:20,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-01 19:57:20,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-01 19:57:20,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-01 19:57:20,329 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:57:20,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 19:57:20,344 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 1257022
2019-08-01 19:57:20,348 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 16ms
2019-08-01 19:57:20,348 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 20ms
2019-08-01 19:57:20,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 19:57:20,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 4ms
2019-08-01 19:57:20,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2019-08-01 19:57:20,357 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564689741357 with interval 21600000
2019-08-01 19:57:20,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-01 19:57:20,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-01 19:57:20,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-01 19:57:20,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=104
2019-08-01 19:57:20,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-01 19:57:20,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x62f417d57d,  containing 1 storage report(s), of which we sent 1. The reports had 13 total blocks and used 1 RPC(s). This took 1 msec to generate and 26 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-01 19:57:20,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:57:20,459 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-01 19:57:20,459 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-01 19:57:20,461 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-01 19:57:20,461 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-01 19:57:20,461 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 19:57:20,467 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-01 19:57:25,537 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741836_1012
2019-08-01 19:58:05,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741838_1014 src: /192.168.56.101:51154 dest: /192.168.56.102:50010
2019-08-01 19:58:05,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51154, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_949241278_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741838_1014, duration: 92824872
2019-08-01 19:58:05,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:06,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741839_1015 src: /192.168.56.101:51158 dest: /192.168.56.102:50010
2019-08-01 19:58:06,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51158, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_949241278_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741839_1015, duration: 8052318
2019-08-01 19:58:06,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:06,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741840_1016 src: /192.168.56.101:51162 dest: /192.168.56.102:50010
2019-08-01 19:58:06,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51162, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_949241278_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741840_1016, duration: 6587240
2019-08-01 19:58:06,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:06,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741841_1017 src: /192.168.56.101:51166 dest: /192.168.56.102:50010
2019-08-01 19:58:06,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51166, dest: /192.168.56.102:50010, bytes: 88595, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_949241278_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741841_1017, duration: 9090945
2019-08-01 19:58:06,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:12,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741842_1018 src: /192.168.56.101:51190 dest: /192.168.56.102:50010
2019-08-01 19:58:12,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51190, dest: /192.168.56.102:50010, bytes: 105357, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_790650281_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741842_1018, duration: 61918043
2019-08-01 19:58:12,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:14,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741843_1019 src: /192.168.56.101:51198 dest: /192.168.56.102:50010
2019-08-01 19:58:21,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51198, dest: /192.168.56.102:50010, bytes: 21649, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_790650281_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741843_1019, duration: 6566816389
2019-08-01 19:58:21,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:21,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741844_1020 src: /192.168.56.101:51226 dest: /192.168.56.102:50010
2019-08-01 19:58:21,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51226, dest: /192.168.56.102:50010, bytes: 331, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_790650281_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741844_1020, duration: 1694784
2019-08-01 19:58:21,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:21,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741845_1021 src: /192.168.56.101:51232 dest: /192.168.56.102:50010
2019-08-01 19:58:21,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51232, dest: /192.168.56.102:50010, bytes: 21649, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_790650281_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741845_1021, duration: 4897717
2019-08-01 19:58:21,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:21,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741846_1022 src: /192.168.56.101:51236 dest: /192.168.56.102:50010
2019-08-01 19:58:21,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51236, dest: /192.168.56.102:50010, bytes: 105357, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_790650281_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741846_1022, duration: 11685124
2019-08-01 19:58:21,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:29,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741847_1023 src: /192.168.56.101:51244 dest: /192.168.56.102:50010
2019-08-01 19:58:29,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51244, dest: /192.168.56.102:50010, bytes: 52353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1331946884_86, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741847_1023, duration: 22618623
2019-08-01 19:58:29,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 19:58:29,484 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2019-08-01 19:58:29,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2019-08-01 19:58:29,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2019-08-01 19:58:29,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2019-08-01 19:58:29,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2019-08-01 19:58:29,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2019-08-01 19:58:29,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741840_1016 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741840
2019-08-01 19:58:29,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741841_1017 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741841
2019-08-01 19:58:29,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741842_1018 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741842
2019-08-01 19:58:29,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741843_1019 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741843
2019-08-01 19:58:29,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741838_1014 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741838
2019-08-01 19:58:29,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741839_1015 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741839
2019-08-01 19:58:50,468 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-01 19:58:54,062 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-01 19:58:54,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-01 20:00:30,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-01 20:00:30,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-01 20:00:31,105 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 20:00:31,632 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-01 20:00:31,749 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-01 20:00:31,749 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-01 20:00:31,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-01 20:00:31,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-01 20:00:31,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-01 20:00:31,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-01 20:00:31,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-01 20:00:31,877 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-01 20:00:31,895 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-01 20:00:31,907 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-01 20:00:31,917 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-01 20:00:31,917 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-01 20:00:31,917 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-01 20:00:31,937 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-01 20:00:31,940 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-01 20:00:31,940 INFO org.mortbay.log: jetty-6.1.26
2019-08-01 20:00:32,323 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-01 20:00:32,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-01 20:00:32,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-01 20:00:32,853 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-01 20:00:32,874 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-01 20:00:32,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-01 20:00:32,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-01 20:00:32,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-01 20:00:32,936 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 20:00:32,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-01 20:00:32,946 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-01 20:00:32,951 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-01 20:00:33,166 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3961@um2
2019-08-01 20:00:33,258 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:00:33,258 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:00:33,259 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-01 20:00:33,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-01 20:00:33,283 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 20:00:33,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-01 20:00:33,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-01 20:00:33,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-01 20:00:33,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:00:33,405 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 20:00:33,417 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 1438152
2019-08-01 20:00:33,424 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 19ms
2019-08-01 20:00:33,424 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 19ms
2019-08-01 20:00:33,425 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 20:00:33,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 6ms
2019-08-01 20:00:33,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2019-08-01 20:00:33,434 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564702044433 with interval 21600000
2019-08-01 20:00:33,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-01 20:00:33,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-01 20:00:33,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-01 20:00:33,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=189
2019-08-01 20:00:33,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-01 20:00:33,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8fe771d154,  containing 1 storage report(s), of which we sent 1. The reports had 17 total blocks and used 1 RPC(s). This took 1 msec to generate and 16 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-01 20:00:33,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:00:33,518 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-01 20:00:33,518 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-01 20:00:33,518 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-01 20:00:33,518 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-01 20:00:33,519 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:00:33,523 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-01 20:00:38,507 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741834_1010
2019-08-01 20:01:40,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741848_1024 src: /192.168.56.101:51346 dest: /192.168.56.102:50010
2019-08-01 20:01:40,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51346, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-387720898_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741848_1024, duration: 80225632
2019-08-01 20:01:40,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:01:41,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741849_1025 src: /192.168.56.101:51350 dest: /192.168.56.102:50010
2019-08-01 20:01:41,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51350, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-387720898_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741849_1025, duration: 16731049
2019-08-01 20:01:41,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:01:41,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741850_1026 src: /192.168.56.101:51354 dest: /192.168.56.102:50010
2019-08-01 20:01:41,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51354, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-387720898_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741850_1026, duration: 6533501
2019-08-01 20:01:41,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:01:41,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741851_1027 src: /192.168.56.101:51358 dest: /192.168.56.102:50010
2019-08-01 20:01:41,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51358, dest: /192.168.56.102:50010, bytes: 88718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-387720898_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741851_1027, duration: 15210889
2019-08-01 20:01:41,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:01:47,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741852_1028 src: /192.168.56.101:51380 dest: /192.168.56.102:50010
2019-08-01 20:01:47,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51380, dest: /192.168.56.102:50010, bytes: 105504, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_985673261_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741852_1028, duration: 29445843
2019-08-01 20:01:47,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:01:55,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741853_1029 src: /192.168.56.101:51390 dest: /192.168.56.102:50010
2019-08-01 20:02:02,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741854_1030 src: /192.168.56.102:58084 dest: /192.168.56.102:50010
2019-08-01 20:02:03,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:58084, dest: /192.168.56.102:50010, bytes: 3005, op: HDFS_WRITE, cliID: DFSClient_attempt_1564682442748_0001_r_000000_0_2113286169_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741854_1030, duration: 57937906
2019-08-01 20:02:03,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741854_1030, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-01 20:02:03,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51390, dest: /192.168.56.102:50010, bytes: 33626, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_985673261_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741853_1029, duration: 7396499697
2019-08-01 20:02:03,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:03,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741855_1031 src: /192.168.56.101:51398 dest: /192.168.56.102:50010
2019-08-01 20:02:03,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51398, dest: /192.168.56.102:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_985673261_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741855_1031, duration: 5372422
2019-08-01 20:02:03,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:03,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741856_1032 src: /192.168.56.101:51404 dest: /192.168.56.102:50010
2019-08-01 20:02:03,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51404, dest: /192.168.56.102:50010, bytes: 33626, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_985673261_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741856_1032, duration: 2981809
2019-08-01 20:02:03,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:03,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741857_1033 src: /192.168.56.101:51408 dest: /192.168.56.102:50010
2019-08-01 20:02:03,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51408, dest: /192.168.56.102:50010, bytes: 105504, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_985673261_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741857_1033, duration: 7707599
2019-08-01 20:02:03,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:09,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2019-08-01 20:02:09,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2019-08-01 20:02:09,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2019-08-01 20:02:09,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2019-08-01 20:02:09,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2019-08-01 20:02:09,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2019-08-01 20:02:09,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741848_1024 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741848
2019-08-01 20:02:09,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741849_1025 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741849
2019-08-01 20:02:09,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741850_1026 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741850
2019-08-01 20:02:09,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741851_1027 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741851
2019-08-01 20:02:09,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741852_1028 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741852
2019-08-01 20:02:09,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741853_1029 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741853
2019-08-01 20:02:10,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741858_1034 src: /192.168.56.102:58090 dest: /192.168.56.102:50010
2019-08-01 20:02:10,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:58090, dest: /192.168.56.102:50010, bytes: 8696, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1574879390_89, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741858_1034, duration: 44044516
2019-08-01 20:02:10,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741858_1034, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-01 20:02:11,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741859_1035 src: /192.168.56.101:51416 dest: /192.168.56.102:50010
2019-08-01 20:02:11,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51416, dest: /192.168.56.102:50010, bytes: 34509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_705465465_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741859_1035, duration: 23600088
2019-08-01 20:02:11,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:29,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741860_1036 src: /192.168.56.101:51424 dest: /192.168.56.102:50010
2019-08-01 20:02:29,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51424, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2135738974_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741860_1036, duration: 38216151
2019-08-01 20:02:29,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:30,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741861_1037 src: /192.168.56.101:51428 dest: /192.168.56.102:50010
2019-08-01 20:02:30,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51428, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2135738974_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741861_1037, duration: 2649977
2019-08-01 20:02:30,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:30,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741862_1038 src: /192.168.56.101:51432 dest: /192.168.56.102:50010
2019-08-01 20:02:30,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51432, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2135738974_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741862_1038, duration: 6967978
2019-08-01 20:02:30,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:30,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741863_1039 src: /192.168.56.101:51436 dest: /192.168.56.102:50010
2019-08-01 20:02:30,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51436, dest: /192.168.56.102:50010, bytes: 88718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2135738974_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741863_1039, duration: 5900210
2019-08-01 20:02:30,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:39,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741864_1040 src: /192.168.56.101:51446 dest: /192.168.56.102:50010
2019-08-01 20:02:39,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51446, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1478316134_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741864_1040, duration: 38607271
2019-08-01 20:02:39,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:39,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741865_1041 src: /192.168.56.101:51450 dest: /192.168.56.102:50010
2019-08-01 20:02:39,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51450, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1478316134_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741865_1041, duration: 4198599
2019-08-01 20:02:39,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:39,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741866_1042 src: /192.168.56.101:51454 dest: /192.168.56.102:50010
2019-08-01 20:02:39,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51454, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1478316134_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741866_1042, duration: 6272440
2019-08-01 20:02:39,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741866_1042, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:39,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741867_1043 src: /192.168.56.101:51458 dest: /192.168.56.102:50010
2019-08-01 20:02:39,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51458, dest: /192.168.56.102:50010, bytes: 88718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1478316134_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741867_1043, duration: 6890976
2019-08-01 20:02:39,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:02:48,549 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741859_1035
2019-08-01 20:04:51,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741868_1044 src: /192.168.56.102:58204 dest: /192.168.56.102:50010
2019-08-01 20:04:51,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:58204, dest: /192.168.56.102:50010, bytes: 18829, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1967051961_89, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741868_1044, duration: 14545064
2019-08-01 20:04:51,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741868_1044, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-01 20:05:13,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741869_1045 src: /192.168.56.102:58246 dest: /192.168.56.102:50010
2019-08-01 20:05:13,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:58246, dest: /192.168.56.102:50010, bytes: 11720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_694037733_89, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741869_1045, duration: 13051816
2019-08-01 20:05:13,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741869_1045, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-01 20:14:03,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-01 20:14:03,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-01 20:14:04,173 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 20:14:04,758 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-01 20:14:04,935 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-01 20:14:04,935 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-01 20:14:04,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-01 20:14:04,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-01 20:14:04,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-01 20:14:04,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-01 20:14:04,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-01 20:14:05,096 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-01 20:14:05,098 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-01 20:14:05,113 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-01 20:14:05,119 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-01 20:14:05,119 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-01 20:14:05,119 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-01 20:14:05,139 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-01 20:14:05,144 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-01 20:14:05,144 INFO org.mortbay.log: jetty-6.1.26
2019-08-01 20:14:05,446 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-01 20:14:05,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-01 20:14:05,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-01 20:14:05,992 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-01 20:14:06,012 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-01 20:14:06,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-01 20:14:06,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-01 20:14:06,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-01 20:14:06,131 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 20:14:06,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-01 20:14:06,163 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-01 20:14:06,165 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-01 20:14:06,540 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 2537@um2
2019-08-01 20:14:06,668 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:14:06,668 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:14:06,669 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-01 20:14:06,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-01 20:14:06,687 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-01 20:14:06,728 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-01 20:14:06,728 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-01 20:14:06,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-01 20:14:06,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:14:06,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 20:14:06,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 32ms
2019-08-01 20:14:06,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 39ms
2019-08-01 20:14:06,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-01 20:14:06,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 11ms
2019-08-01 20:14:06,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2019-08-01 20:14:06,852 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564686774852 with interval 21600000
2019-08-01 20:14:06,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-01 20:14:06,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-01 20:14:06,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-01 20:14:06,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=357
2019-08-01 20:14:06,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-01 20:14:06,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x59894c1e02,  containing 1 storage report(s), of which we sent 1. The reports had 33 total blocks and used 1 RPC(s). This took 1 msec to generate and 32 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-01 20:14:06,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:14:07,005 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-01 20:14:07,005 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-01 20:14:07,008 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-01 20:14:07,008 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-01 20:14:07,008 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:14:07,015 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-01 20:14:11,904 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741858_1034
2019-08-01 20:16:45,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7e8bdf0fc6,  containing 1 storage report(s), of which we sent 1. The reports had 33 total blocks and used 1 RPC(s). This took 0 msec to generate and 7 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-01 20:16:45,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-01 20:17:09,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741870_1046 src: /192.168.56.101:56570 dest: /192.168.56.102:50010
2019-08-01 20:17:09,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56570, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1990929208_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741870_1046, duration: 65540862
2019-08-01 20:17:09,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:10,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741871_1047 src: /192.168.56.101:56574 dest: /192.168.56.102:50010
2019-08-01 20:17:10,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56574, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1990929208_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741871_1047, duration: 3043936
2019-08-01 20:17:10,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:10,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741872_1048 src: /192.168.56.101:56578 dest: /192.168.56.102:50010
2019-08-01 20:17:10,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56578, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1990929208_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741872_1048, duration: 2041243
2019-08-01 20:17:10,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:10,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741873_1049 src: /192.168.56.101:56582 dest: /192.168.56.102:50010
2019-08-01 20:17:10,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56582, dest: /192.168.56.102:50010, bytes: 88718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1990929208_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741873_1049, duration: 7293925
2019-08-01 20:17:10,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:16,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741874_1050 src: /192.168.56.101:56604 dest: /192.168.56.102:50010
2019-08-01 20:17:16,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56604, dest: /192.168.56.102:50010, bytes: 105504, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529454124_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741874_1050, duration: 17018787
2019-08-01 20:17:16,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:21,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741875_1051 src: /192.168.56.101:56620 dest: /192.168.56.102:50010
2019-08-01 20:17:29,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741876_1052 src: /192.168.56.102:48922 dest: /192.168.56.102:50010
2019-08-01 20:17:29,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:48922, dest: /192.168.56.102:50010, bytes: 3005, op: HDFS_WRITE, cliID: DFSClient_attempt_1564683256393_0001_r_000000_0_820567930_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741876_1052, duration: 53003637
2019-08-01 20:17:29,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741876_1052, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-01 20:17:30,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56620, dest: /192.168.56.102:50010, bytes: 33617, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529454124_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741875_1051, duration: 8820861893
2019-08-01 20:17:30,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:30,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741877_1053 src: /192.168.56.101:56636 dest: /192.168.56.102:50010
2019-08-01 20:17:30,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56636, dest: /192.168.56.102:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529454124_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741877_1053, duration: 4115283
2019-08-01 20:17:30,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:30,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741878_1054 src: /192.168.56.101:56642 dest: /192.168.56.102:50010
2019-08-01 20:17:30,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56642, dest: /192.168.56.102:50010, bytes: 33617, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529454124_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741878_1054, duration: 5451923
2019-08-01 20:17:30,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741878_1054, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:30,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741879_1055 src: /192.168.56.101:56646 dest: /192.168.56.102:50010
2019-08-01 20:17:30,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56646, dest: /192.168.56.102:50010, bytes: 105504, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529454124_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741879_1055, duration: 5350689
2019-08-01 20:17:30,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:17:36,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2019-08-01 20:17:36,923 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2019-08-01 20:17:36,923 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2019-08-01 20:17:36,923 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2019-08-01 20:17:36,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2019-08-01 20:17:36,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2019-08-01 20:17:36,932 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741872_1048 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741872
2019-08-01 20:17:36,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741873_1049 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741873
2019-08-01 20:17:36,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741874_1050 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741874
2019-08-01 20:17:36,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741875_1051 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741875
2019-08-01 20:17:36,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741870_1046 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741870
2019-08-01 20:17:36,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741871_1047 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741871
2019-08-01 20:17:37,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741880_1056 src: /192.168.56.102:48928 dest: /192.168.56.102:50010
2019-08-01 20:17:37,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:48928, dest: /192.168.56.102:50010, bytes: 5753, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_764785691_77, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741880_1056, duration: 33565371
2019-08-01 20:17:37,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741880_1056, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-01 20:17:38,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741881_1057 src: /192.168.56.101:56654 dest: /192.168.56.102:50010
2019-08-01 20:17:38,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56654, dest: /192.168.56.102:50010, bytes: 39253, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2087050470_79, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741881_1057, duration: 25913507
2019-08-01 20:17:38,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:18:02,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741882_1058 src: /192.168.56.101:56662 dest: /192.168.56.102:50010
2019-08-01 20:18:02,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56662, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_847553117_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741882_1058, duration: 67729666
2019-08-01 20:18:02,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:18:02,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741883_1059 src: /192.168.56.101:56666 dest: /192.168.56.102:50010
2019-08-01 20:18:02,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56666, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_847553117_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741883_1059, duration: 4317035
2019-08-01 20:18:02,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741883_1059, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:18:02,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741884_1060 src: /192.168.56.101:56670 dest: /192.168.56.102:50010
2019-08-01 20:18:02,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56670, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_847553117_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741884_1060, duration: 3890375
2019-08-01 20:18:02,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:18:02,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741885_1061 src: /192.168.56.101:56674 dest: /192.168.56.102:50010
2019-08-01 20:18:02,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56674, dest: /192.168.56.102:50010, bytes: 88718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_847553117_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741885_1061, duration: 7131852
2019-08-01 20:18:02,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741885_1061, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 20:44:15,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741886_1062 src: /192.168.56.102:49878 dest: /192.168.56.102:50010
2019-08-01 20:44:15,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:49878, dest: /192.168.56.102:50010, bytes: 126884, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-430364486_77, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741886_1062, duration: 5388280
2019-08-01 20:44:15,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741886_1062, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-01 21:12:54,885 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1619971864-192.168.56.101-1564664778252 Total blocks: 44, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-08-01 21:59:38,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741887_1063 src: /192.168.56.101:56720 dest: /192.168.56.102:50010
2019-08-01 21:59:38,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56720, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_253554213_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741887_1063, duration: 38634481
2019-08-01 21:59:38,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:39,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741888_1064 src: /192.168.56.101:56724 dest: /192.168.56.102:50010
2019-08-01 21:59:39,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56724, dest: /192.168.56.102:50010, bytes: 103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_253554213_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741888_1064, duration: 10520595
2019-08-01 21:59:39,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741888_1064, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:39,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741889_1065 src: /192.168.56.101:56728 dest: /192.168.56.102:50010
2019-08-01 21:59:39,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56728, dest: /192.168.56.102:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_253554213_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741889_1065, duration: 2743052
2019-08-01 21:59:39,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741889_1065, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:39,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741890_1066 src: /192.168.56.101:56732 dest: /192.168.56.102:50010
2019-08-01 21:59:39,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56732, dest: /192.168.56.102:50010, bytes: 88718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_253554213_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741890_1066, duration: 14557579
2019-08-01 21:59:39,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741890_1066, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:43,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741891_1067 src: /192.168.56.101:56756 dest: /192.168.56.102:50010
2019-08-01 21:59:43,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56756, dest: /192.168.56.102:50010, bytes: 105504, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-149946142_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741891_1067, duration: 24966139
2019-08-01 21:59:43,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741891_1067, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:48,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741892_1068 src: /192.168.56.101:56770 dest: /192.168.56.102:50010
2019-08-01 21:59:52,903 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741877_1053
2019-08-01 21:59:52,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741893_1069 src: /192.168.56.101:56782 dest: /192.168.56.102:50010
2019-08-01 21:59:52,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56782, dest: /192.168.56.102:50010, bytes: 3005, op: HDFS_WRITE, cliID: DFSClient_attempt_1564683256393_0003_r_000000_0_-1701970946_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741893_1069, duration: 27116913
2019-08-01 21:59:52,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741893_1069, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:53,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56770, dest: /192.168.56.102:50010, bytes: 33622, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-149946142_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741892_1068, duration: 5033240001
2019-08-01 21:59:53,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741892_1068, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:53,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741894_1070 src: /192.168.56.101:56788 dest: /192.168.56.102:50010
2019-08-01 21:59:53,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56788, dest: /192.168.56.102:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-149946142_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741894_1070, duration: 4462614
2019-08-01 21:59:53,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741894_1070, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:53,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741895_1071 src: /192.168.56.101:56794 dest: /192.168.56.102:50010
2019-08-01 21:59:53,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56794, dest: /192.168.56.102:50010, bytes: 33622, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-149946142_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741895_1071, duration: 7197014
2019-08-01 21:59:53,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741895_1071, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:53,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741896_1072 src: /192.168.56.101:56798 dest: /192.168.56.102:50010
2019-08-01 21:59:53,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56798, dest: /192.168.56.102:50010, bytes: 105504, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-149946142_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741896_1072, duration: 5542697
2019-08-01 21:59:53,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741896_1072, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-01 21:59:57,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2019-08-01 21:59:57,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2019-08-01 21:59:57,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2019-08-01 21:59:57,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2019-08-01 21:59:57,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2019-08-01 21:59:57,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2019-08-01 21:59:57,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741888_1064 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741888
2019-08-01 21:59:57,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741889_1065 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741889
2019-08-01 21:59:57,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741890_1066 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741890
2019-08-01 21:59:57,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741891_1067 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741891
2019-08-01 21:59:57,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741892_1068 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741892
2019-08-01 21:59:57,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741887_1063 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741887
2019-08-01 22:00:00,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741897_1073 src: /192.168.56.101:56806 dest: /192.168.56.102:50010
2019-08-01 22:00:00,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56806, dest: /192.168.56.102:50010, bytes: 46410, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1764646633_130, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741897_1073, duration: 7946373
2019-08-01 22:00:00,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741897_1073, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:18:28,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-02 08:18:28,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-02 08:18:29,262 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 08:18:29,845 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-02 08:18:30,004 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-02 08:18:30,004 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-02 08:18:30,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-02 08:18:30,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-02 08:18:30,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-02 08:18:30,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-02 08:18:30,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-02 08:18:30,180 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-02 08:18:30,182 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-02 08:18:30,200 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-02 08:18:30,204 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-02 08:18:30,204 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-02 08:18:30,204 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-02 08:18:30,221 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-02 08:18:30,225 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-02 08:18:30,225 INFO org.mortbay.log: jetty-6.1.26
2019-08-02 08:18:30,580 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-02 08:18:31,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-02 08:18:31,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-02 08:18:31,127 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-02 08:18:31,180 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-02 08:18:31,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-02 08:18:31,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-02 08:18:31,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-02 08:18:31,279 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 08:18:31,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-02 08:18:31,301 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-02 08:18:31,309 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-02 08:18:31,764 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3674@um2
2019-08-02 08:18:31,882 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:18:31,882 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:18:31,883 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-02 08:18:31,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-02 08:18:31,904 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 08:18:31,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-02 08:18:31,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-02 08:18:32,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-02 08:18:32,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:18:32,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 08:18:32,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 30ms
2019-08-02 08:18:32,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 32ms
2019-08-02 08:18:32,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 08:18:32,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 17ms
2019-08-02 08:18:32,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 17ms
2019-08-02 08:18:32,056 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564741840056 with interval 21600000
2019-08-02 08:18:32,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-02 08:18:32,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-02 08:18:32,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-02 08:18:32,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=581
2019-08-02 08:18:32,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-02 08:18:32,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3f930b7ecc3,  containing 1 storage report(s), of which we sent 1. The reports had 49 total blocks and used 1 RPC(s). This took 1 msec to generate and 40 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-02 08:18:32,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:18:32,226 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-02 08:18:32,226 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-02 08:18:32,228 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-02 08:18:32,228 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-02 08:18:32,229 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:18:32,242 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-02 08:18:37,115 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741856_1032
2019-08-02 08:22:23,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x42ef7ba9516,  containing 1 storage report(s), of which we sent 1. The reports had 49 total blocks and used 1 RPC(s). This took 1 msec to generate and 11 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-02 08:22:23,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:26:09,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741898_1074 src: /192.168.56.101:47374 dest: /192.168.56.102:50010
2019-08-02 08:26:11,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47374, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_255973456_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741898_1074, duration: 2113474231
2019-08-02 08:26:11,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741898_1074, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:26:11,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741899_1075 src: /192.168.56.101:47378 dest: /192.168.56.102:50010
2019-08-02 08:26:12,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47378, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_255973456_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741899_1075, duration: 993353228
2019-08-02 08:26:12,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741899_1075, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:26:12,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741900_1076 src: /192.168.56.101:47382 dest: /192.168.56.102:50010
2019-08-02 08:26:12,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47382, dest: /192.168.56.102:50010, bytes: 83406, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_255973456_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741900_1076, duration: 5413192
2019-08-02 08:26:12,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741900_1076, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:26:33,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741901_1077 src: /192.168.56.101:47432 dest: /192.168.56.102:50010
2019-08-02 08:26:35,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47432, dest: /192.168.56.102:50010, bytes: 18925, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_255973456_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741901_1077, duration: 2730376289
2019-08-02 08:26:35,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741901_1077, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:26:36,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741902_1078 src: /192.168.56.102:53700 dest: /192.168.56.102:50010
2019-08-02 08:26:36,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741903_1079 src: /192.168.56.101:47442 dest: /192.168.56.102:50010
2019-08-02 08:26:36,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:53700, dest: /192.168.56.102:50010, bytes: 4265, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-462445473_77, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741902_1078, duration: 81655864
2019-08-02 08:26:36,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741902_1078, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-02 08:26:37,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47442, dest: /192.168.56.102:50010, bytes: 9109, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1655973524_79, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741903_1079, duration: 48822475
2019-08-02 08:26:37,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741903_1079, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:26:41,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741898_1074 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741898 for deletion
2019-08-02 08:26:41,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2019-08-02 08:26:41,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741900_1076 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741900 for deletion
2019-08-02 08:26:41,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741898_1074 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741898
2019-08-02 08:26:41,208 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741899_1075 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741899
2019-08-02 08:26:41,209 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741900_1076 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741900
2019-08-02 08:28:24,712 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741898_1074
2019-08-02 08:32:02,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741904_1080 src: /192.168.56.101:47650 dest: /192.168.56.102:50010
2019-08-02 08:32:11,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47650, dest: /192.168.56.102:50010, bytes: 18677, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2113020261_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741904_1080, duration: 9022300350
2019-08-02 08:32:11,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741904_1080, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:38:17,294 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-02 08:38:20,997 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-02 08:38:20,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-02 08:41:03,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-02 08:41:03,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-02 08:41:03,536 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 08:41:04,043 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-02 08:41:04,165 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-02 08:41:04,165 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-02 08:41:04,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-02 08:41:04,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-02 08:41:04,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-02 08:41:04,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-02 08:41:04,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-02 08:41:04,301 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-02 08:41:04,308 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-02 08:41:04,321 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-02 08:41:04,331 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-02 08:41:04,331 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-02 08:41:04,331 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-02 08:41:04,349 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-02 08:41:04,356 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-02 08:41:04,356 INFO org.mortbay.log: jetty-6.1.26
2019-08-02 08:41:04,727 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-02 08:41:05,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-02 08:41:05,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-02 08:41:05,213 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-02 08:41:05,233 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-02 08:41:05,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-02 08:41:05,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-02 08:41:05,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-02 08:41:05,305 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 08:41:05,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-02 08:41:05,314 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-02 08:41:05,315 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-02 08:41:05,573 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 4584@um2
2019-08-02 08:41:05,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:41:05,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:41:05,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-02 08:41:05,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-02 08:41:05,712 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 08:41:05,738 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-02 08:41:05,738 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-02 08:41:05,780 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-02 08:41:05,780 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:41:05,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 08:41:05,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 3631504
2019-08-02 08:41:05,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 16ms
2019-08-02 08:41:05,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 19ms
2019-08-02 08:41:05,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 08:41:05,811 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 11ms
2019-08-02 08:41:05,811 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2019-08-02 08:41:05,813 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564742547813 with interval 21600000
2019-08-02 08:41:05,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-02 08:41:05,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-02 08:41:05,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-02 08:41:05,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=640
2019-08-02 08:41:05,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-02 08:41:05,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5345fe37a9d,  containing 1 storage report(s), of which we sent 1. The reports had 53 total blocks and used 1 RPC(s). This took 1 msec to generate and 17 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-02 08:41:05,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:41:05,905 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-02 08:41:05,905 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-02 08:41:05,908 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-02 08:41:05,908 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-02 08:41:05,908 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:41:05,919 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-02 08:41:10,906 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741860_1036
2019-08-02 08:41:37,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741905_1081 src: /192.168.56.101:47864 dest: /192.168.56.102:50010
2019-08-02 08:41:40,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47864, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1552199985_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741905_1081, duration: 2021551688
2019-08-02 08:41:40,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741905_1081, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:41:40,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741906_1082 src: /192.168.56.101:47868 dest: /192.168.56.102:50010
2019-08-02 08:41:40,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47868, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1552199985_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741906_1082, duration: 671971789
2019-08-02 08:41:40,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741906_1082, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:41:40,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741907_1083 src: /192.168.56.101:47872 dest: /192.168.56.102:50010
2019-08-02 08:41:40,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47872, dest: /192.168.56.102:50010, bytes: 83594, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1552199985_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741907_1083, duration: 6394796
2019-08-02 08:41:40,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741907_1083, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:42:53,566 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741906_1082
2019-08-02 08:48:56,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741908_1084 src: /192.168.56.101:47900 dest: /192.168.56.102:50010
2019-08-02 08:48:57,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47900, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1221738968_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741908_1084, duration: 1612593052
2019-08-02 08:48:57,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741908_1084, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:48:57,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741909_1085 src: /192.168.56.101:47904 dest: /192.168.56.102:50010
2019-08-02 08:48:58,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47904, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1221738968_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741909_1085, duration: 765479940
2019-08-02 08:48:58,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741909_1085, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:48:58,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741910_1086 src: /192.168.56.101:47908 dest: /192.168.56.102:50010
2019-08-02 08:48:58,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:47908, dest: /192.168.56.102:50010, bytes: 83593, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1221738968_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741910_1086, duration: 1619421
2019-08-02 08:48:58,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741910_1086, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:50:12,603 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741909_1085
2019-08-02 08:55:21,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5fb78c0907f,  containing 1 storage report(s), of which we sent 1. The reports had 59 total blocks and used 1 RPC(s). This took 2 msec to generate and 11 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-02 08:55:21,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 08:58:08,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741911_1087 src: /192.168.56.101:49882 dest: /192.168.56.102:50010
2019-08-02 08:58:08,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:49882, dest: /192.168.56.102:50010, bytes: 114704, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1470782897_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741911_1087, duration: 23958008
2019-08-02 08:58:08,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741911_1087, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 08:58:08,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741912_1088 src: /192.168.56.102:56170 dest: /192.168.56.102:50010
2019-08-02 08:58:08,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:56170, dest: /192.168.56.102:50010, bytes: 44924, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1668971553_91, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741912_1088, duration: 81359305
2019-08-02 08:58:08,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741912_1088, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-02 08:58:12,016 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2019-08-02 08:58:12,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2019-08-02 08:58:12,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2019-08-02 08:58:12,041 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741905_1081 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741905
2019-08-02 08:58:12,041 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741906_1082 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741906
2019-08-02 08:58:12,041 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741907_1083 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741907
2019-08-02 08:58:17,944 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741912_1088
2019-08-02 08:59:03,024 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-02 08:59:05,517 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-02 08:59:05,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-02 09:04:58,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-02 09:04:58,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-02 09:04:59,174 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:04:59,678 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-02 09:04:59,819 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-02 09:04:59,819 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-02 09:04:59,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-02 09:04:59,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-02 09:04:59,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-02 09:04:59,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-02 09:04:59,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-02 09:04:59,961 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-02 09:04:59,964 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-02 09:04:59,984 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-02 09:04:59,988 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-02 09:04:59,988 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-02 09:04:59,988 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-02 09:05:00,018 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-02 09:05:00,024 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-02 09:05:00,024 INFO org.mortbay.log: jetty-6.1.26
2019-08-02 09:05:00,572 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-02 09:05:01,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-02 09:05:01,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-02 09:05:01,088 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-02 09:05:01,109 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-02 09:05:01,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-02 09:05:01,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-02 09:05:01,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-02 09:05:01,171 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:05:01,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-02 09:05:01,182 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-02 09:05:01,185 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-02 09:05:01,506 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 5705@um2
2019-08-02 09:05:01,608 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:05:01,608 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:05:01,608 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-02 09:05:01,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-02 09:05:01,633 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:05:01,696 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-02 09:05:01,696 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-02 09:05:01,768 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-02 09:05:01,769 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:05:01,769 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 09:05:01,780 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 211372429
2019-08-02 09:05:01,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 18ms
2019-08-02 09:05:01,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 20ms
2019-08-02 09:05:01,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 09:05:01,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 11ms
2019-08-02 09:05:01,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 11ms
2019-08-02 09:05:01,804 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564731301804 with interval 21600000
2019-08-02 09:05:01,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-02 09:05:01,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-02 09:05:01,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-02 09:05:01,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=698
2019-08-02 09:05:01,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-02 09:05:01,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x682ba996ade,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 17 msec to generate and 38 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-02 09:05:01,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:05:01,966 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-02 09:05:01,966 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-02 09:05:01,967 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-02 09:05:01,967 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-02 09:05:01,967 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:05:01,974 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-02 09:09:16,859 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-02 09:09:20,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-02 09:09:21,686 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-02 09:09:21,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-02 09:10:01,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-02 09:10:01,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-02 09:10:02,156 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:10:02,637 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-02 09:10:02,755 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-02 09:10:02,755 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-02 09:10:02,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-02 09:10:02,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-02 09:10:02,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-02 09:10:02,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-02 09:10:02,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-02 09:10:02,900 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-02 09:10:02,903 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-02 09:10:02,920 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-02 09:10:02,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-02 09:10:02,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-02 09:10:02,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-02 09:10:02,955 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-02 09:10:02,964 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-02 09:10:02,964 INFO org.mortbay.log: jetty-6.1.26
2019-08-02 09:10:03,348 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-02 09:10:03,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-02 09:10:03,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-02 09:10:03,823 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-02 09:10:03,844 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-02 09:10:03,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-02 09:10:03,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-02 09:10:03,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-02 09:10:03,901 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:10:03,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-02 09:10:03,910 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-02 09:10:03,919 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-02 09:10:04,159 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 6640@um2
2019-08-02 09:10:04,287 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:10:04,287 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:10:04,287 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-02 09:10:04,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-02 09:10:04,300 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:10:04,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-02 09:10:04,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-02 09:10:04,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-02 09:10:04,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:10:04,384 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 09:10:04,395 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 211372429
2019-08-02 09:10:04,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 16ms
2019-08-02 09:10:04,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 20ms
2019-08-02 09:10:04,400 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 09:10:04,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 13ms
2019-08-02 09:10:04,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 14ms
2019-08-02 09:10:04,416 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564740557416 with interval 21600000
2019-08-02 09:10:04,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-02 09:10:04,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-02 09:10:04,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-02 09:10:04,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=701
2019-08-02 09:10:04,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-02 09:10:04,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6c92d3efb1b,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 6 msec to generate and 25 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-02 09:10:04,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:10:04,521 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-02 09:10:04,521 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-02 09:10:04,528 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-02 09:10:04,528 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-02 09:10:04,528 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:10:04,533 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-02 09:10:09,480 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741902_1078
2019-08-02 09:11:07,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741913_1089 src: /192.168.56.101:50028 dest: /192.168.56.102:50010
2019-08-02 09:11:08,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50028, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1673687096_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741913_1089, duration: 1620952065
2019-08-02 09:11:08,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741913_1089, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:11:08,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741914_1090 src: /192.168.56.101:50032 dest: /192.168.56.102:50010
2019-08-02 09:11:10,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50032, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1673687096_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741914_1090, duration: 1215643887
2019-08-02 09:11:10,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741914_1090, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:11:10,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741915_1091 src: /192.168.56.101:50036 dest: /192.168.56.102:50010
2019-08-02 09:11:10,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50036, dest: /192.168.56.102:50010, bytes: 83417, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1673687096_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741915_1091, duration: 1814204
2019-08-02 09:11:10,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741915_1091, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:11:17,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741916_1092 src: /192.168.56.101:50064 dest: /192.168.56.102:50010
2019-08-02 09:11:26,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50064, dest: /192.168.56.102:50010, bytes: 18925, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1673687096_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741916_1092, duration: 9574358462
2019-08-02 09:11:26,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741916_1092, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:11:27,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741917_1093 src: /192.168.56.101:50072 dest: /192.168.56.102:50010
2019-08-02 09:11:27,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50072, dest: /192.168.56.102:50010, bytes: 5981, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1608334811_91, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741917_1093, duration: 52170799
2019-08-02 09:11:27,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741917_1093, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:11:27,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741918_1094 src: /192.168.56.102:57624 dest: /192.168.56.102:50010
2019-08-02 09:11:27,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:57624, dest: /192.168.56.102:50010, bytes: 3091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-849299445_89, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741918_1094, duration: 43954950
2019-08-02 09:11:27,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741918_1094, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-02 09:11:31,474 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2019-08-02 09:11:31,475 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2019-08-02 09:11:31,475 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2019-08-02 09:11:31,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741913_1089 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741913
2019-08-02 09:11:31,490 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741914_1090 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741914
2019-08-02 09:11:31,490 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741915_1091 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741915
2019-08-02 09:13:22,075 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741913_1089
2019-08-02 09:14:01,472 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-02 09:14:03,918 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-02 09:14:03,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-02 09:19:45,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-02 09:19:45,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-02 09:19:45,741 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:19:46,204 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-02 09:19:46,332 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-02 09:19:46,332 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-02 09:19:46,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-02 09:19:46,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-02 09:19:46,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-02 09:19:46,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-02 09:19:46,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-02 09:19:46,505 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-02 09:19:46,508 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-02 09:19:46,525 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-02 09:19:46,535 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-02 09:19:46,536 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-02 09:19:46,536 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-02 09:19:46,557 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-02 09:19:46,559 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-02 09:19:46,560 INFO org.mortbay.log: jetty-6.1.26
2019-08-02 09:19:47,003 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-02 09:19:47,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-02 09:19:47,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-02 09:19:47,473 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-02 09:19:47,496 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-02 09:19:47,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-02 09:19:47,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-02 09:19:47,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-02 09:19:47,593 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:19:47,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-02 09:19:47,618 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-02 09:19:47,627 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-02 09:19:47,866 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 7338@um2
2019-08-02 09:19:48,005 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:19:48,005 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:19:48,005 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-02 09:19:48,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-02 09:19:48,023 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:19:48,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-02 09:19:48,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-02 09:19:48,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-02 09:19:48,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:19:48,103 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 09:19:48,117 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 211400671
2019-08-02 09:19:48,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 21ms
2019-08-02 09:19:48,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 24ms
2019-08-02 09:19:48,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 09:19:48,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 17ms
2019-08-02 09:19:48,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 19ms
2019-08-02 09:19:48,149 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564740816149 with interval 21600000
2019-08-02 09:19:48,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-02 09:19:48,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-02 09:19:48,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-02 09:19:48,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=747
2019-08-02 09:19:48,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-02 09:19:48,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x75116e9dbcb,  containing 1 storage report(s), of which we sent 1. The reports had 61 total blocks and used 1 RPC(s). This took 1 msec to generate and 72 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-02 09:19:48,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:19:48,314 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-02 09:19:48,314 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-02 09:19:48,315 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2019-08-02 09:19:48,315 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-02 09:19:48,315 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:19:48,329 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-02 09:19:53,206 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741867_1043
2019-08-02 09:20:21,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741919_1095 src: /192.168.56.101:50176 dest: /192.168.56.102:50010
2019-08-02 09:20:23,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50176, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-783241082_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741919_1095, duration: 1502544461
2019-08-02 09:20:23,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741919_1095, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:20:23,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741920_1096 src: /192.168.56.101:50180 dest: /192.168.56.102:50010
2019-08-02 09:20:24,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50180, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-783241082_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741920_1096, duration: 757728626
2019-08-02 09:20:24,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741920_1096, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:20:24,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741921_1097 src: /192.168.56.101:50184 dest: /192.168.56.102:50010
2019-08-02 09:20:24,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50184, dest: /192.168.56.102:50010, bytes: 83641, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-783241082_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741921_1097, duration: 4166835
2019-08-02 09:20:24,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741921_1097, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:20:39,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741922_1098 src: /192.168.56.101:50212 dest: /192.168.56.102:50010
2019-08-02 09:20:40,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50212, dest: /192.168.56.102:50010, bytes: 19143, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-783241082_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741922_1098, duration: 1797762540
2019-08-02 09:20:40,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741922_1098, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:20:42,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741923_1099 src: /192.168.56.102:57654 dest: /192.168.56.102:50010
2019-08-02 09:20:42,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:57654, dest: /192.168.56.102:50010, bytes: 3483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-633393520_91, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741923_1099, duration: 89104387
2019-08-02 09:20:42,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741923_1099, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-02 09:20:42,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741924_1100 src: /192.168.56.101:50222 dest: /192.168.56.102:50010
2019-08-02 09:20:42,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:50222, dest: /192.168.56.102:50010, bytes: 7864, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1150331287_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741924_1100, duration: 61216909
2019-08-02 09:20:42,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741924_1100, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:20:48,182 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2019-08-02 09:20:48,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2019-08-02 09:20:48,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2019-08-02 09:20:48,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741920_1096 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741920
2019-08-02 09:20:48,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741921_1097 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741921
2019-08-02 09:20:48,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741919_1095 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741919
                                                                                                                                                                                                                                                                                                                                                                          2019-08-02 09:26:38,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-02 09:26:38,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-02 09:26:38,514 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:26:38,935 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-02 09:26:39,045 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-02 09:26:39,045 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-02 09:26:39,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-02 09:26:39,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-02 09:26:39,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-02 09:26:39,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-02 09:26:39,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-02 09:26:39,188 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-02 09:26:39,192 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-02 09:26:39,200 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-02 09:26:39,202 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-02 09:26:39,203 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-02 09:26:39,203 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-02 09:26:39,217 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-02 09:26:39,219 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-02 09:26:39,219 INFO org.mortbay.log: jetty-6.1.26
2019-08-02 09:26:39,481 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-02 09:26:39,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-02 09:26:39,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-02 09:26:39,869 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-02 09:26:39,885 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-02 09:26:39,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-02 09:26:39,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-02 09:26:39,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-02 09:26:39,959 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:26:39,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-02 09:26:39,987 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-02 09:26:39,987 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-02 09:26:40,246 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 2387@um2
2019-08-02 09:26:40,289 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:26:40,289 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:26:40,291 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-02 09:26:40,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-02 09:26:40,302 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-02 09:26:40,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-02 09:26:40,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-02 09:26:40,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-02 09:26:40,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:26:40,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 09:26:40,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 16ms
2019-08-02 09:26:40,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 17ms
2019-08-02 09:26:40,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-02 09:26:40,384 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 9ms
2019-08-02 09:26:40,384 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 10ms
2019-08-02 09:26:40,386 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1564739374386 with interval 21600000
2019-08-02 09:26:40,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-02 09:26:40,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-02 09:26:40,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-02 09:26:40,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=793
2019-08-02 09:26:40,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-02 09:26:40,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x32c0ccbe7d,  containing 1 storage report(s), of which we sent 1. The reports had 64 total blocks and used 1 RPC(s). This took 1 msec to generate and 25 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-02 09:26:40,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:26:40,478 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-02 09:26:40,479 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-02 09:26:40,480 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-02 09:26:40,480 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-02 09:26:40,481 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-02 09:26:40,485 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-02 09:26:45,421 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741910_1086
2019-08-02 09:28:07,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741925_1101 src: /192.168.56.101:32984 dest: /192.168.56.102:50010
2019-08-02 09:28:08,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:32984, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_542394315_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741925_1101, duration: 1445483289
2019-08-02 09:28:08,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741925_1101, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:28:08,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741926_1102 src: /192.168.56.101:32988 dest: /192.168.56.102:50010
2019-08-02 09:28:11,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:32988, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_542394315_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741926_1102, duration: 2378925916
2019-08-02 09:28:11,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741926_1102, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:28:11,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741927_1103 src: /192.168.56.101:32992 dest: /192.168.56.102:50010
2019-08-02 09:28:11,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:32992, dest: /192.168.56.102:50010, bytes: 83417, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_542394315_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741927_1103, duration: 4559433
2019-08-02 09:28:11,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741927_1103, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:28:25,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741928_1104 src: /192.168.56.101:33022 dest: /192.168.56.102:50010
2019-08-02 09:28:25,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33022, dest: /192.168.56.102:50010, bytes: 18927, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_542394315_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741928_1104, duration: 290762561
2019-08-02 09:28:25,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741928_1104, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:28:25,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741929_1105 src: /192.168.56.102:49074 dest: /192.168.56.102:50010
2019-08-02 09:28:25,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:49074, dest: /192.168.56.102:50010, bytes: 5280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2130853408_81, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741929_1105, duration: 43027171
2019-08-02 09:28:25,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741929_1105, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-02 09:28:26,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741930_1106 src: /192.168.56.101:33028 dest: /192.168.56.102:50010
2019-08-02 09:28:26,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33028, dest: /192.168.56.102:50010, bytes: 8441, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1400490921_81, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741930_1106, duration: 51874182
2019-08-02 09:28:26,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741930_1106, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:28:28,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741925_1101 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741925 for deletion
2019-08-02 09:28:28,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741926_1102 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741926 for deletion
2019-08-02 09:28:28,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2019-08-02 09:28:28,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741925_1101 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741925
2019-08-02 09:28:28,515 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741926_1102 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741926
2019-08-02 09:28:28,515 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741927_1103 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741927
2019-08-02 09:35:07,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741931_1107 src: /192.168.56.101:33230 dest: /192.168.56.102:50010
2019-08-02 09:35:08,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33230, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529876177_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741931_1107, duration: 1505692468
2019-08-02 09:35:08,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741931_1107, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:35:09,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741932_1108 src: /192.168.56.101:33234 dest: /192.168.56.102:50010
2019-08-02 09:35:10,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33234, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529876177_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741932_1108, duration: 1174666476
2019-08-02 09:35:10,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741932_1108, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:35:10,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741933_1109 src: /192.168.56.101:33238 dest: /192.168.56.102:50010
2019-08-02 09:35:10,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33238, dest: /192.168.56.102:50010, bytes: 83394, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529876177_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741933_1109, duration: 3482154
2019-08-02 09:35:10,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741933_1109, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:35:15,799 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741933_1109
2019-08-02 09:35:18,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741934_1110 src: /192.168.56.101:33264 dest: /192.168.56.102:50010
2019-08-02 09:35:25,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33264, dest: /192.168.56.102:50010, bytes: 18898, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529876177_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741934_1110, duration: 6781685600
2019-08-02 09:35:25,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741934_1110, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:35:25,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741935_1111 src: /192.168.56.101:33274 dest: /192.168.56.102:50010
2019-08-02 09:35:25,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33274, dest: /192.168.56.102:50010, bytes: 5457, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_256004408_123, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741935_1111, duration: 4330925
2019-08-02 09:35:25,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741935_1111, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:35:26,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741936_1112 src: /192.168.56.102:49112 dest: /192.168.56.102:50010
2019-08-02 09:35:26,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:49112, dest: /192.168.56.102:50010, bytes: 5379, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-162203942_117, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741936_1112, duration: 6909415
2019-08-02 09:35:26,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741936_1112, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-02 09:35:28,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2019-08-02 09:35:28,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2019-08-02 09:35:28,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2019-08-02 09:35:28,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741931_1107 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741931
2019-08-02 09:35:28,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741932_1108 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741932
2019-08-02 09:35:28,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741933_1109 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741933
2019-08-02 09:35:30,815 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741934_1110
2019-08-02 09:35:35,820 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741936_1112
2019-08-02 09:38:43,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741937_1113 src: /192.168.56.101:33288 dest: /192.168.56.102:50010
2019-08-02 09:38:46,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33288, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867617233_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741937_1113, duration: 2169163871
2019-08-02 09:38:46,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741937_1113, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:38:46,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741938_1114 src: /192.168.56.101:33292 dest: /192.168.56.102:50010
2019-08-02 09:38:46,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33292, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867617233_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741938_1114, duration: 867161228
2019-08-02 09:38:46,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741938_1114, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:38:47,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741939_1115 src: /192.168.56.101:33296 dest: /192.168.56.102:50010
2019-08-02 09:38:47,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33296, dest: /192.168.56.102:50010, bytes: 83394, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867617233_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741939_1115, duration: 4971616
2019-08-02 09:38:47,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741939_1115, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:38:55,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741940_1116 src: /192.168.56.101:33328 dest: /192.168.56.102:50010
2019-08-02 09:39:01,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33328, dest: /192.168.56.102:50010, bytes: 18898, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867617233_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741940_1116, duration: 5589029544
2019-08-02 09:39:01,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741940_1116, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:39:01,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741941_1117 src: /192.168.56.101:33336 dest: /192.168.56.102:50010
2019-08-02 09:39:01,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:33336, dest: /192.168.56.102:50010, bytes: 8445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_824400951_155, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741941_1117, duration: 10341818
2019-08-02 09:39:01,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741941_1117, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-02 09:39:02,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741942_1118 src: /192.168.56.102:49148 dest: /192.168.56.102:50010
2019-08-02 09:39:02,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:49148, dest: /192.168.56.102:50010, bytes: 5379, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1378250823_148, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741942_1118, duration: 12131988
2019-08-02 09:39:02,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741942_1118, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-02 09:39:07,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741937_1113 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741937 for deletion
2019-08-02 09:39:07,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741938_1114 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741938 for deletion
2019-08-02 09:39:07,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741939_1115 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741939 for deletion
2019-08-02 09:39:07,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741937_1113 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741937
2019-08-02 09:39:08,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741938_1114 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741938
2019-08-02 09:39:08,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741939_1115 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741939
2019-08-15 11:31:16,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-15 11:31:16,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-15 11:31:16,418 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-15 11:31:16,861 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-15 11:31:16,971 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-15 11:31:16,971 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-15 11:31:16,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-15 11:31:16,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-15 11:31:17,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-15 11:31:17,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-15 11:31:17,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-15 11:31:17,187 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-15 11:31:17,192 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-15 11:31:17,203 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-15 11:31:17,206 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-15 11:31:17,206 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-15 11:31:17,206 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-15 11:31:17,224 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-15 11:31:17,226 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-15 11:31:17,226 INFO org.mortbay.log: jetty-6.1.26
2019-08-15 11:31:17,502 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-15 11:31:17,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-15 11:31:17,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-15 11:31:17,968 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-15 11:31:17,982 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-15 11:31:18,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-15 11:31:18,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-15 11:31:18,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-15 11:31:18,068 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-15 11:31:18,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-15 11:31:18,090 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-15 11:31:18,090 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-15 11:31:18,361 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 16978@um2
2019-08-15 11:31:18,412 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-15 11:31:18,412 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-15 11:31:18,413 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-15 11:31:18,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-15 11:31:18,423 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-15 11:31:18,451 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-15 11:31:18,451 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-15 11:31:18,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-15 11:31:18,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-15 11:31:18,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-15 11:31:18,510 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 24ms
2019-08-15 11:31:18,511 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 25ms
2019-08-15 11:31:18,511 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-15 11:31:18,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 9ms
2019-08-15 11:31:18,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 11ms
2019-08-15 11:31:18,524 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1565882751524 with interval 21600000
2019-08-15 11:31:18,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-15 11:31:18,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-15 11:31:18,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-15 11:31:18,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=923
2019-08-15 11:31:18,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-15 11:31:18,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5bb839ef6f4,  containing 1 storage report(s), of which we sent 1. The reports had 73 total blocks and used 1 RPC(s). This took 1 msec to generate and 30 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-15 11:31:18,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-15 11:31:18,623 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-15 11:31:18,623 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-15 11:31:18,625 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-15 11:31:18,625 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-15 11:31:18,626 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-15 11:31:18,633 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-15 11:39:57,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741943_1119 src: /192.168.56.101:52034 dest: /192.168.56.102:50010
2019-08-15 11:39:57,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52034, dest: /192.168.56.102:50010, bytes: 137, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1763686131_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741943_1119, duration: 64784533
2019-08-15 11:39:57,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741943_1119, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-15 11:41:53,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741944_1120 src: /192.168.56.101:52042 dest: /192.168.56.102:50010
2019-08-15 11:41:53,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52042, dest: /192.168.56.102:50010, bytes: 137, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1413233975_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741944_1120, duration: 33176223
2019-08-15 11:41:53,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741944_1120, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-15 11:41:58,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741945_1121 src: /192.168.56.101:52050 dest: /192.168.56.102:50010
2019-08-15 11:41:58,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52050, dest: /192.168.56.102:50010, bytes: 137, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-721400241_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741945_1121, duration: 33300221
2019-08-15 11:41:58,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741945_1121, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-15 13:36:08,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741946_1122 src: /192.168.56.102:37586 dest: /192.168.56.102:50010
2019-08-15 13:36:09,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741947_1123 src: /192.168.56.101:52096 dest: /192.168.56.102:50010
2019-08-15 13:36:09,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:37586, dest: /192.168.56.102:50010, bytes: 141, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_624724893_38, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741946_1122, duration: 67886692
2019-08-15 13:36:09,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741946_1122, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-15 13:36:09,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52096, dest: /192.168.56.102:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1982635923_37, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741947_1123, duration: 82501934
2019-08-15 13:36:09,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741947_1123, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-15 13:36:18,714 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741946_1122
2019-08-15 13:48:42,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd3b04b1b884,  containing 1 storage report(s), of which we sent 1. The reports had 78 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-15 13:48:42,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-15 16:22:19,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741948_1124 src: /192.168.56.101:52390 dest: /192.168.56.102:50010
2019-08-15 16:22:19,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52390, dest: /192.168.56.102:50010, bytes: 40200, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_257478162_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741948_1124, duration: 53459304
2019-08-15 16:22:19,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741948_1124, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-15 16:22:19,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741949_1125 src: /192.168.56.101:52394 dest: /192.168.56.102:50010
2019-08-15 16:22:19,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:52394, dest: /192.168.56.102:50010, bytes: 41658, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_257478162_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741949_1125, duration: 4414700
2019-08-15 16:22:19,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741949_1125, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-16 15:09:23,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-16 15:09:23,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-16 15:09:24,197 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-16 15:09:24,662 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-16 15:09:24,792 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-16 15:09:24,792 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-16 15:09:24,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-16 15:09:24,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-16 15:09:24,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-16 15:09:24,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-16 15:09:24,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-16 15:09:29,977 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-16 15:09:29,981 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-16 15:09:29,994 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-16 15:09:30,005 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-16 15:09:30,005 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-16 15:09:30,006 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-16 15:09:30,020 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-16 15:09:30,022 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-16 15:09:30,022 INFO org.mortbay.log: jetty-6.1.26
2019-08-16 15:09:30,390 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-16 15:09:30,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-16 15:09:30,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-16 15:09:30,743 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-16 15:09:30,773 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-16 15:09:30,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-16 15:09:30,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-16 15:09:30,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-16 15:09:30,888 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-16 15:09:30,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-16 15:09:30,904 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-16 15:09:30,904 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-16 15:09:31,171 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 2778@um2
2019-08-16 15:09:31,221 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-16 15:09:31,222 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-16 15:09:31,223 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-16 15:09:31,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-16 15:09:31,238 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-16 15:09:31,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-16 15:09:31,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-16 15:09:31,290 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-16 15:09:31,290 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-16 15:09:31,291 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-16 15:09:31,323 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 32ms
2019-08-16 15:09:31,323 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 32ms
2019-08-16 15:09:31,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-16 15:09:31,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 14ms
2019-08-16 15:09:31,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 19ms
2019-08-16 15:09:31,345 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1565979966345 with interval 21600000
2019-08-16 15:09:31,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-16 15:09:31,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-16 15:09:31,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-16 15:09:31,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=1000
2019-08-16 15:09:31,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-16 15:09:31,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xae6c2226f9,  containing 1 storage report(s), of which we sent 1. The reports had 80 total blocks and used 1 RPC(s). This took 11 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-16 15:09:31,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-16 15:09:31,481 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-16 15:09:31,481 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-16 15:09:31,482 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-16 15:09:31,482 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-16 15:09:31,483 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-16 15:09:31,488 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-16 15:09:36,434 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741882_1058
2019-08-16 15:09:36,444 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741863_1039
2019-08-16 15:31:47,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741950_1126 src: /192.168.56.101:53720 dest: /192.168.56.102:50010
2019-08-16 15:31:47,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:53720, dest: /192.168.56.102:50010, bytes: 181, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921881597_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741950_1126, duration: 71012814
2019-08-16 15:31:47,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741950_1126, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-16 15:31:57,471 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741950_1126
2019-08-16 15:49:12,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741951_1127 src: /192.168.56.101:53748 dest: /192.168.56.102:50010
2019-08-16 15:49:12,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:53748, dest: /192.168.56.102:50010, bytes: 3882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1123984567_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741951_1127, duration: 46247088
2019-08-16 15:49:12,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741951_1127, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-16 15:49:17,959 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741951_1127
2019-08-17 19:46:25,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 19:46:25,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 19:46:26,212 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 19:46:26,665 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 19:46:26,815 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 19:46:26,815 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 19:46:26,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 19:46:26,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 19:46:26,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 19:46:26,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 19:46:26,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 19:46:26,974 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 19:46:26,977 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 19:46:26,985 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 19:46:26,988 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 19:46:26,988 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 19:46:26,988 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 19:46:27,002 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 19:46:27,005 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 19:46:27,006 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 19:46:27,314 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 19:46:27,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 19:46:27,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 19:46:27,687 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 19:46:27,702 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 19:46:27,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 19:46:27,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 19:46:27,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 19:46:27,810 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 19:46:27,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 19:46:27,852 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 19:46:27,852 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 19:46:28,185 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3228@um2
2019-08-17 19:46:28,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:46:28,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:46:28,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 19:46:28,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 19:46:28,252 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 19:46:28,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 19:46:28,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 19:46:28,319 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 19:46:28,319 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:46:28,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 19:46:28,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 26ms
2019-08-17 19:46:28,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 28ms
2019-08-17 19:46:28,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 19:46:28,369 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 22ms
2019-08-17 19:46:28,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 23ms
2019-08-17 19:46:28,374 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566075044374 with interval 21600000
2019-08-17 19:46:28,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 19:46:28,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 19:46:28,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 19:46:28,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=1048
2019-08-17 19:46:28,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 19:46:28,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6c4b9550de,  containing 1 storage report(s), of which we sent 1. The reports had 82 total blocks and used 1 RPC(s). This took 3 msec to generate and 33 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 19:46:28,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:46:28,503 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 19:46:28,503 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 19:46:28,505 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 19:46:28,505 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 19:46:28,505 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:46:28,513 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 19:46:33,436 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741826_1002
2019-08-17 19:47:46,441 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-17 19:47:50,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:47:50,541 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-17 19:47:50,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-17 19:49:05,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 19:49:05,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 19:49:05,697 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 19:49:05,971 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 19:49:06,037 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 19:49:06,037 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 19:49:06,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 19:49:06,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 19:49:06,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 19:49:06,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 19:49:06,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 19:49:06,176 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 19:49:06,178 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 19:49:06,189 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 19:49:06,191 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 19:49:06,191 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 19:49:06,191 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 19:49:06,202 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 19:49:06,205 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 19:49:06,205 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 19:49:06,395 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 19:49:06,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 19:49:06,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 19:49:06,678 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 19:49:06,691 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 19:49:06,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 19:49:06,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 19:49:06,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 19:49:06,748 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 19:49:06,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 19:49:06,761 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 19:49:06,764 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 19:49:07,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:08,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:09,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:10,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:11,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:12,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:13,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:14,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:15,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:16,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:16,832 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.56.101:9000
2019-08-17 19:49:22,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:23,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:24,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:25,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:26,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:27,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:28,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:29,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:30,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:31,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:31,852 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.56.101:9000
2019-08-17 19:49:37,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:38,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:39,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:40,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:41,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:42,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:43,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:44,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:45,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:46,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:46,926 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.56.101:9000
2019-08-17 19:49:52,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:53,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:54,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:55,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:56,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:57,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:58,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:49:59,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:00,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:01,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:01,971 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.56.101:9000
2019-08-17 19:50:07,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:08,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:09,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:10,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:11,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:12,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:13,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:14,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:15,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:16,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:16,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.56.101:9000
2019-08-17 19:50:22,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:23,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:25,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:26,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:27,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:28,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:29,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:30,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:31,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:32,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:32,025 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.56.101:9000
2019-08-17 19:50:38,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:39,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:40,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:41,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:42,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:43,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:44,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:45,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:46,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:47,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:47,086 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.56.101:9000
2019-08-17 19:50:53,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:54,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:55,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:56,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:57,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:58,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:50:59,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:51:00,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:51:01,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:51:02,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:51:02,115 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.56.101:9000
2019-08-17 19:51:08,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:51:09,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 19:51:09,290 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3855@um2
2019-08-17 19:51:09,341 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:51:09,341 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:51:09,342 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 19:51:09,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 19:51:09,353 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 19:51:09,367 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 19:51:09,367 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 19:51:09,398 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 19:51:09,398 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:51:09,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 19:51:09,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 211775488
2019-08-17 19:51:09,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 11ms
2019-08-17 19:51:09,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 12ms
2019-08-17 19:51:09,411 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 19:51:09,425 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 14ms
2019-08-17 19:51:09,425 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 15ms
2019-08-17 19:51:09,428 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566065387428 with interval 21600000
2019-08-17 19:51:09,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 19:51:09,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 19:51:09,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 19:51:09,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=1049
2019-08-17 19:51:09,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 19:51:09,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xadb94c7697,  containing 1 storage report(s), of which we sent 1. The reports had 82 total blocks and used 1 RPC(s). This took 1 msec to generate and 26 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 19:51:09,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:51:09,509 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 19:51:09,509 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 19:51:09,510 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 19:51:09,510 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 19:51:09,510 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 19:51:09,515 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 19:51:14,463 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741877_1053
2019-08-17 20:14:18,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 20:14:18,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 20:14:18,493 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 20:14:19,119 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 20:14:19,315 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 20:14:19,316 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 20:14:19,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 20:14:19,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 20:14:19,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 20:14:19,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 20:14:19,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 20:14:19,540 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 20:14:19,546 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 20:14:19,556 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 20:14:19,563 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 20:14:19,563 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 20:14:19,563 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 20:14:19,589 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 20:14:19,592 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 20:14:19,592 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 20:14:19,942 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 20:14:20,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 20:14:20,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 20:14:20,527 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 20:14:20,541 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 20:14:20,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 20:14:20,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 20:14:20,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 20:14:20,684 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 20:14:20,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 20:14:20,717 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 20:14:20,717 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 20:14:21,168 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 2471@um2
2019-08-17 20:14:21,265 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:14:21,265 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:14:21,267 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 20:14:21,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 20:14:21,281 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 20:14:21,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 20:14:21,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 20:14:21,378 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 20:14:21,378 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:14:21,383 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 20:14:21,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 48ms
2019-08-17 20:14:21,434 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 54ms
2019-08-17 20:14:21,442 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 20:14:21,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 18ms
2019-08-17 20:14:21,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 25ms
2019-08-17 20:14:21,468 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566087166468 with interval 21600000
2019-08-17 20:14:21,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 20:14:21,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 20:14:21,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 20:14:21,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=1050
2019-08-17 20:14:21,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 20:14:21,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa6418dd8c4,  containing 1 storage report(s), of which we sent 1. The reports had 82 total blocks and used 1 RPC(s). This took 2 msec to generate and 52 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 20:14:21,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:14:21,684 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 20:14:21,684 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 20:14:21,686 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 20:14:21,686 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 20:14:21,692 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:14:21,722 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 20:14:26,513 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741855_1031
2019-08-17 20:14:51,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741826_1002 to 192.168.56.103:50010 
2019-08-17 20:14:51,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741827_1003 to 192.168.56.103:50010 
2019-08-17 20:14:51,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741827_1003 (numBytes=103) to /192.168.56.103:50010
2019-08-17 20:14:51,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741826_1002 (numBytes=292710) to /192.168.56.103:50010
2019-08-17 20:14:54,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741864_1040 to 192.168.56.103:50010 
2019-08-17 20:14:54,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741865_1041 to 192.168.56.103:50010 
2019-08-17 20:14:54,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741865_1041 (numBytes=103) to /192.168.56.103:50010
2019-08-17 20:14:54,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741864_1040 (numBytes=292710) to /192.168.56.103:50010
2019-08-17 20:15:00,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741834_1010 to 192.168.56.103:50010 
2019-08-17 20:15:00,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741834_1010 (numBytes=292710) to /192.168.56.103:50010
2019-08-17 20:15:03,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741883_1059 to 192.168.56.103:50010 
2019-08-17 20:15:03,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741883_1059 (numBytes=103) to /192.168.56.103:50010
2019-08-17 20:20:57,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741956_1132 src: /192.168.56.101:37392 dest: /192.168.56.102:50010
2019-08-17 20:21:15,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37392, dest: /192.168.56.102:50010, bytes: 19179, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1666883810_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741956_1132, duration: 17793316735
2019-08-17 20:21:15,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741956_1132, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:21:15,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741957_1133 src: /192.168.56.102:44908 dest: /192.168.56.102:50010
2019-08-17 20:21:15,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:44908, dest: /192.168.56.102:50010, bytes: 263, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1332368862_81, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741957_1133, duration: 63570513
2019-08-17 20:21:15,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741957_1133, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 20:21:15,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741958_1134 src: /192.168.56.101:37450 dest: /192.168.56.102:50010
2019-08-17 20:21:15,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37450, dest: /192.168.56.102:50010, bytes: 9510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1989289256_81, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741958_1134, duration: 50002006
2019-08-17 20:21:15,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741958_1134, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:21:22,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741957_1133 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741957 for deletion
2019-08-17 20:21:22,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741957_1133 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741957
2019-08-17 20:26:38,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741963_1139 src: /192.168.56.101:37588 dest: /192.168.56.102:50010
2019-08-17 20:26:38,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37588, dest: /192.168.56.102:50010, bytes: 482222, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-749703228_11, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741963_1139, duration: 7063496
2019-08-17 20:26:38,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741963_1139, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:26:38,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741965_1141 src: /192.168.56.101:37596 dest: /192.168.56.102:50010
2019-08-17 20:26:38,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37596, dest: /192.168.56.102:50010, bytes: 83769, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-749703228_11, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741965_1141, duration: 2472691
2019-08-17 20:26:38,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741965_1141, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:26:46,776 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741958_1134
2019-08-17 20:26:54,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741966_1142 src: /192.168.56.103:55196 dest: /192.168.56.102:50010
2019-08-17 20:26:54,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:55196, dest: /192.168.56.102:50010, bytes: 5705, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1220478622_113, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741966_1142, duration: 11746954
2019-08-17 20:26:54,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741966_1142, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:26:54,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741967_1143 src: /192.168.56.102:44948 dest: /192.168.56.102:50010
2019-08-17 20:26:54,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:44948, dest: /192.168.56.102:50010, bytes: 6006, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1036299550_105, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741967_1143, duration: 9305239
2019-08-17 20:26:54,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741967_1143, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 20:26:58,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741969_1145 src: /192.168.56.101:37610 dest: /192.168.56.102:50010
2019-08-17 20:26:58,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37610, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-749703228_11, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741969_1145, duration: 587143894
2019-08-17 20:26:58,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741969_1145, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:26:58,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741970_1146 src: /192.168.56.101:37614 dest: /192.168.56.102:50010
2019-08-17 20:26:58,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37614, dest: /192.168.56.102:50010, bytes: 482222, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-749703228_11, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741970_1146, duration: 10288332
2019-08-17 20:26:58,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741970_1146, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:27:04,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2019-08-17 20:27:04,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2019-08-17 20:27:04,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741963_1139 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741963
2019-08-17 20:27:04,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741965_1141 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741965
2019-08-17 20:27:05,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741973_1149 src: /192.168.56.101:37652 dest: /192.168.56.102:50010
2019-08-17 20:27:06,804 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741967_1143
2019-08-17 20:27:15,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37652, dest: /192.168.56.102:50010, bytes: 18997, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-749703228_11, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741973_1149, duration: 9421482538
2019-08-17 20:27:15,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741973_1149, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:27:15,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741974_1150 src: /192.168.56.101:37666 dest: /192.168.56.102:50010
2019-08-17 20:27:15,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37666, dest: /192.168.56.102:50010, bytes: 9333, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-19901138_126, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741974_1150, duration: 3386376
2019-08-17 20:27:15,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741974_1150, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:27:15,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741975_1151 src: /192.168.56.103:55222 dest: /192.168.56.102:50010
2019-08-17 20:27:15,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:55222, dest: /192.168.56.102:50010, bytes: 2476, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-175051132_113, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741975_1151, duration: 3242676
2019-08-17 20:27:15,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741975_1151, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:27:22,306 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741969_1145 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741969 for deletion
2019-08-17 20:27:22,310 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741970_1146 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741970 for deletion
2019-08-17 20:27:22,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741969_1145 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741969
2019-08-17 20:27:22,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741970_1146 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741970
2019-08-17 20:39:13,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741976_1152 src: /192.168.56.101:37892 dest: /192.168.56.102:50010
2019-08-17 20:39:13,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37892, dest: /192.168.56.102:50010, bytes: 16993, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741976_1152, duration: 36280148
2019-08-17 20:39:13,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741976_1152, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741977_1153 src: /192.168.56.101:37896 dest: /192.168.56.102:50010
2019-08-17 20:39:13,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37896, dest: /192.168.56.102:50010, bytes: 201928, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741977_1153, duration: 4986370
2019-08-17 20:39:13,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741977_1153, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741979_1155 src: /192.168.56.101:37904 dest: /192.168.56.102:50010
2019-08-17 20:39:13,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37904, dest: /192.168.56.102:50010, bytes: 69409, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741979_1155, duration: 3611348
2019-08-17 20:39:13,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741979_1155, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741980_1156 src: /192.168.56.101:37908 dest: /192.168.56.102:50010
2019-08-17 20:39:13,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37908, dest: /192.168.56.102:50010, bytes: 445288, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741980_1156, duration: 11093756
2019-08-17 20:39:13,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741980_1156, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741981_1157 src: /192.168.56.101:37912 dest: /192.168.56.102:50010
2019-08-17 20:39:13,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37912, dest: /192.168.56.102:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741981_1157, duration: 4775394
2019-08-17 20:39:13,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741981_1157, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741983_1159 src: /192.168.56.101:37920 dest: /192.168.56.102:50010
2019-08-17 20:39:13,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37920, dest: /192.168.56.102:50010, bytes: 4467, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741983_1159, duration: 13603574
2019-08-17 20:39:13,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741983_1159, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741984_1160 src: /192.168.56.101:37924 dest: /192.168.56.102:50010
2019-08-17 20:39:13,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37924, dest: /192.168.56.102:50010, bytes: 14766, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741984_1160, duration: 2879241
2019-08-17 20:39:13,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741984_1160, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741985_1161 src: /192.168.56.101:37928 dest: /192.168.56.102:50010
2019-08-17 20:39:13,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37928, dest: /192.168.56.102:50010, bytes: 448794, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741985_1161, duration: 6091234
2019-08-17 20:39:13,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741985_1161, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741986_1162 src: /192.168.56.101:37932 dest: /192.168.56.102:50010
2019-08-17 20:39:13,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37932, dest: /192.168.56.102:50010, bytes: 44925, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741986_1162, duration: 2814382
2019-08-17 20:39:13,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741986_1162, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741990_1166 src: /192.168.56.101:37948 dest: /192.168.56.102:50010
2019-08-17 20:39:13,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37948, dest: /192.168.56.102:50010, bytes: 1194003, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741990_1166, duration: 14257718
2019-08-17 20:39:13,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741990_1166, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:13,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741991_1167 src: /192.168.56.101:37952 dest: /192.168.56.102:50010
2019-08-17 20:39:14,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37952, dest: /192.168.56.102:50010, bytes: 436303, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741991_1167, duration: 6741912
2019-08-17 20:39:14,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741991_1167, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741992_1168 src: /192.168.56.101:37956 dest: /192.168.56.102:50010
2019-08-17 20:39:14,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37956, dest: /192.168.56.102:50010, bytes: 192993, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741992_1168, duration: 3413462
2019-08-17 20:39:14,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741992_1168, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741994_1170 src: /192.168.56.101:37964 dest: /192.168.56.102:50010
2019-08-17 20:39:14,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37964, dest: /192.168.56.102:50010, bytes: 17008, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741994_1170, duration: 3291669
2019-08-17 20:39:14,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741994_1170, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741995_1171 src: /192.168.56.101:37968 dest: /192.168.56.102:50010
2019-08-17 20:39:14,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37968, dest: /192.168.56.102:50010, bytes: 2842667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741995_1171, duration: 21017413
2019-08-17 20:39:14,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741995_1171, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741997_1173 src: /192.168.56.101:37976 dest: /192.168.56.102:50010
2019-08-17 20:39:14,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37976, dest: /192.168.56.102:50010, bytes: 187882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741997_1173, duration: 3376640
2019-08-17 20:39:14,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741997_1173, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741999_1175 src: /192.168.56.101:37984 dest: /192.168.56.102:50010
2019-08-17 20:39:14,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37984, dest: /192.168.56.102:50010, bytes: 258370, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073741999_1175, duration: 4522824
2019-08-17 20:39:14,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073741999_1175, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742002_1178 src: /192.168.56.101:37996 dest: /192.168.56.102:50010
2019-08-17 20:39:14,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:37996, dest: /192.168.56.102:50010, bytes: 50619, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742002_1178, duration: 2543285
2019-08-17 20:39:14,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742002_1178, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742003_1179 src: /192.168.56.101:38000 dest: /192.168.56.102:50010
2019-08-17 20:39:14,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38000, dest: /192.168.56.102:50010, bytes: 223573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742003_1179, duration: 3698016
2019-08-17 20:39:14,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742003_1179, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742005_1181 src: /192.168.56.101:38008 dest: /192.168.56.102:50010
2019-08-17 20:39:14,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38008, dest: /192.168.56.102:50010, bytes: 206035, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742005_1181, duration: 8731990
2019-08-17 20:39:14,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742005_1181, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742006_1182 src: /192.168.56.101:38012 dest: /192.168.56.102:50010
2019-08-17 20:39:14,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38012, dest: /192.168.56.102:50010, bytes: 41123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742006_1182, duration: 4461019
2019-08-17 20:39:14,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742006_1182, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742010_1186 src: /192.168.56.101:38028 dest: /192.168.56.102:50010
2019-08-17 20:39:14,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38028, dest: /192.168.56.102:50010, bytes: 241367, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742010_1186, duration: 3013596
2019-08-17 20:39:14,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742010_1186, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742014_1190 src: /192.168.56.101:38044 dest: /192.168.56.102:50010
2019-08-17 20:39:14,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38044, dest: /192.168.56.102:50010, bytes: 143602, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742014_1190, duration: 4402457
2019-08-17 20:39:14,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742014_1190, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742017_1193 src: /192.168.56.101:38056 dest: /192.168.56.102:50010
2019-08-17 20:39:14,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38056, dest: /192.168.56.102:50010, bytes: 284220, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742017_1193, duration: 4654058
2019-08-17 20:39:14,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742017_1193, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742018_1194 src: /192.168.56.101:38060 dest: /192.168.56.102:50010
2019-08-17 20:39:14,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38060, dest: /192.168.56.102:50010, bytes: 479881, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742018_1194, duration: 8223456
2019-08-17 20:39:14,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742018_1194, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742020_1196 src: /192.168.56.101:38068 dest: /192.168.56.102:50010
2019-08-17 20:39:14,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38068, dest: /192.168.56.102:50010, bytes: 2035066, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742020_1196, duration: 13851929
2019-08-17 20:39:14,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742020_1196, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742023_1199 src: /192.168.56.101:38080 dest: /192.168.56.102:50010
2019-08-17 20:39:14,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38080, dest: /192.168.56.102:50010, bytes: 79845, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742023_1199, duration: 3017364
2019-08-17 20:39:14,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742023_1199, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742024_1200 src: /192.168.56.101:38084 dest: /192.168.56.102:50010
2019-08-17 20:39:14,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38084, dest: /192.168.56.102:50010, bytes: 164422, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742024_1200, duration: 7721223
2019-08-17 20:39:14,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742024_1200, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742025_1201 src: /192.168.56.101:38088 dest: /192.168.56.102:50010
2019-08-17 20:39:14,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38088, dest: /192.168.56.102:50010, bytes: 68866, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742025_1201, duration: 2658995
2019-08-17 20:39:14,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742025_1201, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:14,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742026_1202 src: /192.168.56.101:38092 dest: /192.168.56.102:50010
2019-08-17 20:39:14,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38092, dest: /192.168.56.102:50010, bytes: 185245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742026_1202, duration: 2882455
2019-08-17 20:39:14,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742026_1202, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742028_1204 src: /192.168.56.101:38100 dest: /192.168.56.102:50010
2019-08-17 20:39:15,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38100, dest: /192.168.56.102:50010, bytes: 339666, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742028_1204, duration: 5227168
2019-08-17 20:39:15,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742028_1204, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742029_1205 src: /192.168.56.101:38104 dest: /192.168.56.102:50010
2019-08-17 20:39:15,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38104, dest: /192.168.56.102:50010, bytes: 1890075, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742029_1205, duration: 15564942
2019-08-17 20:39:15,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742029_1205, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742030_1206 src: /192.168.56.101:38108 dest: /192.168.56.102:50010
2019-08-17 20:39:15,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38108, dest: /192.168.56.102:50010, bytes: 1809447, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742030_1206, duration: 14037061
2019-08-17 20:39:15,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742030_1206, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742035_1211 src: /192.168.56.101:38128 dest: /192.168.56.102:50010
2019-08-17 20:39:15,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38128, dest: /192.168.56.102:50010, bytes: 710492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742035_1211, duration: 6108888
2019-08-17 20:39:15,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742035_1211, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742037_1213 src: /192.168.56.101:38136 dest: /192.168.56.102:50010
2019-08-17 20:39:15,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38136, dest: /192.168.56.102:50010, bytes: 40471, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742037_1213, duration: 1765046
2019-08-17 20:39:15,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742037_1213, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742038_1214 src: /192.168.56.101:38140 dest: /192.168.56.102:50010
2019-08-17 20:39:15,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38140, dest: /192.168.56.102:50010, bytes: 90610, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742038_1214, duration: 2087531
2019-08-17 20:39:15,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742038_1214, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742039_1215 src: /192.168.56.101:38144 dest: /192.168.56.102:50010
2019-08-17 20:39:15,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38144, dest: /192.168.56.102:50010, bytes: 25986, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742039_1215, duration: 7151411
2019-08-17 20:39:15,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742039_1215, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742040_1216 src: /192.168.56.101:38148 dest: /192.168.56.102:50010
2019-08-17 20:39:15,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38148, dest: /192.168.56.102:50010, bytes: 3354982, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742040_1216, duration: 23609644
2019-08-17 20:39:15,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742040_1216, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742042_1218 src: /192.168.56.101:38156 dest: /192.168.56.102:50010
2019-08-17 20:39:15,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38156, dest: /192.168.56.102:50010, bytes: 528586, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742042_1218, duration: 4505445
2019-08-17 20:39:15,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742042_1218, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:15,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742044_1220 src: /192.168.56.101:38164 dest: /192.168.56.102:50010
2019-08-17 20:39:15,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38164, dest: /192.168.56.102:50010, bytes: 1537966, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742044_1220, duration: 12884321
2019-08-17 20:39:15,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742044_1220, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742047_1223 src: /192.168.56.101:38176 dest: /192.168.56.102:50010
2019-08-17 20:39:16,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38176, dest: /192.168.56.102:50010, bytes: 1896185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742047_1223, duration: 13327744
2019-08-17 20:39:16,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742047_1223, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742050_1226 src: /192.168.56.101:38188 dest: /192.168.56.102:50010
2019-08-17 20:39:16,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38188, dest: /192.168.56.102:50010, bytes: 319959, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742050_1226, duration: 3454661
2019-08-17 20:39:16,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742050_1226, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742051_1227 src: /192.168.56.101:38192 dest: /192.168.56.102:50010
2019-08-17 20:39:16,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38192, dest: /192.168.56.102:50010, bytes: 52264, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742051_1227, duration: 1877326
2019-08-17 20:39:16,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742051_1227, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742057_1233 src: /192.168.56.101:38216 dest: /192.168.56.102:50010
2019-08-17 20:39:16,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38216, dest: /192.168.56.102:50010, bytes: 178947, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742057_1233, duration: 6906910
2019-08-17 20:39:16,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742057_1233, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742059_1235 src: /192.168.56.101:38224 dest: /192.168.56.102:50010
2019-08-17 20:39:16,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38224, dest: /192.168.56.102:50010, bytes: 118973, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742059_1235, duration: 1972349
2019-08-17 20:39:16,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742059_1235, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742060_1236 src: /192.168.56.101:38228 dest: /192.168.56.102:50010
2019-08-17 20:39:16,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38228, dest: /192.168.56.102:50010, bytes: 31212, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742060_1236, duration: 9974394
2019-08-17 20:39:16,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742060_1236, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742061_1237 src: /192.168.56.101:38232 dest: /192.168.56.102:50010
2019-08-17 20:39:16,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38232, dest: /192.168.56.102:50010, bytes: 736658, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742061_1237, duration: 6540403
2019-08-17 20:39:16,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742061_1237, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742064_1240 src: /192.168.56.101:38244 dest: /192.168.56.102:50010
2019-08-17 20:39:16,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38244, dest: /192.168.56.102:50010, bytes: 46983, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742064_1240, duration: 2283731
2019-08-17 20:39:16,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742064_1240, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742066_1242 src: /192.168.56.101:38252 dest: /192.168.56.102:50010
2019-08-17 20:39:16,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38252, dest: /192.168.56.102:50010, bytes: 232248, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742066_1242, duration: 3241287
2019-08-17 20:39:16,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742066_1242, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742068_1244 src: /192.168.56.101:38260 dest: /192.168.56.102:50010
2019-08-17 20:39:16,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38260, dest: /192.168.56.102:50010, bytes: 18336, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742068_1244, duration: 1453893
2019-08-17 20:39:16,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742068_1244, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742070_1246 src: /192.168.56.101:38268 dest: /192.168.56.102:50010
2019-08-17 20:39:16,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38268, dest: /192.168.56.102:50010, bytes: 41263, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742070_1246, duration: 1948330
2019-08-17 20:39:16,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742070_1246, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742071_1247 src: /192.168.56.101:38272 dest: /192.168.56.102:50010
2019-08-17 20:39:16,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38272, dest: /192.168.56.102:50010, bytes: 515604, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742071_1247, duration: 6670755
2019-08-17 20:39:16,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742071_1247, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742072_1248 src: /192.168.56.101:38276 dest: /192.168.56.102:50010
2019-08-17 20:39:16,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38276, dest: /192.168.56.102:50010, bytes: 27084, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742072_1248, duration: 2180401
2019-08-17 20:39:16,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742072_1248, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742073_1249 src: /192.168.56.101:38280 dest: /192.168.56.102:50010
2019-08-17 20:39:16,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38280, dest: /192.168.56.102:50010, bytes: 735574, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742073_1249, duration: 15227761
2019-08-17 20:39:16,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742073_1249, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742074_1250 src: /192.168.56.101:38284 dest: /192.168.56.102:50010
2019-08-17 20:39:16,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38284, dest: /192.168.56.102:50010, bytes: 10023, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742074_1250, duration: 1677328
2019-08-17 20:39:16,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742074_1250, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:16,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742075_1251 src: /192.168.56.101:38288 dest: /192.168.56.102:50010
2019-08-17 20:39:16,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38288, dest: /192.168.56.102:50010, bytes: 714194, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742075_1251, duration: 8080626
2019-08-17 20:39:16,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742075_1251, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742078_1254 src: /192.168.56.101:38300 dest: /192.168.56.102:50010
2019-08-17 20:39:17,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38300, dest: /192.168.56.102:50010, bytes: 5950, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742078_1254, duration: 2552520
2019-08-17 20:39:17,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742078_1254, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742080_1256 src: /192.168.56.101:38308 dest: /192.168.56.102:50010
2019-08-17 20:39:17,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38308, dest: /192.168.56.102:50010, bytes: 115534, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742080_1256, duration: 2132283
2019-08-17 20:39:17,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742080_1256, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742081_1257 src: /192.168.56.101:38312 dest: /192.168.56.102:50010
2019-08-17 20:39:17,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38312, dest: /192.168.56.102:50010, bytes: 395195, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742081_1257, duration: 5265776
2019-08-17 20:39:17,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742081_1257, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742082_1258 src: /192.168.56.101:38316 dest: /192.168.56.102:50010
2019-08-17 20:39:17,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38316, dest: /192.168.56.102:50010, bytes: 105134, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742082_1258, duration: 2241659
2019-08-17 20:39:17,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742082_1258, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742083_1259 src: /192.168.56.101:38320 dest: /192.168.56.102:50010
2019-08-17 20:39:17,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38320, dest: /192.168.56.102:50010, bytes: 16430, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742083_1259, duration: 9543950
2019-08-17 20:39:17,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742083_1259, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742084_1260 src: /192.168.56.101:38324 dest: /192.168.56.102:50010
2019-08-17 20:39:17,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38324, dest: /192.168.56.102:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742084_1260, duration: 2748112
2019-08-17 20:39:17,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742084_1260, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742087_1263 src: /192.168.56.101:38336 dest: /192.168.56.102:50010
2019-08-17 20:39:17,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38336, dest: /192.168.56.102:50010, bytes: 18098, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742087_1263, duration: 1472092
2019-08-17 20:39:17,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742087_1263, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742088_1264 src: /192.168.56.101:38340 dest: /192.168.56.102:50010
2019-08-17 20:39:17,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38340, dest: /192.168.56.102:50010, bytes: 66270, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742088_1264, duration: 5583711
2019-08-17 20:39:17,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742088_1264, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742091_1267 src: /192.168.56.101:38352 dest: /192.168.56.102:50010
2019-08-17 20:39:17,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38352, dest: /192.168.56.102:50010, bytes: 951701, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742091_1267, duration: 16543886
2019-08-17 20:39:17,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742091_1267, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742092_1268 src: /192.168.56.101:38356 dest: /192.168.56.102:50010
2019-08-17 20:39:17,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38356, dest: /192.168.56.102:50010, bytes: 2041628, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742092_1268, duration: 17343694
2019-08-17 20:39:17,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742092_1268, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742093_1269 src: /192.168.56.101:38360 dest: /192.168.56.102:50010
2019-08-17 20:39:17,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38360, dest: /192.168.56.102:50010, bytes: 539912, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742093_1269, duration: 5969926
2019-08-17 20:39:17,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742093_1269, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742094_1270 src: /192.168.56.101:38364 dest: /192.168.56.102:50010
2019-08-17 20:39:17,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38364, dest: /192.168.56.102:50010, bytes: 177131, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742094_1270, duration: 2042758
2019-08-17 20:39:17,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742094_1270, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742095_1271 src: /192.168.56.101:38368 dest: /192.168.56.102:50010
2019-08-17 20:39:17,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38368, dest: /192.168.56.102:50010, bytes: 213911, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742095_1271, duration: 4715042
2019-08-17 20:39:17,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742095_1271, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742097_1273 src: /192.168.56.101:38376 dest: /192.168.56.102:50010
2019-08-17 20:39:17,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38376, dest: /192.168.56.102:50010, bytes: 427780, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742097_1273, duration: 5271574
2019-08-17 20:39:17,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742097_1273, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742098_1274 src: /192.168.56.101:38380 dest: /192.168.56.102:50010
2019-08-17 20:39:17,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38380, dest: /192.168.56.102:50010, bytes: 12131, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742098_1274, duration: 1841197
2019-08-17 20:39:17,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742098_1274, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742099_1275 src: /192.168.56.101:38384 dest: /192.168.56.102:50010
2019-08-17 20:39:17,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38384, dest: /192.168.56.102:50010, bytes: 82421, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742099_1275, duration: 2565741
2019-08-17 20:39:17,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742099_1275, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742103_1279 src: /192.168.56.101:38400 dest: /192.168.56.102:50010
2019-08-17 20:39:17,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38400, dest: /192.168.56.102:50010, bytes: 15071, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742103_1279, duration: 1524138
2019-08-17 20:39:17,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742103_1279, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742105_1281 src: /192.168.56.101:38408 dest: /192.168.56.102:50010
2019-08-17 20:39:17,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38408, dest: /192.168.56.102:50010, bytes: 4596, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742105_1281, duration: 1604677
2019-08-17 20:39:17,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742105_1281, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742107_1283 src: /192.168.56.101:38416 dest: /192.168.56.102:50010
2019-08-17 20:39:17,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38416, dest: /192.168.56.102:50010, bytes: 1045744, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742107_1283, duration: 10689047
2019-08-17 20:39:17,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742107_1283, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742108_1284 src: /192.168.56.101:38420 dest: /192.168.56.102:50010
2019-08-17 20:39:17,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38420, dest: /192.168.56.102:50010, bytes: 313702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742108_1284, duration: 3511642
2019-08-17 20:39:17,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742108_1284, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742109_1285 src: /192.168.56.101:38424 dest: /192.168.56.102:50010
2019-08-17 20:39:17,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38424, dest: /192.168.56.102:50010, bytes: 234201, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742109_1285, duration: 3577035
2019-08-17 20:39:17,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742109_1285, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742110_1286 src: /192.168.56.101:38428 dest: /192.168.56.102:50010
2019-08-17 20:39:17,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38428, dest: /192.168.56.102:50010, bytes: 489884, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742110_1286, duration: 6155563
2019-08-17 20:39:17,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742110_1286, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742111_1287 src: /192.168.56.101:38432 dest: /192.168.56.102:50010
2019-08-17 20:39:17,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38432, dest: /192.168.56.102:50010, bytes: 236880, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742111_1287, duration: 3175432
2019-08-17 20:39:17,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742111_1287, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742112_1288 src: /192.168.56.101:38436 dest: /192.168.56.102:50010
2019-08-17 20:39:17,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38436, dest: /192.168.56.102:50010, bytes: 34603, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742112_1288, duration: 1767202
2019-08-17 20:39:17,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742112_1288, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742113_1289 src: /192.168.56.101:38440 dest: /192.168.56.102:50010
2019-08-17 20:39:17,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38440, dest: /192.168.56.102:50010, bytes: 3142, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742113_1289, duration: 1637723
2019-08-17 20:39:17,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742113_1289, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742114_1290 src: /192.168.56.101:38444 dest: /192.168.56.102:50010
2019-08-17 20:39:17,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38444, dest: /192.168.56.102:50010, bytes: 521157, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742114_1290, duration: 8406958
2019-08-17 20:39:17,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742114_1290, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742115_1291 src: /192.168.56.101:38448 dest: /192.168.56.102:50010
2019-08-17 20:39:17,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38448, dest: /192.168.56.102:50010, bytes: 3499285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742115_1291, duration: 26895678
2019-08-17 20:39:17,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742115_1291, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742118_1294 src: /192.168.56.101:38460 dest: /192.168.56.102:50010
2019-08-17 20:39:17,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38460, dest: /192.168.56.102:50010, bytes: 15827, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742118_1294, duration: 2412075
2019-08-17 20:39:17,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742118_1294, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742119_1295 src: /192.168.56.101:38464 dest: /192.168.56.102:50010
2019-08-17 20:39:17,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38464, dest: /192.168.56.102:50010, bytes: 39280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742119_1295, duration: 1802625
2019-08-17 20:39:17,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742119_1295, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742120_1296 src: /192.168.56.101:38468 dest: /192.168.56.102:50010
2019-08-17 20:39:17,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38468, dest: /192.168.56.102:50010, bytes: 5711, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742120_1296, duration: 1393248
2019-08-17 20:39:17,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742120_1296, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742122_1298 src: /192.168.56.101:38476 dest: /192.168.56.102:50010
2019-08-17 20:39:17,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38476, dest: /192.168.56.102:50010, bytes: 1330219, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742122_1298, duration: 11826642
2019-08-17 20:39:17,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742122_1298, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742123_1299 src: /192.168.56.101:38480 dest: /192.168.56.102:50010
2019-08-17 20:39:17,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38480, dest: /192.168.56.102:50010, bytes: 2326492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742123_1299, duration: 23409094
2019-08-17 20:39:17,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742123_1299, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742124_1300 src: /192.168.56.101:38484 dest: /192.168.56.102:50010
2019-08-17 20:39:17,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38484, dest: /192.168.56.102:50010, bytes: 41755, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742124_1300, duration: 2212373
2019-08-17 20:39:17,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742124_1300, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:17,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742125_1301 src: /192.168.56.101:38488 dest: /192.168.56.102:50010
2019-08-17 20:39:17,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38488, dest: /192.168.56.102:50010, bytes: 19827, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742125_1301, duration: 1628924
2019-08-17 20:39:17,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742125_1301, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742128_1304 src: /192.168.56.101:38500 dest: /192.168.56.102:50010
2019-08-17 20:39:18,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38500, dest: /192.168.56.102:50010, bytes: 32806, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742128_1304, duration: 1846382
2019-08-17 20:39:18,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742128_1304, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742130_1306 src: /192.168.56.101:38508 dest: /192.168.56.102:50010
2019-08-17 20:39:18,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38508, dest: /192.168.56.102:50010, bytes: 42032, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742130_1306, duration: 7704214
2019-08-17 20:39:18,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742130_1306, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742132_1308 src: /192.168.56.101:38516 dest: /192.168.56.102:50010
2019-08-17 20:39:18,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38516, dest: /192.168.56.102:50010, bytes: 390733, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742132_1308, duration: 4696159
2019-08-17 20:39:18,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742132_1308, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742134_1310 src: /192.168.56.101:38524 dest: /192.168.56.102:50010
2019-08-17 20:39:18,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38524, dest: /192.168.56.102:50010, bytes: 2796935, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742134_1310, duration: 20804749
2019-08-17 20:39:18,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742134_1310, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742135_1311 src: /192.168.56.101:38528 dest: /192.168.56.102:50010
2019-08-17 20:39:18,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38528, dest: /192.168.56.102:50010, bytes: 1048116, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742135_1311, duration: 7593208
2019-08-17 20:39:18,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742135_1311, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742138_1314 src: /192.168.56.101:38540 dest: /192.168.56.102:50010
2019-08-17 20:39:18,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38540, dest: /192.168.56.102:50010, bytes: 533455, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742138_1314, duration: 6458504
2019-08-17 20:39:18,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742138_1314, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742139_1315 src: /192.168.56.101:38544 dest: /192.168.56.102:50010
2019-08-17 20:39:18,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38544, dest: /192.168.56.102:50010, bytes: 115472, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742139_1315, duration: 2382145
2019-08-17 20:39:18,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742139_1315, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742142_1318 src: /192.168.56.101:38556 dest: /192.168.56.102:50010
2019-08-17 20:39:18,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38556, dest: /192.168.56.102:50010, bytes: 5744974, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742142_1318, duration: 78998284
2019-08-17 20:39:18,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742142_1318, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742144_1320 src: /192.168.56.101:38564 dest: /192.168.56.102:50010
2019-08-17 20:39:18,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38564, dest: /192.168.56.102:50010, bytes: 4573750, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742144_1320, duration: 43107824
2019-08-17 20:39:18,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742144_1320, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742148_1324 src: /192.168.56.101:38580 dest: /192.168.56.102:50010
2019-08-17 20:39:18,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38580, dest: /192.168.56.102:50010, bytes: 40509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742148_1324, duration: 1962772
2019-08-17 20:39:18,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742148_1324, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:18,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742149_1325 src: /192.168.56.101:38584 dest: /192.168.56.102:50010
2019-08-17 20:39:18,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38584, dest: /192.168.56.102:50010, bytes: 9939, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742149_1325, duration: 7221268
2019-08-17 20:39:18,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742149_1325, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742154_1330 src: /192.168.56.101:38604 dest: /192.168.56.102:50010
2019-08-17 20:39:19,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38604, dest: /192.168.56.102:50010, bytes: 708129, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742154_1330, duration: 9711440
2019-08-17 20:39:19,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742154_1330, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742157_1333 src: /192.168.56.101:38616 dest: /192.168.56.102:50010
2019-08-17 20:39:19,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38616, dest: /192.168.56.102:50010, bytes: 66840, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742157_1333, duration: 1586954
2019-08-17 20:39:19,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742157_1333, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742161_1337 src: /192.168.56.101:38632 dest: /192.168.56.102:50010
2019-08-17 20:39:19,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38632, dest: /192.168.56.102:50010, bytes: 2375041, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742161_1337, duration: 19349977
2019-08-17 20:39:19,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742161_1337, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742162_1338 src: /192.168.56.101:38636 dest: /192.168.56.102:50010
2019-08-17 20:39:19,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38636, dest: /192.168.56.102:50010, bytes: 62656, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742162_1338, duration: 2445498
2019-08-17 20:39:19,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742162_1338, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742164_1340 src: /192.168.56.101:38644 dest: /192.168.56.102:50010
2019-08-17 20:39:19,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38644, dest: /192.168.56.102:50010, bytes: 30003, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742164_1340, duration: 8623450
2019-08-17 20:39:19,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742164_1340, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742165_1341 src: /192.168.56.101:38648 dest: /192.168.56.102:50010
2019-08-17 20:39:19,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38648, dest: /192.168.56.102:50010, bytes: 7146762, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742165_1341, duration: 67374777
2019-08-17 20:39:19,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742165_1341, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742167_1343 src: /192.168.56.101:38656 dest: /192.168.56.102:50010
2019-08-17 20:39:19,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38656, dest: /192.168.56.102:50010, bytes: 15461, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742167_1343, duration: 1232988
2019-08-17 20:39:19,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742167_1343, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742168_1344 src: /192.168.56.101:38660 dest: /192.168.56.102:50010
2019-08-17 20:39:19,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38660, dest: /192.168.56.102:50010, bytes: 46580, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742168_1344, duration: 2383571
2019-08-17 20:39:19,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742168_1344, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742169_1345 src: /192.168.56.101:38664 dest: /192.168.56.102:50010
2019-08-17 20:39:19,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38664, dest: /192.168.56.102:50010, bytes: 699095, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742169_1345, duration: 9179705
2019-08-17 20:39:19,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742169_1345, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742170_1346 src: /192.168.56.101:38668 dest: /192.168.56.102:50010
2019-08-17 20:39:19,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38668, dest: /192.168.56.102:50010, bytes: 87192, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742170_1346, duration: 2432084
2019-08-17 20:39:19,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742170_1346, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742171_1347 src: /192.168.56.101:38672 dest: /192.168.56.102:50010
2019-08-17 20:39:19,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38672, dest: /192.168.56.102:50010, bytes: 10121868, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742171_1347, duration: 100872400
2019-08-17 20:39:19,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742171_1347, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742172_1348 src: /192.168.56.101:38676 dest: /192.168.56.102:50010
2019-08-17 20:39:19,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38676, dest: /192.168.56.102:50010, bytes: 23346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742172_1348, duration: 1053302
2019-08-17 20:39:19,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742172_1348, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742174_1350 src: /192.168.56.101:38684 dest: /192.168.56.102:50010
2019-08-17 20:39:19,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38684, dest: /192.168.56.102:50010, bytes: 174351, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742174_1350, duration: 2525455
2019-08-17 20:39:19,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742174_1350, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742175_1351 src: /192.168.56.101:38688 dest: /192.168.56.102:50010
2019-08-17 20:39:19,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38688, dest: /192.168.56.102:50010, bytes: 148627, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742175_1351, duration: 6661670
2019-08-17 20:39:19,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742175_1351, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742176_1352 src: /192.168.56.101:38692 dest: /192.168.56.102:50010
2019-08-17 20:39:19,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38692, dest: /192.168.56.102:50010, bytes: 93210, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742176_1352, duration: 3981767
2019-08-17 20:39:19,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742176_1352, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742180_1356 src: /192.168.56.101:38708 dest: /192.168.56.102:50010
2019-08-17 20:39:19,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38708, dest: /192.168.56.102:50010, bytes: 1229125, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742180_1356, duration: 12127585
2019-08-17 20:39:19,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742180_1356, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:19,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742182_1358 src: /192.168.56.101:38716 dest: /192.168.56.102:50010
2019-08-17 20:39:19,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38716, dest: /192.168.56.102:50010, bytes: 94672, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1757465562_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742182_1358, duration: 2007333
2019-08-17 20:39:19,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742182_1358, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:39:22,284 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742081_1257
2019-08-17 20:39:27,298 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742157_1333
2019-08-17 20:40:58,841 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742080_1256 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742080 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742081_1257 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742081 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742082_1258 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742082 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742080_1256 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742080
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742083_1259 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742083 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742084_1260 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742084 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742087_1263 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742087 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742081_1257 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742081
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742088_1264 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742088 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742091_1267 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742091 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742082_1258 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742082
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742092_1268 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742092 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742083_1259 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742083
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742093_1269 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742093 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742094_1270 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742094 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742095_1271 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742095 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742084_1260 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742084
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742097_1273 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742097 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742087_1263 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742087
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742098_1274 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742098 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742099_1275 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742099 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742088_1264 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742088
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742103_1279 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742103 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742105_1281 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742105 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742107_1283 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742107 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742091_1267 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742091
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742108_1284 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742108 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742109_1285 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742109 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742110_1286 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742110 for deletion
2019-08-17 20:40:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742111_1287 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742111 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742112_1288 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742112 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742113_1289 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742113 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742114_1290 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742114 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742092_1268 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742092
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742115_1291 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742115 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742118_1294 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742118 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742093_1269 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742093
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742119_1295 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742119 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742120_1296 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742120 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742094_1270 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742094
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742122_1298 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742122 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742123_1299 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742123 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742095_1271 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742095
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742124_1300 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742124 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742125_1301 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742125 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742097_1273 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742097
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742128_1304 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742128 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742098_1274 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742098
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742130_1306 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742130 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742132_1308 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742132 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742099_1275 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742099
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742134_1310 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742134 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742103_1279 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742103
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742135_1311 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742135 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742105_1281 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742105
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742138_1314 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742138 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742139_1315 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742139 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742142_1318 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742142 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742144_1320 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742144 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742107_1283 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742107
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742148_1324 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742148 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742149_1325 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742149 for deletion
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742108_1284 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742108
2019-08-17 20:40:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742154_1330 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742154 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742157_1333 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742157 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742109_1285 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742109
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742161_1337 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742161 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742162_1338 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742162 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742164_1340 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742164 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742110_1286 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742110
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742165_1341 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742165 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742167_1343 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742167 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742111_1287 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742111
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742168_1344 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742168 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742112_1288 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742112
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742169_1345 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742169 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742113_1289 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742113
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742170_1346 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742170 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742171_1347 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742171 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742172_1348 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742172 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742114_1290 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742114
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742174_1350 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742174 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742175_1351 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742175 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742176_1352 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742176 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742180_1356 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742180 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742182_1358 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742182 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741976_1152 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741976 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741977_1153 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741977 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741979_1155 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741979 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741980_1156 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741980 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741981_1157 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741981 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742115_1291 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742115
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741983_1159 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741983 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742118_1294 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742118
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741984_1160 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741984 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741985_1161 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741985 for deletion
2019-08-17 20:40:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742119_1295 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742119
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741986_1162 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741986 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742120_1296 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742120
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741990_1166 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741990 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741991_1167 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741991 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741992_1168 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741992 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741994_1170 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741994 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741995_1171 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741995 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742122_1298 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742122
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741997_1173 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741997 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741999_1175 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741999 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742002_1178 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742002 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742003_1179 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742003 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742005_1181 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742005 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742006_1182 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742006 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742123_1299 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742123
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742010_1186 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742010 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742014_1190 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742014 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742124_1300 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742124
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742017_1193 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742017 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742125_1301 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742125
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742018_1194 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742018 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742128_1304 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742128
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742020_1196 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742020 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742130_1306 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742130
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742023_1199 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742023 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742024_1200 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742024 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742132_1308 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742132
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742025_1201 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742025 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742026_1202 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742026 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742028_1204 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742028 for deletion
2019-08-17 20:40:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742029_1205 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742029 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742030_1206 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742030 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742035_1211 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742035 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742037_1213 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742037 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742038_1214 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742038 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742134_1310 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742134
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742039_1215 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742039 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742040_1216 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742040 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742042_1218 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742042 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742044_1220 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742044 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742135_1311 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742135
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742047_1223 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742047 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742050_1226 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742050 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742051_1227 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742051 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742138_1314 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742138
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742057_1233 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742057 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742139_1315 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742139
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742059_1235 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742059 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742060_1236 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742060 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742061_1237 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742061 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742064_1240 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742064 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742066_1242 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742066 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742068_1244 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742068 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742070_1246 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742070 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742071_1247 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742071 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742072_1248 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742072 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742073_1249 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742073 for deletion
2019-08-17 20:40:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742074_1250 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742074 for deletion
2019-08-17 20:40:58,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742075_1251 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742075 for deletion
2019-08-17 20:40:58,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742078_1254 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742078 for deletion
2019-08-17 20:40:58,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742142_1318 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742142
2019-08-17 20:40:58,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742144_1320 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742144
2019-08-17 20:40:58,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742148_1324 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742148
2019-08-17 20:40:58,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742149_1325 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742149
2019-08-17 20:40:58,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742154_1330 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742154
2019-08-17 20:40:58,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742157_1333 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742157
2019-08-17 20:40:58,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742161_1337 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742161
2019-08-17 20:40:58,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742162_1338 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742162
2019-08-17 20:40:58,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742164_1340 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742164
2019-08-17 20:40:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742165_1341 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742165
2019-08-17 20:40:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742167_1343 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742167
2019-08-17 20:40:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742168_1344 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742168
2019-08-17 20:40:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742169_1345 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742169
2019-08-17 20:40:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742170_1346 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742170
2019-08-17 20:40:58,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742171_1347 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742171
2019-08-17 20:40:58,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742172_1348 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742172
2019-08-17 20:40:58,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742174_1350 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742174
2019-08-17 20:40:58,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742175_1351 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742175
2019-08-17 20:40:58,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742176_1352 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742176
2019-08-17 20:40:58,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742180_1356 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742180
2019-08-17 20:40:58,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742182_1358 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742182
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741976_1152 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741976
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741977_1153 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741977
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741979_1155 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741979
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741980_1156 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741980
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741981_1157 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741981
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741983_1159 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741983
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741984_1160 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741984
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741985_1161 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741985
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741986_1162 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741986
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741990_1166 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741990
2019-08-17 20:40:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741991_1167 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741991
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741992_1168 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741992
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741994_1170 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741994
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741995_1171 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741995
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741997_1173 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741997
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741999_1175 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741999
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742002_1178 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742002
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742003_1179 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742003
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742005_1181 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742005
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742006_1182 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742006
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742010_1186 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742010
2019-08-17 20:40:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742014_1190 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742014
2019-08-17 20:40:58,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742017_1193 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742017
2019-08-17 20:40:58,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742018_1194 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742018
2019-08-17 20:40:58,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742020_1196 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742020
2019-08-17 20:40:58,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742023_1199 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742023
2019-08-17 20:40:58,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742024_1200 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742024
2019-08-17 20:40:58,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742025_1201 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742025
2019-08-17 20:40:58,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742026_1202 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742026
2019-08-17 20:40:58,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742028_1204 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742028
2019-08-17 20:40:58,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742029_1205 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742029
2019-08-17 20:40:58,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742030_1206 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742030
2019-08-17 20:40:58,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742035_1211 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742035
2019-08-17 20:40:58,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742037_1213 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742037
2019-08-17 20:40:58,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742038_1214 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742038
2019-08-17 20:40:58,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742039_1215 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742039
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742040_1216 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742040
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742042_1218 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742042
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742044_1220 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742044
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742047_1223 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742047
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742050_1226 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742050
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742051_1227 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742051
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742057_1233 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742057
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742059_1235 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742059
2019-08-17 20:40:58,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742060_1236 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742060
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742061_1237 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742061
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742064_1240 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742064
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742066_1242 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742066
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742068_1244 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742068
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742070_1246 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742070
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742071_1247 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742071
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742072_1248 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742072
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742073_1249 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742073
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742074_1250 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742074
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742075_1251 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742075
2019-08-17 20:40:58,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742078_1254 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073742078
2019-08-17 20:42:18,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742188_1364 src: /192.168.56.102:44994 dest: /192.168.56.102:50010
2019-08-17 20:42:18,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:44994, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1608492730_143, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742188_1364, duration: 3672225
2019-08-17 20:42:18,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742188_1364, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 20:46:07,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742190_1366 src: /192.168.56.101:38864 dest: /192.168.56.102:50010
2019-08-17 20:46:07,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:38864, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_155281240_200, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742190_1366, duration: 1821207
2019-08-17 20:46:07,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742190_1366, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:46:07,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742191_1367 src: /192.168.56.103:55238 dest: /192.168.56.102:50010
2019-08-17 20:46:07,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:55238, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1377076322_198, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742191_1367, duration: 2250085
2019-08-17 20:46:07,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742191_1367, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:47:02,041 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-17 20:47:06,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 20:47:06,342 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-17 20:47:06,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-17 20:56:27,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 20:56:27,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 20:56:27,878 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 20:56:28,372 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 20:56:28,541 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 20:56:28,541 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 20:56:28,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 20:56:28,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 20:56:28,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 20:56:28,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 20:56:28,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 20:56:28,721 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 20:56:28,724 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 20:56:28,733 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 20:56:28,736 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 20:56:28,737 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 20:56:28,737 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 20:56:28,752 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 20:56:28,755 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 20:56:28,755 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 20:56:29,122 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 20:56:29,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 20:56:29,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 20:56:29,668 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 20:56:29,685 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 20:56:29,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 20:56:29,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 20:56:29,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 20:56:29,803 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 20:56:29,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 20:56:29,830 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 20:56:29,832 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 20:56:30,583 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 2553@um2
2019-08-17 20:56:30,697 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:56:30,697 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:56:30,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 20:56:30,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 20:56:30,716 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 20:56:30,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 20:56:30,753 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 20:56:30,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 20:56:30,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:56:30,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 20:56:30,833 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 34ms
2019-08-17 20:56:30,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 36ms
2019-08-17 20:56:30,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 20:56:30,858 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 23ms
2019-08-17 20:56:30,858 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 24ms
2019-08-17 20:56:30,862 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566079167862 with interval 21600000
2019-08-17 20:56:30,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 20:56:30,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 20:56:30,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 20:56:31,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=2546
2019-08-17 20:56:31,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 20:56:31,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6b0ad2000d,  containing 1 storage report(s), of which we sent 1. The reports had 92 total blocks and used 1 RPC(s). This took 7 msec to generate and 183 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 20:56:31,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:56:31,346 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 20:56:31,346 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 20:56:31,348 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 20:56:31,349 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 20:56:31,349 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 20:56:31,360 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 20:56:35,928 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741918_1094
2019-08-17 20:57:50,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742192_1368 src: /192.168.56.101:41026 dest: /192.168.56.102:50010
2019-08-17 20:57:50,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41026, dest: /192.168.56.102:50010, bytes: 83456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_544277370_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742192_1368, duration: 92954530
2019-08-17 20:57:50,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742192_1368, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:58:02,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742193_1369 src: /192.168.56.103:58598 dest: /192.168.56.102:50010
2019-08-17 20:58:02,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:58598, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_947957373_79, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742193_1369, duration: 93604672
2019-08-17 20:58:02,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742193_1369, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 20:58:07,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742192_1368 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742192 for deletion
2019-08-17 20:58:07,047 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742192_1368 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742192
2019-08-17 21:01:01,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742195_1371 src: /192.168.56.101:41102 dest: /192.168.56.102:50010
2019-08-17 21:01:01,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41102, dest: /192.168.56.102:50010, bytes: 83456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1407276194_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742195_1371, duration: 29581464
2019-08-17 21:01:01,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742195_1371, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:01:04,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742196_1372 src: /192.168.56.103:58612 dest: /192.168.56.102:50010
2019-08-17 21:01:04,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:58612, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1451632579_113, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742196_1372, duration: 2815380
2019-08-17 21:01:04,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742196_1372, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:01:04,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742197_1373 src: /192.168.56.101:41120 dest: /192.168.56.102:50010
2019-08-17 21:01:04,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41120, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_750951700_113, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742197_1373, duration: 18400744
2019-08-17 21:01:04,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742197_1373, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:01:07,301 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742195_1371 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742195 for deletion
2019-08-17 21:01:07,301 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742195_1371 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742195
2019-08-17 21:01:23,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742198_1374 src: /192.168.56.101:41128 dest: /192.168.56.102:50010
2019-08-17 21:01:23,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41128, dest: /192.168.56.102:50010, bytes: 83456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_985053335_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742198_1374, duration: 41387487
2019-08-17 21:01:23,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742198_1374, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:01:29,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742199_1375 src: /192.168.56.101:41146 dest: /192.168.56.102:50010
2019-08-17 21:01:29,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41146, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-297864732_113, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742199_1375, duration: 3334627
2019-08-17 21:01:29,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742199_1375, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:01:30,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742200_1376 src: /192.168.56.102:50988 dest: /192.168.56.102:50010
2019-08-17 21:01:30,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:50988, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-234944186_79, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742200_1376, duration: 35020874
2019-08-17 21:01:30,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742200_1376, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 21:01:34,316 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742198_1374 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742198 for deletion
2019-08-17 21:01:34,316 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742198_1374 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742198
2019-08-17 21:10:36,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742202_1378 src: /192.168.56.101:41252 dest: /192.168.56.102:50010
2019-08-17 21:10:36,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41252, dest: /192.168.56.102:50010, bytes: 399, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1510798395_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742202_1378, duration: 11789292
2019-08-17 21:10:36,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742202_1378, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 21:10:39,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742201_1377 src: /192.168.56.101:41264 dest: /192.168.56.102:50010
2019-08-17 21:10:39,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742201_1377 src: /192.168.56.101:41264 dest: /192.168.56.102:50010 of size 292710
2019-08-17 21:15:59,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-17 21:16:03,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 21:16:04,289 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-17 21:16:04,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-17 21:19:26,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 21:19:26,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 21:19:26,678 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 21:19:27,090 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 21:19:27,174 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 21:19:27,174 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 21:19:27,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 21:19:27,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 21:19:27,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 21:19:27,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 21:19:27,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 21:19:29,203 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 21:19:29,206 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 21:19:29,217 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 21:19:29,220 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 21:19:29,220 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 21:19:29,220 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 21:19:29,248 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 21:19:29,250 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 21:19:29,251 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 21:19:29,517 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 21:19:29,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 21:19:29,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 21:19:29,958 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 21:19:29,975 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 21:19:30,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 21:19:30,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 21:19:30,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 21:19:30,049 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 21:19:30,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 21:19:30,057 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 21:19:30,061 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 21:19:30,447 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3435@um2
2019-08-17 21:19:30,539 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:19:30,539 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:19:30,539 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 21:19:30,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 21:19:30,551 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 21:19:30,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 21:19:30,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 21:19:30,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 21:19:30,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:19:30,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 21:19:30,631 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 212255223
2019-08-17 21:19:30,632 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 12ms
2019-08-17 21:19:30,633 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 13ms
2019-08-17 21:19:30,633 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 21:19:30,652 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 18ms
2019-08-17 21:19:30,652 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 19ms
2019-08-17 21:19:30,654 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566086845654 with interval 21600000
2019-08-17 21:19:30,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 21:19:30,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 21:19:30,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 21:19:30,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=2649
2019-08-17 21:19:30,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 21:19:30,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1ac48536707,  containing 1 storage report(s), of which we sent 1. The reports had 99 total blocks and used 1 RPC(s). This took 2 msec to generate and 119 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 21:19:30,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:19:30,991 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 21:19:30,991 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 21:19:30,991 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 21:19:30,991 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 21:19:30,992 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:19:31,000 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 21:19:35,755 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741830_1006
2019-08-17 21:20:10,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742206_1382 src: /192.168.56.103:58838 dest: /192.168.56.102:50010
2019-08-17 21:20:10,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:58838, dest: /192.168.56.102:50010, bytes: 399, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1703051498_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742206_1382, duration: 34846421
2019-08-17 21:20:10,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742206_1382, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:10,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742207_1383 src: /192.168.56.101:41494 dest: /192.168.56.102:50010
2019-08-17 21:20:10,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41494, dest: /192.168.56.102:50010, bytes: 62, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1703051498_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742207_1383, duration: 4403124
2019-08-17 21:20:10,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742207_1383, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:10,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742208_1384 src: /192.168.56.101:41498 dest: /192.168.56.102:50010
2019-08-17 21:20:10,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41498, dest: /192.168.56.102:50010, bytes: 88714, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1703051498_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742208_1384, duration: 9038734
2019-08-17 21:20:10,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742208_1384, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:15,820 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742206_1382
2019-08-17 21:20:16,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742209_1385 src: /192.168.56.101:41520 dest: /192.168.56.102:50010
2019-08-17 21:20:16,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41520, dest: /192.168.56.102:50010, bytes: 105500, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1050378009_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742209_1385, duration: 21617942
2019-08-17 21:20:16,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742209_1385, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:18,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742205_1381 src: /192.168.56.103:58840 dest: /192.168.56.102:50010
2019-08-17 21:20:18,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742205_1381 src: /192.168.56.103:58840 dest: /192.168.56.102:50010 of size 292710
2019-08-17 21:20:20,829 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742208_1384
2019-08-17 21:20:22,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742210_1386 src: /192.168.56.101:41542 dest: /192.168.56.102:50010
2019-08-17 21:20:25,839 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742205_1381
2019-08-17 21:20:27,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742211_1387 src: /192.168.56.102:51056 dest: /192.168.56.102:50010
2019-08-17 21:20:27,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:51056, dest: /192.168.56.102:50010, bytes: 3113, op: HDFS_WRITE, cliID: DFSClient_attempt_1566069580403_0001_r_000000_0_1155384816_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742211_1387, duration: 58404416
2019-08-17 21:20:27,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742211_1387, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 21:20:28,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41542, dest: /192.168.56.102:50010, bytes: 53499, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1050378009_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742210_1386, duration: 5975918323
2019-08-17 21:20:28,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742210_1386, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:28,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742213_1389 src: /192.168.56.101:41574 dest: /192.168.56.102:50010
2019-08-17 21:20:28,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41574, dest: /192.168.56.102:50010, bytes: 53499, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1050378009_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742213_1389, duration: 9862508
2019-08-17 21:20:28,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742213_1389, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:35,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742215_1391 src: /192.168.56.102:51064 dest: /192.168.56.102:50010
2019-08-17 21:20:35,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:51064, dest: /192.168.56.102:50010, bytes: 11566, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-144820749_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742215_1391, duration: 23938647
2019-08-17 21:20:35,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742215_1391, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 21:20:35,866 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742210_1386
2019-08-17 21:20:36,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742216_1392 src: /192.168.56.101:41586 dest: /192.168.56.102:50010
2019-08-17 21:20:36,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41586, dest: /192.168.56.102:50010, bytes: 69584, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1107806593_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742216_1392, duration: 27403281
2019-08-17 21:20:36,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742216_1392, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:36,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742208_1384 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742208 for deletion
2019-08-17 21:20:36,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742209_1385 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742209 for deletion
2019-08-17 21:20:36,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742210_1386 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742210 for deletion
2019-08-17 21:20:36,811 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742205_1381 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742205 for deletion
2019-08-17 21:20:36,811 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742206_1382 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742206 for deletion
2019-08-17 21:20:36,811 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742207_1383 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742207 for deletion
2019-08-17 21:20:36,811 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742208_1384 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742208
2019-08-17 21:20:36,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742209_1385 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742209
2019-08-17 21:20:36,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742210_1386 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742210
2019-08-17 21:20:36,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742205_1381 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742205
2019-08-17 21:20:36,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742206_1382 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742206
2019-08-17 21:20:36,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742207_1383 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742207
2019-08-17 21:20:40,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742217_1393 src: /192.168.56.101:41594 dest: /192.168.56.102:50010
2019-08-17 21:20:40,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41594, dest: /192.168.56.102:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1492337851_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742217_1393, duration: 37715556
2019-08-17 21:20:40,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742217_1393, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:40,883 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742215_1391
2019-08-17 21:20:40,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742218_1394 src: /192.168.56.101:41598 dest: /192.168.56.102:50010
2019-08-17 21:20:40,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41598, dest: /192.168.56.102:50010, bytes: 399, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1492337851_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742218_1394, duration: 8660235
2019-08-17 21:20:40,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742218_1394, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 21:20:40,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742219_1395 src: /192.168.56.101:41602 dest: /192.168.56.102:50010
2019-08-17 21:20:40,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41602, dest: /192.168.56.102:50010, bytes: 62, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1492337851_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742219_1395, duration: 3398345
2019-08-17 21:20:40,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742219_1395, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:20:45,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742217_1393 to 192.168.56.103:50010 
2019-08-17 21:20:45,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742217_1393 (numBytes=292710) to /192.168.56.103:50010
2019-08-17 21:20:45,896 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742216_1392
2019-08-17 21:20:50,920 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742219_1395
2019-08-17 21:22:08,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742222_1398 src: /192.168.56.101:41618 dest: /192.168.56.102:50010
2019-08-17 21:22:09,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41618, dest: /192.168.56.102:50010, bytes: 83643, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_744343860_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742222_1398, duration: 60470751
2019-08-17 21:22:09,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742222_1398, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:22:14,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742223_1399 src: /192.168.56.101:41638 dest: /192.168.56.102:50010
2019-08-17 21:22:14,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41638, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-513709567_157, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742223_1399, duration: 4560595
2019-08-17 21:22:14,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742223_1399, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:22:15,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742224_1400 src: /192.168.56.103:58918 dest: /192.168.56.102:50010
2019-08-17 21:22:15,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:58918, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1702640638_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742224_1400, duration: 6169897
2019-08-17 21:22:15,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742224_1400, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:22:16,038 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742222_1398
2019-08-17 21:22:18,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742222_1398 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742222 for deletion
2019-08-17 21:22:18,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742222_1398 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742222
2019-08-17 21:22:21,057 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742223_1399
2019-08-17 21:25:23,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742226_1402 src: /192.168.56.103:58932 dest: /192.168.56.102:50010
2019-08-17 21:25:23,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:58932, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2018244021_159, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742226_1402, duration: 10444492
2019-08-17 21:25:23,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742226_1402, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:25:24,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742227_1403 src: /192.168.56.101:41720 dest: /192.168.56.102:50010
2019-08-17 21:25:24,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41720, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1353042462_187, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742227_1403, duration: 5937801
2019-08-17 21:25:24,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742227_1403, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:25:31,221 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742227_1403
2019-08-17 21:42:01,582 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-17 21:42:05,341 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-17 21:42:05,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-17 21:42:45,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 21:42:45,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 21:42:46,131 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 21:42:46,510 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 21:42:46,618 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 21:42:46,618 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 21:42:46,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 21:42:46,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 21:42:46,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 21:42:46,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 21:42:46,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 21:42:46,750 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 21:42:46,753 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 21:42:46,764 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 21:42:46,768 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 21:42:46,768 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 21:42:46,769 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 21:42:46,784 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 21:42:46,787 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 21:42:46,787 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 21:42:47,073 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 21:42:47,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 21:42:47,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 21:42:47,466 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 21:42:47,481 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 21:42:47,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 21:42:47,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 21:42:47,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 21:42:47,556 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 21:42:47,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 21:42:47,565 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 21:42:47,567 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 21:42:48,156 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 4480@um2
2019-08-17 21:42:48,249 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:42:48,250 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:42:48,250 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 21:42:48,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 21:42:48,264 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 21:42:48,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 21:42:48,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 21:42:48,322 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 21:42:48,322 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:42:48,322 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 21:42:48,329 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 212779008
2019-08-17 21:42:48,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 10ms
2019-08-17 21:42:48,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 11ms
2019-08-17 21:42:48,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 21:42:48,345 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 11ms
2019-08-17 21:42:48,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 13ms
2019-08-17 21:42:48,349 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566071407349 with interval 21600000
2019-08-17 21:42:48,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 21:42:48,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 21:42:48,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 21:42:48,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=2832
2019-08-17 21:42:48,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 21:42:48,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2f1adc38975,  containing 1 storage report(s), of which we sent 1. The reports had 110 total blocks and used 1 RPC(s). This took 2 msec to generate and 70 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 21:42:48,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:42:48,508 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 21:42:48,508 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 21:42:48,509 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 21:42:48,509 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 21:42:48,510 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 21:42:48,516 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 21:42:53,375 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741866_1042
2019-08-17 21:44:12,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742229_1405 src: /192.168.56.103:58954 dest: /192.168.56.102:50010
2019-08-17 21:44:12,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:58954, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1392199257_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742229_1405, duration: 36613288
2019-08-17 21:44:12,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742229_1405, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:44:12,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742230_1406 src: /192.168.56.102:51136 dest: /192.168.56.102:50010
2019-08-17 21:44:12,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:51136, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1363135066_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742230_1406, duration: 43633368
2019-08-17 21:44:12,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742230_1406, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 21:44:18,454 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742230_1406
2019-08-17 21:46:09,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742231_1407 src: /192.168.56.101:41864 dest: /192.168.56.102:50010
2019-08-17 21:46:10,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:41864, dest: /192.168.56.102:50010, bytes: 83598, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-746413345_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742231_1407, duration: 37099483
2019-08-17 21:46:10,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742231_1407, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 21:46:17,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742232_1408 src: /192.168.56.102:51154 dest: /192.168.56.102:50010
2019-08-17 21:46:17,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:51154, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1447172270_123, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742232_1408, duration: 10094946
2019-08-17 21:46:17,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742232_1408, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 21:46:18,575 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742231_1407
2019-08-17 21:46:21,475 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742231_1407 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742231 for deletion
2019-08-17 21:46:21,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742231_1407 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742231
2019-08-17 21:46:23,582 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073742232_1408
2019-08-17 21:50:07,399 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1619971864-192.168.56.101-1564664778252 Total blocks: 113, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-08-17 21:51:57,747 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-17 21:52:01,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 21:52:02,415 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-17 21:52:02,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-17 22:05:48,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 22:05:48,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 22:05:48,465 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:05:48,967 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 22:05:49,133 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 22:05:49,133 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 22:05:49,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 22:05:49,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 22:05:49,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 22:05:49,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 22:05:49,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 22:05:49,287 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 22:05:49,290 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 22:05:49,298 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 22:05:49,306 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 22:05:49,306 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 22:05:49,306 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 22:05:49,321 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 22:05:49,323 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 22:05:49,323 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 22:05:49,647 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 22:05:50,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 22:05:50,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 22:05:50,146 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 22:05:50,161 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 22:05:50,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 22:05:50,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 22:05:50,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 22:05:50,273 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:05:50,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 22:05:50,296 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 22:05:50,297 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 22:05:50,584 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 2546@um2
2019-08-17 22:05:50,650 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:05:50,650 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:05:50,651 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 22:05:50,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 22:05:50,666 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:05:50,697 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 22:05:50,697 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 22:05:50,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 22:05:50,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:05:50,747 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 22:05:50,780 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 33ms
2019-08-17 22:05:50,780 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 35ms
2019-08-17 22:05:50,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 22:05:50,807 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 24ms
2019-08-17 22:05:50,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 27ms
2019-08-17 22:05:50,812 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566075496812 with interval 21600000
2019-08-17 22:05:50,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 22:05:50,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 22:05:50,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 22:05:50,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=2885
2019-08-17 22:05:50,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 22:05:50,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1b54be82dd,  containing 1 storage report(s), of which we sent 1. The reports had 113 total blocks and used 1 RPC(s). This took 2 msec to generate and 38 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 22:05:50,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:05:50,939 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 22:05:50,939 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 22:05:50,941 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 22:05:50,941 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 22:05:50,941 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:05:50,947 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 22:05:55,844 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1619971864-192.168.56.101-1564664778252:blk_1073741825_1001
2019-08-17 22:09:35,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742235_1411 src: /192.168.56.102:49814 dest: /192.168.56.102:50010
2019-08-17 22:09:35,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:49814, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-373754353_79, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742235_1411, duration: 73672983
2019-08-17 22:09:35,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742235_1411, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 22:13:29,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742239_1415 src: /192.168.56.103:52580 dest: /192.168.56.102:50010
2019-08-17 22:13:29,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:52580, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1531180065_79, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742239_1415, duration: 32238926
2019-08-17 22:13:29,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742239_1415, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:20:42,512 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-17 22:20:46,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 22:20:47,117 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-17 22:20:47,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-17 22:21:25,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 22:21:25,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 22:21:25,997 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:21:26,420 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 22:21:26,518 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 22:21:26,518 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 22:21:26,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 22:21:26,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 22:21:26,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 22:21:26,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 22:21:26,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 22:21:26,613 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 22:21:26,618 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 22:21:26,629 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 22:21:26,631 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 22:21:26,631 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 22:21:26,632 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 22:21:26,645 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 22:21:26,647 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 22:21:26,647 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 22:21:26,905 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 22:21:27,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 22:21:27,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 22:21:27,331 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 22:21:27,348 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 22:21:27,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 22:21:27,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 22:21:27,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 22:21:27,402 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:21:27,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 22:21:27,409 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 22:21:27,413 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 22:21:27,752 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3336@um2
2019-08-17 22:21:27,810 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:21:27,810 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:21:27,811 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 22:21:27,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 22:21:27,823 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:21:27,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 22:21:27,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 22:21:27,894 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 22:21:27,894 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:21:27,894 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 22:21:27,916 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 212819968
2019-08-17 22:21:27,918 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 23ms
2019-08-17 22:21:27,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 28ms
2019-08-17 22:21:27,928 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 22:21:27,950 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 21ms
2019-08-17 22:21:27,950 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 28ms
2019-08-17 22:21:27,954 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566081692953 with interval 21600000
2019-08-17 22:21:27,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 22:21:28,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 22:21:28,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 22:21:28,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=2934
2019-08-17 22:21:28,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 22:21:28,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf58acd3d71,  containing 1 storage report(s), of which we sent 1. The reports had 115 total blocks and used 1 RPC(s). This took 2 msec to generate and 58 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 22:21:28,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:21:28,169 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 22:21:28,169 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 22:21:28,170 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 22:21:28,170 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 22:21:28,170 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:21:28,177 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 22:24:49,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742241_1417 src: /192.168.56.101:51456 dest: /192.168.56.102:50010
2019-08-17 22:24:49,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:51456, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1979470875_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742241_1417, duration: 65884470
2019-08-17 22:24:49,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742241_1417, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:24:49,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742242_1418 src: /192.168.56.103:52604 dest: /192.168.56.102:50010
2019-08-17 22:24:49,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.103:52604, dest: /192.168.56.102:50010, bytes: 440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-584968734_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742242_1418, duration: 24560641
2019-08-17 22:24:49,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742242_1418, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:25:31,233 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2019-08-17 22:25:31,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2019-08-17 22:25:31,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2019-08-17 22:25:31,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741856_1032 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741856
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741857_1033 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741857
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741845_1021 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741845
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741846_1022 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741846
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741878_1054 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741878
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741879_1055 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741879
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741895_1071 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741895
2019-08-17 22:25:31,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741896_1072 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741896 for deletion
2019-08-17 22:25:31,236 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741896_1072 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741896
2019-08-17 22:25:34,217 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-17 22:25:38,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 22:25:39,129 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-17 22:25:39,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-17 22:27:42,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 22:27:42,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 22:27:43,324 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:27:43,712 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 22:27:43,804 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 22:27:43,804 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 22:27:43,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 22:27:43,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 22:27:43,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 22:27:43,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 22:27:43,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 22:27:43,899 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 22:27:43,902 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 22:27:43,912 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 22:27:43,914 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 22:27:43,915 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 22:27:43,915 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 22:27:43,930 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 22:27:43,933 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 22:27:43,933 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 22:27:44,175 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 22:27:44,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 22:27:44,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 22:27:44,581 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 22:27:44,599 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 22:27:44,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 22:27:44,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 22:27:44,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 22:27:44,674 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:27:44,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 22:27:44,696 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 22:27:44,703 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 22:27:45,056 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3990@um2
2019-08-17 22:27:45,139 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:27:45,139 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:27:45,140 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 22:27:45,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 22:27:45,150 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:27:45,169 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 22:27:45,169 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 22:27:45,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 22:27:45,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:27:45,218 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 22:27:45,230 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 212272159
2019-08-17 22:27:45,231 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 14ms
2019-08-17 22:27:45,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 14ms
2019-08-17 22:27:45,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 22:27:45,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 26ms
2019-08-17 22:27:45,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 27ms
2019-08-17 22:27:45,262 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566081090262 with interval 21600000
2019-08-17 22:27:45,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 22:27:45,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 22:27:45,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 22:27:45,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=2976
2019-08-17 22:27:45,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 22:27:45,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x14d66fad547,  containing 1 storage report(s), of which we sent 1. The reports had 109 total blocks and used 1 RPC(s). This took 2 msec to generate and 90 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 22:27:45,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:27:45,558 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 22:27:45,558 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 22:27:45,559 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 22:27:45,559 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 22:27:45,560 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:27:45,570 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 22:28:46,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742243_1419 src: /192.168.56.101:55830 dest: /192.168.56.102:50010
2019-08-17 22:28:46,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:55830, dest: /192.168.56.102:50010, bytes: 83950, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_237158993_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742243_1419, duration: 100821673
2019-08-17 22:28:46,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742243_1419, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:29:03,464 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742243_1419 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742243 for deletion
2019-08-17 22:29:03,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742243_1419 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742243
2019-08-17 22:33:02,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742246_1422 src: /192.168.56.101:55998 dest: /192.168.56.102:50010
2019-08-17 22:33:04,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:55998, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-419809463_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742246_1422, duration: 1757260207
2019-08-17 22:33:04,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742246_1422, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:33:04,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742247_1423 src: /192.168.56.101:56002 dest: /192.168.56.102:50010
2019-08-17 22:33:05,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56002, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-419809463_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742247_1423, duration: 694295281
2019-08-17 22:33:05,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742247_1423, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:33:21,614 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742246_1422 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742246 for deletion
2019-08-17 22:33:21,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742247_1423 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742247 for deletion
2019-08-17 22:33:21,631 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742246_1422 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742246
2019-08-17 22:33:21,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742247_1423 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742247
2019-08-17 22:35:48,727 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.56.102"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-17 22:35:52,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.56.101:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-17 22:35:53,068 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-17 22:35:53,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.56.102
************************************************************/
2019-08-17 22:37:50,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.56.102
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-17 22:37:50,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-17 22:37:50,394 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:37:50,690 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-17 22:37:50,750 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-17 22:37:50,750 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-17 22:37:50,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-17 22:37:50,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-17 22:37:50,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-17 22:37:50,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-17 22:37:50,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-17 22:37:50,850 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-17 22:37:50,853 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-17 22:37:50,861 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-17 22:37:50,863 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-17 22:37:50,864 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-17 22:37:50,864 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-17 22:37:50,879 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-17 22:37:50,882 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-17 22:37:50,882 INFO org.mortbay.log: jetty-6.1.26
2019-08-17 22:37:51,099 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-17 22:37:51,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-17 22:37:51,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-17 22:37:51,434 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-17 22:37:51,446 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-17 22:37:51,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-17 22:37:51,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-17 22:37:51,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-17 22:37:51,496 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:37:51,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.56.101:9000 starting to offer service
2019-08-17 22:37:51,507 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-17 22:37:51,512 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-17 22:37:51,904 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 4840@um2
2019-08-17 22:37:52,002 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:37:52,002 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:37:52,002 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-17 22:37:52,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=558994093;bpid=BP-1619971864-192.168.56.101-1564664778252;lv=-56;nsInfo=lv=-60;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0;bpid=BP-1619971864-192.168.56.101-1564664778252;dnuuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b
2019-08-17 22:37:52,015 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-17 22:37:52,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-17 22:37:52,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-17 22:37:52,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-17 22:37:52,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:37:52,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 22:37:52,088 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current: 212272159
2019-08-17 22:37:52,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1619971864-192.168.56.101-1564664778252 on /abc/datanode/current: 12ms
2019-08-17 22:37:52,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1619971864-192.168.56.101-1564664778252: 14ms
2019-08-17 22:37:52,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current...
2019-08-17 22:37:52,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1619971864-192.168.56.101-1564664778252 on volume /abc/datanode/current: 18ms
2019-08-17 22:37:52,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 20ms
2019-08-17 22:37:52,115 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566094892115 with interval 21600000
2019-08-17 22:37:52,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 beginning handshake with NN
2019-08-17 22:37:52,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid null) service to um1/192.168.56.101:9000 successfully registered with NN
2019-08-17 22:37:52,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.56.101:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-17 22:37:52,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000 trying to claim ACTIVE state with txid=3044
2019-08-17 22:37:52,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1619971864-192.168.56.101-1564664778252 (Datanode Uuid 47a99e3c-e551-40ca-acc8-80e4a48aa24b) service to um1/192.168.56.101:9000
2019-08-17 22:37:52,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1daadec48d3,  containing 1 storage report(s), of which we sent 1. The reports had 109 total blocks and used 1 RPC(s). This took 2 msec to generate and 78 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-17 22:37:52,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:37:52,326 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-17 22:37:52,326 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-17 22:37:52,327 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-17 22:37:52,327 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-17 22:37:52,327 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1619971864-192.168.56.101-1564664778252
2019-08-17 22:37:52,337 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1619971864-192.168.56.101-1564664778252 to blockPoolScannerMap, new size=1
2019-08-17 22:38:25,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741952_1128 src: /192.168.56.101:56128 dest: /192.168.56.102:50010
2019-08-17 22:38:25,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742251_1427 src: /192.168.56.101:56126 dest: /192.168.56.102:50010
2019-08-17 22:38:25,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073741952_1128 src: /192.168.56.101:56128 dest: /192.168.56.102:50010 of size 18431
2019-08-17 22:38:25,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742251_1427 src: /192.168.56.101:56126 dest: /192.168.56.102:50010 of size 5078
2019-08-17 22:38:28,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741966_1142 to 192.168.56.101:50010 
2019-08-17 22:38:28,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741967_1143 to 192.168.56.101:50010 
2019-08-17 22:38:28,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741959_1135 src: /192.168.56.101:56130 dest: /192.168.56.102:50010
2019-08-17 22:38:28,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073741959_1135 src: /192.168.56.101:56130 dest: /192.168.56.102:50010 of size 5079
2019-08-17 22:38:28,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073741960_1136 src: /192.168.56.101:56132 dest: /192.168.56.102:50010
2019-08-17 22:38:28,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073741960_1136 src: /192.168.56.101:56132 dest: /192.168.56.102:50010 of size 18470
2019-08-17 22:38:28,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741966_1142 (numBytes=5705) to /192.168.56.101:50010
2019-08-17 22:38:28,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741967_1143 (numBytes=6006) to /192.168.56.101:50010
2019-08-17 22:38:31,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073741975_1151 to 192.168.56.101:50010 
2019-08-17 22:38:31,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742191_1367 to 192.168.56.101:50010 
2019-08-17 22:38:31,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073741975_1151 (numBytes=2476) to /192.168.56.101:50010
2019-08-17 22:38:31,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742191_1367 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:31,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742185_1361 src: /192.168.56.101:56136 dest: /192.168.56.102:50010
2019-08-17 22:38:31,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742184_1360 src: /192.168.56.101:56134 dest: /192.168.56.102:50010
2019-08-17 22:38:31,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742185_1361 src: /192.168.56.101:56136 dest: /192.168.56.102:50010 of size 50775239
2019-08-17 22:38:32,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742184_1360 src: /192.168.56.101:56134 dest: /192.168.56.102:50010 of size 134217728
2019-08-17 22:38:34,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742193_1369 to 192.168.56.101:50010 
2019-08-17 22:38:34,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742196_1372 to 192.168.56.101:50010 
2019-08-17 22:38:34,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742193_1369 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:34,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742196_1372 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:34,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742187_1363 src: /192.168.56.101:56138 dest: /192.168.56.102:50010
2019-08-17 22:38:34,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742194_1370 src: /192.168.56.101:56140 dest: /192.168.56.102:50010
2019-08-17 22:38:34,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742187_1363 src: /192.168.56.101:56138 dest: /192.168.56.102:50010 of size 440
2019-08-17 22:38:34,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742194_1370 src: /192.168.56.101:56140 dest: /192.168.56.102:50010 of size 440
2019-08-17 22:38:37,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742215_1391 to 192.168.56.101:50010 
2019-08-17 22:38:37,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742203_1379 src: /192.168.56.101:56142 dest: /192.168.56.102:50010
2019-08-17 22:38:37,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742203_1379 src: /192.168.56.101:56142 dest: /192.168.56.102:50010 of size 62
2019-08-17 22:38:37,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742215_1391 (numBytes=11566) to /192.168.56.101:50010
2019-08-17 22:38:37,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742204_1380 src: /192.168.56.101:56144 dest: /192.168.56.102:50010
2019-08-17 22:38:37,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742204_1380 src: /192.168.56.101:56144 dest: /192.168.56.102:50010 of size 88718
2019-08-17 22:38:40,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742224_1400 to 192.168.56.101:50010 
2019-08-17 22:38:40,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742226_1402 to 192.168.56.101:50010 
2019-08-17 22:38:40,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742224_1400 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:40,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742226_1402 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:40,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742220_1396 src: /192.168.56.101:56148 dest: /192.168.56.102:50010
2019-08-17 22:38:40,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742214_1390 src: /192.168.56.101:56146 dest: /192.168.56.102:50010
2019-08-17 22:38:40,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742214_1390 src: /192.168.56.101:56146 dest: /192.168.56.102:50010 of size 105500
2019-08-17 22:38:40,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742220_1396 src: /192.168.56.101:56148 dest: /192.168.56.102:50010 of size 88714
2019-08-17 22:38:43,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742229_1405 to 192.168.56.101:50010 
2019-08-17 22:38:43,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742232_1408 to 192.168.56.101:50010 
2019-08-17 22:38:43,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742229_1405 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:43,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742232_1408 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:43,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742233_1409 src: /192.168.56.101:56150 dest: /192.168.56.102:50010
2019-08-17 22:38:43,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742233_1409 src: /192.168.56.101:56150 dest: /192.168.56.102:50010 of size 440
2019-08-17 22:38:43,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742236_1412 src: /192.168.56.101:56152 dest: /192.168.56.102:50010
2019-08-17 22:38:43,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742236_1412 src: /192.168.56.101:56152 dest: /192.168.56.102:50010 of size 440
2019-08-17 22:38:46,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742239_1415 to 192.168.56.101:50010 
2019-08-17 22:38:46,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.56.102, datanodeUuid=47a99e3c-e551-40ca-acc8-80e4a48aa24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-ff859e50-9fdb-4cef-ad76-3ccb2ce2e9ee;nsid=558994093;c=0) Starting thread to transfer BP-1619971864-192.168.56.101-1564664778252:blk_1073742242_1418 to 192.168.56.101:50010 
2019-08-17 22:38:46,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742242_1418 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:46,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742238_1414 src: /192.168.56.101:56154 dest: /192.168.56.102:50010
2019-08-17 22:38:46,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742238_1414 src: /192.168.56.101:56154 dest: /192.168.56.102:50010 of size 440
2019-08-17 22:38:46,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742244_1420 src: /192.168.56.101:56156 dest: /192.168.56.102:50010
2019-08-17 22:38:46,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1619971864-192.168.56.101-1564664778252:blk_1073742239_1415 (numBytes=440) to /192.168.56.101:50010
2019-08-17 22:38:46,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742244_1420 src: /192.168.56.101:56156 dest: /192.168.56.102:50010 of size 440
2019-08-17 22:38:49,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742245_1421 src: /192.168.56.101:56160 dest: /192.168.56.102:50010
2019-08-17 22:38:49,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742245_1421 src: /192.168.56.101:56160 dest: /192.168.56.102:50010 of size 440
2019-08-17 22:38:49,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742249_1425 src: /192.168.56.101:56162 dest: /192.168.56.102:50010
2019-08-17 22:38:49,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742249_1425 src: /192.168.56.101:56162 dest: /192.168.56.102:50010 of size 18779
2019-08-17 22:38:52,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742250_1426 src: /192.168.56.101:56164 dest: /192.168.56.102:50010
2019-08-17 22:38:52,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742250_1426 src: /192.168.56.101:56164 dest: /192.168.56.102:50010 of size 5890
2019-08-17 22:39:04,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742221_1397 src: /192.168.56.101:56166 dest: /192.168.56.102:50010
2019-08-17 22:39:04,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1619971864-192.168.56.101-1564664778252:blk_1073742221_1397 src: /192.168.56.101:56166 dest: /192.168.56.102:50010 of size 13381
2019-08-17 22:39:49,210 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742211_1387 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742211 for deletion
2019-08-17 22:39:49,211 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2019-08-17 22:39:49,211 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2019-08-17 22:39:49,211 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2019-08-17 22:39:49,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742211_1387 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742211
2019-08-17 22:39:49,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741876_1052 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741876
2019-08-17 22:39:49,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741893_1069 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741893
2019-08-17 22:39:49,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073741854_1030 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir0/blk_1073741854
2019-08-17 22:40:03,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742252_1428 src: /192.168.56.101:56202 dest: /192.168.56.102:50010
2019-08-17 22:40:06,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56202, dest: /192.168.56.102:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_361337793_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742252_1428, duration: 2312655889
2019-08-17 22:40:06,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742252_1428, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:40:06,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742253_1429 src: /192.168.56.101:56206 dest: /192.168.56.102:50010
2019-08-17 22:40:07,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56206, dest: /192.168.56.102:50010, bytes: 71636882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_361337793_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742253_1429, duration: 859750726
2019-08-17 22:40:07,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742253_1429, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:40:07,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742254_1430 src: /192.168.56.101:56210 dest: /192.168.56.102:50010
2019-08-17 22:40:07,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56210, dest: /192.168.56.102:50010, bytes: 83940, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_361337793_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742254_1430, duration: 21915014
2019-08-17 22:40:07,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742254_1430, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:40:22,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742255_1431 src: /192.168.56.101:56240 dest: /192.168.56.102:50010
2019-08-17 22:40:24,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742256_1432 src: /192.168.56.102:49960 dest: /192.168.56.102:50010
2019-08-17 22:40:24,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.102:49960, dest: /192.168.56.102:50010, bytes: 6789, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1373196331_91, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742256_1432, duration: 65578309
2019-08-17 22:40:24,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742256_1432, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-08-17 22:40:25,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56240, dest: /192.168.56.102:50010, bytes: 18779, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_361337793_1, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742255_1431, duration: 2801244091
2019-08-17 22:40:25,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742255_1431, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:40:25,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1619971864-192.168.56.101-1564664778252:blk_1073742257_1433 src: /192.168.56.101:56250 dest: /192.168.56.102:50010
2019-08-17 22:40:26,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.56.101:56250, dest: /192.168.56.102:50010, bytes: 7741, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441960742_93, offset: 0, srvID: 47a99e3c-e551-40ca-acc8-80e4a48aa24b, blockid: BP-1619971864-192.168.56.101-1564664778252:blk_1073742257_1433, duration: 93489736
2019-08-17 22:40:26,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1619971864-192.168.56.101-1564664778252:blk_1073742257_1433, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-17 22:40:31,359 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742252_1428 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742252 for deletion
2019-08-17 22:40:31,360 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742253_1429 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742253 for deletion
2019-08-17 22:40:31,361 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742254_1430 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742254 for deletion
2019-08-17 22:40:31,393 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742252_1428 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742252
2019-08-17 22:40:31,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742253_1429 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742253
2019-08-17 22:40:31,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1619971864-192.168.56.101-1564664778252 blk_1073742254_1430 file /abc/datanode/current/BP-1619971864-192.168.56.101-1564664778252/current/finalized/subdir0/subdir1/blk_1073742254
2019-08-18 20:11:29,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.1.107
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-18 20:11:29,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-18 20:11:30,179 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-18 20:11:30,704 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-18 20:11:30,831 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-18 20:11:30,831 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-18 20:11:30,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-18 20:11:30,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-18 20:11:30,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-18 20:11:30,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-18 20:11:30,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-18 20:11:31,047 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-18 20:11:31,051 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-18 20:11:31,060 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-18 20:11:31,062 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-18 20:11:31,063 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-18 20:11:31,063 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-18 20:11:31,083 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-18 20:11:31,086 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-18 20:11:31,086 INFO org.mortbay.log: jetty-6.1.26
2019-08-18 20:11:31,417 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-18 20:11:31,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-18 20:11:31,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-18 20:11:31,972 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-18 20:11:31,992 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-18 20:11:32,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-18 20:11:32,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-18 20:11:32,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-18 20:11:32,079 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-18 20:11:32,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.1.219:9000 starting to offer service
2019-08-18 20:11:32,110 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-18 20:11:32,110 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-18 20:11:32,450 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode/in_use.lock acquired by nodename 3600@um2
2019-08-18 20:11:32,451 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /abc/datanode is not formatted for BP-155982949-192.168.1.219-1566151777518
2019-08-18 20:11:32,451 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-18 20:11:32,499 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-155982949-192.168.1.219-1566151777518
2019-08-18 20:11:32,499 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode/current/BP-155982949-192.168.1.219-1566151777518
2019-08-18 20:11:32,500 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /abc/datanode/current/BP-155982949-192.168.1.219-1566151777518 is not formatted for BP-155982949-192.168.1.219-1566151777518
2019-08-18 20:11:32,500 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-18 20:11:32,500 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-155982949-192.168.1.219-1566151777518 directory /abc/datanode/current/BP-155982949-192.168.1.219-1566151777518/current
2019-08-18 20:11:32,504 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-18 20:11:32,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=99967847;bpid=BP-155982949-192.168.1.219-1566151777518;lv=-56;nsInfo=lv=-60;cid=CID-53e8f40d-f196-4cd0-b9c3-5682684adae4;nsid=99967847;c=0;bpid=BP-155982949-192.168.1.219-1566151777518;dnuuid=null
2019-08-18 20:11:32,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 9269d4ce-bb2a-4d68-8081-f67a21d9e36a
2019-08-18 20:11:32,518 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-18 20:11:32,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode/current
2019-08-18 20:11:32,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode/current, StorageType: DISK
2019-08-18 20:11:32,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-18 20:11:32,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-155982949-192.168.1.219-1566151777518
2019-08-18 20:11:32,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-155982949-192.168.1.219-1566151777518 on volume /abc/datanode/current...
2019-08-18 20:11:32,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-155982949-192.168.1.219-1566151777518 on /abc/datanode/current: 8ms
2019-08-18 20:11:32,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-155982949-192.168.1.219-1566151777518: 13ms
2019-08-18 20:11:32,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-155982949-192.168.1.219-1566151777518 on volume /abc/datanode/current...
2019-08-18 20:11:32,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-155982949-192.168.1.219-1566151777518 on volume /abc/datanode/current: 0ms
2019-08-18 20:11:32,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-08-18 20:11:32,573 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566152016573 with interval 21600000
2019-08-18 20:11:32,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-155982949-192.168.1.219-1566151777518 (Datanode Uuid null) service to um1/192.168.1.219:9000 beginning handshake with NN
2019-08-18 20:11:32,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-155982949-192.168.1.219-1566151777518 (Datanode Uuid null) service to um1/192.168.1.219:9000 successfully registered with NN
2019-08-18 20:11:32,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.1.219:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-18 20:11:32,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-155982949-192.168.1.219-1566151777518 (Datanode Uuid 9269d4ce-bb2a-4d68-8081-f67a21d9e36a) service to um1/192.168.1.219:9000 trying to claim ACTIVE state with txid=1
2019-08-18 20:11:32,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-155982949-192.168.1.219-1566151777518 (Datanode Uuid 9269d4ce-bb2a-4d68-8081-f67a21d9e36a) service to um1/192.168.1.219:9000
2019-08-18 20:11:32,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9c0e141ba0,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 38 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-18 20:11:32,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-155982949-192.168.1.219-1566151777518
2019-08-18 20:11:32,708 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-18 20:11:32,708 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-18 20:11:32,710 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-18 20:11:32,710 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-18 20:11:32,710 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-155982949-192.168.1.219-1566151777518
2019-08-18 20:11:32,713 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-155982949-192.168.1.219-1566151777518 to blockPoolScannerMap, new size=1
2019-08-18 20:13:36,576 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-155982949-192.168.1.219-1566151777518 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-08-24 15:45:13,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.162
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-24 15:45:13,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-24 15:45:14,005 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 15:45:14,622 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-24 15:45:14,772 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-24 15:45:14,773 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-24 15:45:14,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-24 15:45:14,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-24 15:45:14,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-24 15:45:14,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-24 15:45:14,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-24 15:45:14,995 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-24 15:45:14,999 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-24 15:45:15,011 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-24 15:45:15,014 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-24 15:45:15,014 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-24 15:45:15,014 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-24 15:45:15,032 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-24 15:45:15,035 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-24 15:45:15,035 INFO org.mortbay.log: jetty-6.1.26
2019-08-24 15:45:15,358 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-24 15:45:15,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-24 15:45:15,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-24 15:45:15,828 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-24 15:45:15,842 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-24 15:45:15,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-24 15:45:15,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-24 15:45:15,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-24 15:45:15,921 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 15:45:15,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.240:9000 starting to offer service
2019-08-24 15:45:15,948 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-24 15:45:15,948 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-24 15:45:16,316 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode2/in_use.lock acquired by nodename 6375@um2
2019-08-24 15:45:16,317 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /abc/datanode2 is not formatted for BP-994169456-192.168.0.240-1566654293214
2019-08-24 15:45:16,317 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-24 15:45:16,397 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-994169456-192.168.0.240-1566654293214
2019-08-24 15:45:16,397 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode2/current/BP-994169456-192.168.0.240-1566654293214
2019-08-24 15:45:16,398 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /abc/datanode2/current/BP-994169456-192.168.0.240-1566654293214 is not formatted for BP-994169456-192.168.0.240-1566654293214
2019-08-24 15:45:16,398 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-24 15:45:16,398 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-994169456-192.168.0.240-1566654293214 directory /abc/datanode2/current/BP-994169456-192.168.0.240-1566654293214/current
2019-08-24 15:45:16,400 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-24 15:45:16,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1498218310;bpid=BP-994169456-192.168.0.240-1566654293214;lv=-56;nsInfo=lv=-60;cid=CID-fdaf79ce-19a5-4909-ad0a-5c0d535c7e43;nsid=1498218310;c=0;bpid=BP-994169456-192.168.0.240-1566654293214;dnuuid=null
2019-08-24 15:45:16,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID efa87589-0256-4a1a-b446-20baf24e495c
2019-08-24 15:45:16,416 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 15:45:16,447 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode2/current
2019-08-24 15:45:16,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode2/current, StorageType: DISK
2019-08-24 15:45:16,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-24 15:45:16,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-994169456-192.168.0.240-1566654293214
2019-08-24 15:45:16,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-994169456-192.168.0.240-1566654293214 on volume /abc/datanode2/current...
2019-08-24 15:45:16,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-994169456-192.168.0.240-1566654293214 on /abc/datanode2/current: 13ms
2019-08-24 15:45:16,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-994169456-192.168.0.240-1566654293214: 13ms
2019-08-24 15:45:16,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-994169456-192.168.0.240-1566654293214 on volume /abc/datanode2/current...
2019-08-24 15:45:16,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-994169456-192.168.0.240-1566654293214 on volume /abc/datanode2/current: 0ms
2019-08-24 15:45:16,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-08-24 15:45:16,470 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566671954470 with interval 21600000
2019-08-24 15:45:16,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-994169456-192.168.0.240-1566654293214 (Datanode Uuid null) service to um1/192.168.0.240:9000 beginning handshake with NN
2019-08-24 15:45:16,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-994169456-192.168.0.240-1566654293214 (Datanode Uuid null) service to um1/192.168.0.240:9000 successfully registered with NN
2019-08-24 15:45:16,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.0.240:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-24 15:45:16,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-994169456-192.168.0.240-1566654293214 (Datanode Uuid efa87589-0256-4a1a-b446-20baf24e495c) service to um1/192.168.0.240:9000 trying to claim ACTIVE state with txid=1
2019-08-24 15:45:16,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-994169456-192.168.0.240-1566654293214 (Datanode Uuid efa87589-0256-4a1a-b446-20baf24e495c) service to um1/192.168.0.240:9000
2019-08-24 15:45:16,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xba20edad21,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 25 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-24 15:45:16,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-994169456-192.168.0.240-1566654293214
2019-08-24 15:45:16,602 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-24 15:45:16,602 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-24 15:45:16,606 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-24 15:45:16,606 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-24 15:45:16,606 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-994169456-192.168.0.240-1566654293214
2019-08-24 15:45:16,610 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-994169456-192.168.0.240-1566654293214 to blockPoolScannerMap, new size=1
2019-08-24 16:44:23,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.162
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-24 16:44:23,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-24 16:44:23,928 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 16:44:24,539 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-24 16:44:24,705 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-24 16:44:24,705 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-24 16:44:24,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-24 16:44:24,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-24 16:44:24,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-24 16:44:24,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-24 16:44:24,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-24 16:44:24,912 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-24 16:44:24,916 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-24 16:44:24,928 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-24 16:44:24,932 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-24 16:44:24,932 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-24 16:44:24,932 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-24 16:44:24,956 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-24 16:44:24,960 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-24 16:44:24,961 INFO org.mortbay.log: jetty-6.1.26
2019-08-24 16:44:25,393 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-24 16:44:25,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-24 16:44:25,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-24 16:44:26,030 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-24 16:44:26,052 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-24 16:44:26,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-24 16:44:26,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-24 16:44:26,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-24 16:44:26,143 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 16:44:26,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.240:9000 starting to offer service
2019-08-24 16:44:26,178 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-24 16:44:26,180 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-24 16:44:26,664 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode2/in_use.lock acquired by nodename 2329@um2
2019-08-24 16:44:26,665 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /abc/datanode2 is not formatted for BP-161396067-192.168.0.240-1566657744364
2019-08-24 16:44:26,665 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-24 16:44:26,753 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-161396067-192.168.0.240-1566657744364
2019-08-24 16:44:26,753 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode2/current/BP-161396067-192.168.0.240-1566657744364
2019-08-24 16:44:26,753 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /abc/datanode2/current/BP-161396067-192.168.0.240-1566657744364 is not formatted for BP-161396067-192.168.0.240-1566657744364
2019-08-24 16:44:26,754 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-24 16:44:26,754 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-161396067-192.168.0.240-1566657744364 directory /abc/datanode2/current/BP-161396067-192.168.0.240-1566657744364/current
2019-08-24 16:44:26,756 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-24 16:44:26,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=804391968;bpid=BP-161396067-192.168.0.240-1566657744364;lv=-56;nsInfo=lv=-60;cid=CID-e25e4270-427a-4520-a8b7-59f8b1595f90;nsid=804391968;c=0;bpid=BP-161396067-192.168.0.240-1566657744364;dnuuid=null
2019-08-24 16:44:26,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID cb44805f-5806-41cb-84f7-6fc0cda40cac
2019-08-24 16:44:26,777 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 16:44:26,809 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode2/current
2019-08-24 16:44:26,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode2/current, StorageType: DISK
2019-08-24 16:44:26,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-24 16:44:26,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-161396067-192.168.0.240-1566657744364
2019-08-24 16:44:26,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-161396067-192.168.0.240-1566657744364 on volume /abc/datanode2/current...
2019-08-24 16:44:26,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-161396067-192.168.0.240-1566657744364 on /abc/datanode2/current: 27ms
2019-08-24 16:44:26,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-161396067-192.168.0.240-1566657744364: 34ms
2019-08-24 16:44:26,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-161396067-192.168.0.240-1566657744364 on volume /abc/datanode2/current...
2019-08-24 16:44:26,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-161396067-192.168.0.240-1566657744364 on volume /abc/datanode2/current: 0ms
2019-08-24 16:44:26,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2019-08-24 16:44:26,860 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566673190860 with interval 21600000
2019-08-24 16:44:26,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-161396067-192.168.0.240-1566657744364 (Datanode Uuid null) service to um1/192.168.0.240:9000 beginning handshake with NN
2019-08-24 16:44:26,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-161396067-192.168.0.240-1566657744364 (Datanode Uuid null) service to um1/192.168.0.240:9000 successfully registered with NN
2019-08-24 16:44:26,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.0.240:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-24 16:44:26,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-161396067-192.168.0.240-1566657744364 (Datanode Uuid cb44805f-5806-41cb-84f7-6fc0cda40cac) service to um1/192.168.0.240:9000 trying to claim ACTIVE state with txid=1
2019-08-24 16:44:26,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-161396067-192.168.0.240-1566657744364 (Datanode Uuid cb44805f-5806-41cb-84f7-6fc0cda40cac) service to um1/192.168.0.240:9000
2019-08-24 16:44:27,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9586314cb2,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 38 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-24 16:44:27,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-161396067-192.168.0.240-1566657744364
2019-08-24 16:44:27,011 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-24 16:44:27,011 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-24 16:44:27,015 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-24 16:44:27,016 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-24 16:44:27,016 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-161396067-192.168.0.240-1566657744364
2019-08-24 16:44:27,021 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-161396067-192.168.0.240-1566657744364 to blockPoolScannerMap, new size=1
2019-08-24 17:00:36,714 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.0.162"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-24 17:00:40,026 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-08-24 17:00:40,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.0.162
************************************************************/
2019-08-24 17:02:05,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.162
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-24 17:02:05,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-24 17:02:06,049 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 17:02:06,497 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-24 17:02:06,581 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-24 17:02:06,581 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-24 17:02:06,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-24 17:02:06,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-24 17:02:06,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-24 17:02:06,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-24 17:02:06,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-24 17:02:06,716 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-24 17:02:06,720 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-24 17:02:06,730 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-24 17:02:06,733 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-24 17:02:06,733 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-24 17:02:06,733 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-24 17:02:06,748 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-24 17:02:06,751 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-24 17:02:06,751 INFO org.mortbay.log: jetty-6.1.26
2019-08-24 17:02:07,040 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-24 17:02:07,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-24 17:02:07,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-24 17:02:07,610 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-24 17:02:07,641 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-24 17:02:07,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-24 17:02:07,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-24 17:02:07,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-24 17:02:07,723 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 17:02:07,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.240:9000 starting to offer service
2019-08-24 17:02:07,743 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-24 17:02:07,744 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-24 17:02:08,273 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode2/in_use.lock acquired by nodename 3667@um2
2019-08-24 17:02:08,275 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /abc/datanode2: namenode clusterID = CID-403a0251-1604-44a0-b205-ccebae382a38; datanode clusterID = CID-e25e4270-427a-4520-a8b7-59f8b1595f90
2019-08-24 17:02:08,275 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.240:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:478)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1342)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1308)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:226)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:867)
	at java.lang.Thread.run(Thread.java:748)
2019-08-24 17:02:08,277 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.240:9000
2019-08-24 17:02:08,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2019-08-24 17:02:10,378 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-08-24 17:02:10,380 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-08-24 17:02:10,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.0.162
************************************************************/
2019-08-24 17:09:38,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.162
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-08-24 17:09:38,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-08-24 17:09:39,025 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 17:09:39,430 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-08-24 17:09:39,505 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-08-24 17:09:39,505 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-08-24 17:09:39,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-08-24 17:09:39,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-08-24 17:09:39,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-08-24 17:09:39,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-08-24 17:09:39,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-08-24 17:09:39,612 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-08-24 17:09:39,615 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-08-24 17:09:39,623 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-08-24 17:09:39,624 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-08-24 17:09:39,624 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-08-24 17:09:39,625 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-08-24 17:09:39,635 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-08-24 17:09:39,638 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-08-24 17:09:39,638 INFO org.mortbay.log: jetty-6.1.26
2019-08-24 17:09:39,959 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-08-24 17:09:40,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-08-24 17:09:40,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-08-24 17:09:40,353 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-08-24 17:09:40,365 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-08-24 17:09:40,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-08-24 17:09:40,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-08-24 17:09:40,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-08-24 17:09:40,422 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 17:09:40,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.240:9000 starting to offer service
2019-08-24 17:09:40,429 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-08-24 17:09:40,429 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-08-24 17:09:40,661 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode2/in_use.lock acquired by nodename 4556@um2
2019-08-24 17:09:40,662 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /abc/datanode2 is not formatted for BP-8011334-192.168.0.240-1566658908132
2019-08-24 17:09:40,662 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-24 17:09:40,721 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-8011334-192.168.0.240-1566658908132
2019-08-24 17:09:40,722 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132
2019-08-24 17:09:40,722 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132 is not formatted for BP-8011334-192.168.0.240-1566658908132
2019-08-24 17:09:40,722 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-08-24 17:09:40,722 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-8011334-192.168.0.240-1566658908132 directory /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current
2019-08-24 17:09:40,724 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-08-24 17:09:40,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=322677011;bpid=BP-8011334-192.168.0.240-1566658908132;lv=-56;nsInfo=lv=-60;cid=CID-403a0251-1604-44a0-b205-ccebae382a38;nsid=322677011;c=0;bpid=BP-8011334-192.168.0.240-1566658908132;dnuuid=null
2019-08-24 17:09:40,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 66e04227-1fc5-44d2-a340-3770e34fbbb7
2019-08-24 17:09:40,736 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-08-24 17:09:40,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode2/current
2019-08-24 17:09:40,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode2/current, StorageType: DISK
2019-08-24 17:09:40,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-08-24 17:09:40,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-8011334-192.168.0.240-1566658908132
2019-08-24 17:09:40,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-08-24 17:09:40,774 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-8011334-192.168.0.240-1566658908132 on /abc/datanode2/current: 8ms
2019-08-24 17:09:40,774 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-8011334-192.168.0.240-1566658908132: 9ms
2019-08-24 17:09:40,775 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-08-24 17:09:40,775 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current: 1ms
2019-08-24 17:09:40,775 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-08-24 17:09:40,778 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1566667617778 with interval 21600000
2019-08-24 17:09:40,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.240:9000 beginning handshake with NN
2019-08-24 17:09:40,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.240:9000 successfully registered with NN
2019-08-24 17:09:40,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.0.240:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-08-24 17:09:40,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.240:9000 trying to claim ACTIVE state with txid=15
2019-08-24 17:09:40,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.240:9000
2019-08-24 17:09:40,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1f5ff73ea9d,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 25 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-24 17:09:40,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-08-24 17:09:40,858 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-08-24 17:09:40,858 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-08-24 17:09:40,860 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-08-24 17:09:40,860 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-08-24 17:09:40,860 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-8011334-192.168.0.240-1566658908132
2019-08-24 17:09:40,864 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-8011334-192.168.0.240-1566658908132 to blockPoolScannerMap, new size=1
2019-08-24 17:47:46,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741828_1004 src: /192.168.0.240:43604 dest: /192.168.0.162:50010
2019-08-24 17:47:46,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:43604, dest: /192.168.0.162:50010, bytes: 3048, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1908109270_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741828_1004, duration: 45548661
2019-08-24 17:47:46,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 17:47:46,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741829_1005 src: /192.168.0.240:43608 dest: /192.168.0.162:50010
2019-08-24 17:47:46,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:43608, dest: /192.168.0.162:50010, bytes: 123637, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1908109270_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741829_1005, duration: 14249484
2019-08-24 17:47:46,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 17:47:47,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741831_1007 src: /192.168.0.240:43616 dest: /192.168.0.162:50010
2019-08-24 17:47:47,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:43616, dest: /192.168.0.162:50010, bytes: 4043, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1908109270_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741831_1007, duration: 4079703
2019-08-24 17:47:47,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 17:47:47,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741836_1012 src: /192.168.0.240:43636 dest: /192.168.0.162:50010
2019-08-24 17:47:47,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:43636, dest: /192.168.0.162:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1908109270_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741836_1012, duration: 2707741
2019-08-24 17:47:47,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 17:47:47,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741837_1013 src: /192.168.0.240:43640 dest: /192.168.0.162:50010
2019-08-24 17:47:47,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:43640, dest: /192.168.0.162:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1908109270_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741837_1013, duration: 13495068
2019-08-24 17:47:47,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 17:47:52,777 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741836_1012
2019-08-24 17:47:52,793 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741829_1005
2019-08-24 17:47:52,794 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741831_1007
2019-08-24 17:47:52,794 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741837_1013
2019-08-24 17:47:52,795 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741828_1004
2019-08-24 18:02:07,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741838_1014 src: /192.168.0.240:44226 dest: /192.168.0.162:50010
2019-08-24 18:02:07,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741838_1014 src: /192.168.0.240:44226 dest: /192.168.0.162:50010 of size 334
2019-08-24 18:02:07,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741839_1015 src: /192.168.0.240:44224 dest: /192.168.0.162:50010
2019-08-24 18:02:07,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741839_1015 src: /192.168.0.240:44224 dest: /192.168.0.162:50010 of size 615
2019-08-24 18:02:10,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741835_1011 src: /192.168.0.240:44230 dest: /192.168.0.162:50010
2019-08-24 18:02:10,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741835_1011 src: /192.168.0.240:44230 dest: /192.168.0.162:50010 of size 73
2019-08-24 18:02:10,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741834_1010 src: /192.168.0.240:44232 dest: /192.168.0.162:50010
2019-08-24 18:02:10,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741834_1010 src: /192.168.0.240:44232 dest: /192.168.0.162:50010 of size 5812
2019-08-24 18:02:13,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741833_1009 src: /192.168.0.240:44236 dest: /192.168.0.162:50010
2019-08-24 18:02:13,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741833_1009 src: /192.168.0.240:44236 dest: /192.168.0.162:50010 of size 240
2019-08-24 18:02:13,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741832_1008 src: /192.168.0.240:44238 dest: /192.168.0.162:50010
2019-08-24 18:02:13,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741832_1008 src: /192.168.0.240:44238 dest: /192.168.0.162:50010 of size 130
2019-08-24 18:02:13,885 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741838_1014
2019-08-24 18:02:13,886 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741839_1015
2019-08-24 18:02:16,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741826_1002 src: /192.168.0.240:44244 dest: /192.168.0.162:50010
2019-08-24 18:02:16,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741825_1001 src: /192.168.0.240:44246 dest: /192.168.0.162:50010
2019-08-24 18:02:16,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741825_1001 src: /192.168.0.240:44246 dest: /192.168.0.162:50010 of size 66
2019-08-24 18:02:16,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741826_1002 src: /192.168.0.240:44244 dest: /192.168.0.162:50010 of size 5812
2019-08-24 18:02:18,893 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741833_1009
2019-08-24 18:02:18,894 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741834_1010
2019-08-24 18:02:18,894 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741835_1011
2019-08-24 18:02:18,895 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741832_1008
2019-08-24 18:02:19,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741827_1003 src: /192.168.0.240:44254 dest: /192.168.0.162:50010
2019-08-24 18:02:19,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741830_1006 src: /192.168.0.240:44252 dest: /192.168.0.162:50010
2019-08-24 18:02:19,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741830_1006 src: /192.168.0.240:44252 dest: /192.168.0.162:50010 of size 66
2019-08-24 18:02:19,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-8011334-192.168.0.240-1566658908132:blk_1073741827_1003 src: /192.168.0.240:44254 dest: /192.168.0.162:50010 of size 3751306
2019-08-24 18:02:23,909 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741826_1002
2019-08-24 18:02:23,910 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741825_1001
2019-08-24 18:02:28,951 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741830_1006
2019-08-24 18:02:30,978 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741827_1003
2019-08-24 18:31:46,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741840_1016 src: /192.168.0.240:44576 dest: /192.168.0.162:50010
2019-08-24 18:31:46,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44576, dest: /192.168.0.162:50010, bytes: 66, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1111960286_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741840_1016, duration: 34069325
2019-08-24 18:31:46,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:31:46,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741841_1017 src: /192.168.0.240:44580 dest: /192.168.0.162:50010
2019-08-24 18:31:46,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44580, dest: /192.168.0.162:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1111960286_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741841_1017, duration: 2679447
2019-08-24 18:31:46,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:31:56,836 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741841_1017
2019-08-24 18:31:56,838 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741840_1016
2019-08-24 18:33:45,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2019-08-24 18:33:45,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2019-08-24 18:33:45,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741840_1016 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741840
2019-08-24 18:33:45,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741841_1017 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741841
2019-08-24 18:36:40,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741842_1018 src: /192.168.0.240:44600 dest: /192.168.0.162:50010
2019-08-24 18:36:40,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44600, dest: /192.168.0.162:50010, bytes: 3751306, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741842_1018, duration: 114016302
2019-08-24 18:36:40,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741843_1019 src: /192.168.0.240:44604 dest: /192.168.0.162:50010
2019-08-24 18:36:40,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44604, dest: /192.168.0.162:50010, bytes: 3048, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741843_1019, duration: 3400958
2019-08-24 18:36:40,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741844_1020 src: /192.168.0.240:44608 dest: /192.168.0.162:50010
2019-08-24 18:36:40,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44608, dest: /192.168.0.162:50010, bytes: 123637, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741844_1020, duration: 5471446
2019-08-24 18:36:40,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741845_1021 src: /192.168.0.240:44612 dest: /192.168.0.162:50010
2019-08-24 18:36:40,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44612, dest: /192.168.0.162:50010, bytes: 66, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741845_1021, duration: 4543028
2019-08-24 18:36:40,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741846_1022 src: /192.168.0.240:44616 dest: /192.168.0.162:50010
2019-08-24 18:36:40,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44616, dest: /192.168.0.162:50010, bytes: 4043, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741846_1022, duration: 3660640
2019-08-24 18:36:40,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741847_1023 src: /192.168.0.240:44620 dest: /192.168.0.162:50010
2019-08-24 18:36:40,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44620, dest: /192.168.0.162:50010, bytes: 130, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741847_1023, duration: 2245563
2019-08-24 18:36:40,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741848_1024 src: /192.168.0.240:44624 dest: /192.168.0.162:50010
2019-08-24 18:36:40,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44624, dest: /192.168.0.162:50010, bytes: 240, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741848_1024, duration: 3455067
2019-08-24 18:36:40,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741849_1025 src: /192.168.0.240:44628 dest: /192.168.0.162:50010
2019-08-24 18:36:40,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44628, dest: /192.168.0.162:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741849_1025, duration: 2955777
2019-08-24 18:36:40,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741850_1026 src: /192.168.0.240:44632 dest: /192.168.0.162:50010
2019-08-24 18:36:40,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44632, dest: /192.168.0.162:50010, bytes: 73, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741850_1026, duration: 2937770
2019-08-24 18:36:40,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741851_1027 src: /192.168.0.240:44636 dest: /192.168.0.162:50010
2019-08-24 18:36:40,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44636, dest: /192.168.0.162:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741851_1027, duration: 20308602
2019-08-24 18:36:40,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741852_1028 src: /192.168.0.240:44640 dest: /192.168.0.162:50010
2019-08-24 18:36:40,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44640, dest: /192.168.0.162:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741852_1028, duration: 3731016
2019-08-24 18:36:40,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741853_1029 src: /192.168.0.240:44644 dest: /192.168.0.162:50010
2019-08-24 18:36:40,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44644, dest: /192.168.0.162:50010, bytes: 334, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741853_1029, duration: 2807523
2019-08-24 18:36:40,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:40,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741854_1030 src: /192.168.0.240:44648 dest: /192.168.0.162:50010
2019-08-24 18:36:40,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44648, dest: /192.168.0.162:50010, bytes: 615, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1657012238_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741854_1030, duration: 2154039
2019-08-24 18:36:40,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:36:46,957 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741849_1025
2019-08-24 18:36:46,957 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741843_1019
2019-08-24 18:36:46,958 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741853_1029
2019-08-24 18:36:46,958 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741844_1020
2019-08-24 18:36:46,959 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741847_1023
2019-08-24 18:36:46,959 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741854_1030
2019-08-24 18:36:50,559 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741842_1018
2019-08-24 18:36:50,560 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741848_1024
2019-08-24 18:42:21,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741855_1031 src: /192.168.0.240:44672 dest: /192.168.0.162:50010
2019-08-24 18:42:21,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44672, dest: /192.168.0.162:50010, bytes: 3751306, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741855_1031, duration: 97502309
2019-08-24 18:42:21,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:21,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741856_1032 src: /192.168.0.240:44676 dest: /192.168.0.162:50010
2019-08-24 18:42:21,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44676, dest: /192.168.0.162:50010, bytes: 3048, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741856_1032, duration: 3250279
2019-08-24 18:42:21,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:21,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741857_1033 src: /192.168.0.240:44680 dest: /192.168.0.162:50010
2019-08-24 18:42:21,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44680, dest: /192.168.0.162:50010, bytes: 123637, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741857_1033, duration: 7627748
2019-08-24 18:42:21,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:21,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741858_1034 src: /192.168.0.240:44684 dest: /192.168.0.162:50010
2019-08-24 18:42:21,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44684, dest: /192.168.0.162:50010, bytes: 66, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741858_1034, duration: 2754180
2019-08-24 18:42:21,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:21,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741859_1035 src: /192.168.0.240:44688 dest: /192.168.0.162:50010
2019-08-24 18:42:21,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44688, dest: /192.168.0.162:50010, bytes: 4043, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741859_1035, duration: 3172794
2019-08-24 18:42:21,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:21,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741860_1036 src: /192.168.0.240:44692 dest: /192.168.0.162:50010
2019-08-24 18:42:21,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44692, dest: /192.168.0.162:50010, bytes: 130, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741860_1036, duration: 2104613
2019-08-24 18:42:21,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:21,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741861_1037 src: /192.168.0.240:44696 dest: /192.168.0.162:50010
2019-08-24 18:42:21,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44696, dest: /192.168.0.162:50010, bytes: 240, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741861_1037, duration: 2176237
2019-08-24 18:42:21,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:22,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741862_1038 src: /192.168.0.240:44700 dest: /192.168.0.162:50010
2019-08-24 18:42:22,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44700, dest: /192.168.0.162:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741862_1038, duration: 10586494
2019-08-24 18:42:22,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:22,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741863_1039 src: /192.168.0.240:44704 dest: /192.168.0.162:50010
2019-08-24 18:42:22,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44704, dest: /192.168.0.162:50010, bytes: 73, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741863_1039, duration: 2662984
2019-08-24 18:42:22,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:22,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741864_1040 src: /192.168.0.240:44708 dest: /192.168.0.162:50010
2019-08-24 18:42:22,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44708, dest: /192.168.0.162:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741864_1040, duration: 2379231
2019-08-24 18:42:22,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:22,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741865_1041 src: /192.168.0.240:44712 dest: /192.168.0.162:50010
2019-08-24 18:42:22,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44712, dest: /192.168.0.162:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741865_1041, duration: 18770640
2019-08-24 18:42:22,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:22,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741866_1042 src: /192.168.0.240:44716 dest: /192.168.0.162:50010
2019-08-24 18:42:22,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44716, dest: /192.168.0.162:50010, bytes: 334, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741866_1042, duration: 3183854
2019-08-24 18:42:22,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741866_1042, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:22,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741867_1043 src: /192.168.0.240:44720 dest: /192.168.0.162:50010
2019-08-24 18:42:22,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44720, dest: /192.168.0.162:50010, bytes: 615, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-440503760_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741867_1043, duration: 27239923
2019-08-24 18:42:22,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:42:25,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741827_1003 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741827
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741828_1004 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741828
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741829_1005 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741829
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741830_1006 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741830
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741831_1007 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741831
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741832_1008 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741832
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741833_1009 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741833
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741834_1010 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741834
2019-08-24 18:42:25,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2019-08-24 18:42:25,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741835_1011 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741835
2019-08-24 18:42:25,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741836_1012 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741836
2019-08-24 18:42:25,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741837_1013 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741837
2019-08-24 18:42:25,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741838_1014 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741838
2019-08-24 18:42:25,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741839_1015 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741839
2019-08-24 18:42:25,712 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741851_1027
2019-08-24 18:42:30,719 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741863_1039
2019-08-24 18:42:30,721 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741866_1042
2019-08-24 18:42:30,723 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741865_1041
2019-08-24 18:42:30,723 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741857_1033
2019-08-24 18:42:30,724 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741864_1040
2019-08-24 18:42:30,724 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741859_1035
2019-08-24 18:42:30,724 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741862_1038
2019-08-24 18:42:30,725 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741856_1032
2019-08-24 18:44:25,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x72185ee6001,  containing 1 storage report(s), of which we sent 1. The reports had 28 total blocks and used 1 RPC(s). This took 1 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-24 18:44:25,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-08-24 18:47:43,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741842_1018 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741842
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2019-08-24 18:47:43,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2019-08-24 18:47:43,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741843_1019 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741843
2019-08-24 18:47:43,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741844_1020 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741844
2019-08-24 18:47:43,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741845_1021 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741845
2019-08-24 18:47:43,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741846_1022 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741846
2019-08-24 18:47:43,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741847_1023 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741847
2019-08-24 18:47:43,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741848_1024 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741848
2019-08-24 18:47:43,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741849_1025 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741849
2019-08-24 18:47:43,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741850_1026 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741850
2019-08-24 18:47:43,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741851_1027 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741851
2019-08-24 18:47:43,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741852_1028 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741852
2019-08-24 18:47:43,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741853_1029 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741853
2019-08-24 18:47:43,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741854_1030 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741854
2019-08-24 18:49:25,664 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.0.162"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-24 18:49:29,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:30,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:31,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:32,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:33,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:34,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:35,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:36,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:37,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:38,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 18:49:38,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: RemoteException in offerService
org.apache.hadoop.ipc.RemoteException(java.io.IOException): NameNode still not started
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup(NameNodeRpcServer.java:1632)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendHeartbeat(NameNodeRpcServer.java:1151)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.sendHeartbeat(DatanodeProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:27331)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.call(Client.java:1470)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
2019-08-24 18:49:39,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from um1/192.168.0.240:9000 with active state
2019-08-24 18:49:39,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.240:9000 beginning handshake with NN
2019-08-24 18:49:39,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.240:9000 successfully registered with NN
2019-08-24 18:49:40,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x76ac9bc5b21,  containing 1 storage report(s), of which we sent 1. The reports had 15 total blocks and used 1 RPC(s). This took 0 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-24 18:49:40,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-08-24 18:56:23,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741868_1044 src: /192.168.0.240:44770 dest: /192.168.0.162:50010
2019-08-24 18:56:23,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44770, dest: /192.168.0.162:50010, bytes: 1462476, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741868_1044, duration: 68081442
2019-08-24 18:56:23,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741868_1044, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:23,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741869_1045 src: /192.168.0.240:44774 dest: /192.168.0.162:50010
2019-08-24 18:56:23,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44774, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741869_1045, duration: 4296243
2019-08-24 18:56:23,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741869_1045, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:23,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741870_1046 src: /192.168.0.240:44778 dest: /192.168.0.162:50010
2019-08-24 18:56:23,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44778, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741870_1046, duration: 4351694
2019-08-24 18:56:23,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:23,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741871_1047 src: /192.168.0.240:44782 dest: /192.168.0.162:50010
2019-08-24 18:56:23,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44782, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741871_1047, duration: 3463885
2019-08-24 18:56:23,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:23,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741872_1048 src: /192.168.0.240:44786 dest: /192.168.0.162:50010
2019-08-24 18:56:24,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44786, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741872_1048, duration: 8262642
2019-08-24 18:56:24,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741873_1049 src: /192.168.0.240:44790 dest: /192.168.0.162:50010
2019-08-24 18:56:24,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44790, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741873_1049, duration: 7715947
2019-08-24 18:56:24,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741874_1050 src: /192.168.0.240:44794 dest: /192.168.0.162:50010
2019-08-24 18:56:24,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44794, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741874_1050, duration: 3325569
2019-08-24 18:56:24,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741875_1051 src: /192.168.0.240:44798 dest: /192.168.0.162:50010
2019-08-24 18:56:24,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44798, dest: /192.168.0.162:50010, bytes: 2243058, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741875_1051, duration: 46681090
2019-08-24 18:56:24,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741876_1052 src: /192.168.0.240:44802 dest: /192.168.0.162:50010
2019-08-24 18:56:24,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44802, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741876_1052, duration: 11449788
2019-08-24 18:56:24,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741877_1053 src: /192.168.0.240:44806 dest: /192.168.0.162:50010
2019-08-24 18:56:24,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44806, dest: /192.168.0.162:50010, bytes: 4908, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741877_1053, duration: 18039544
2019-08-24 18:56:24,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741878_1054 src: /192.168.0.240:44810 dest: /192.168.0.162:50010
2019-08-24 18:56:24,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44810, dest: /192.168.0.162:50010, bytes: 4908, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741878_1054, duration: 7640940
2019-08-24 18:56:24,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741878_1054, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741879_1055 src: /192.168.0.240:44814 dest: /192.168.0.162:50010
2019-08-24 18:56:24,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44814, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741879_1055, duration: 8597914
2019-08-24 18:56:24,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741880_1056 src: /192.168.0.240:44818 dest: /192.168.0.162:50010
2019-08-24 18:56:24,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44818, dest: /192.168.0.162:50010, bytes: 4908, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741880_1056, duration: 3201659
2019-08-24 18:56:24,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741881_1057 src: /192.168.0.240:44822 dest: /192.168.0.162:50010
2019-08-24 18:56:24,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44822, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741881_1057, duration: 6277953
2019-08-24 18:56:24,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741882_1058 src: /192.168.0.240:44826 dest: /192.168.0.162:50010
2019-08-24 18:56:24,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44826, dest: /192.168.0.162:50010, bytes: 52091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741882_1058, duration: 5326043
2019-08-24 18:56:24,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741883_1059 src: /192.168.0.240:44830 dest: /192.168.0.162:50010
2019-08-24 18:56:24,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44830, dest: /192.168.0.162:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741883_1059, duration: 3006627
2019-08-24 18:56:24,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741883_1059, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741884_1060 src: /192.168.0.240:44834 dest: /192.168.0.162:50010
2019-08-24 18:56:24,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44834, dest: /192.168.0.162:50010, bytes: 6058, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741884_1060, duration: 3464907
2019-08-24 18:56:24,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741885_1061 src: /192.168.0.240:44838 dest: /192.168.0.162:50010
2019-08-24 18:56:24,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44838, dest: /192.168.0.162:50010, bytes: 117110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741885_1061, duration: 5151464
2019-08-24 18:56:24,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741885_1061, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741886_1062 src: /192.168.0.240:44842 dest: /192.168.0.162:50010
2019-08-24 18:56:24,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44842, dest: /192.168.0.162:50010, bytes: 2031, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741886_1062, duration: 5549004
2019-08-24 18:56:24,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741886_1062, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741887_1063 src: /192.168.0.240:44846 dest: /192.168.0.162:50010
2019-08-24 18:56:24,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44846, dest: /192.168.0.162:50010, bytes: 2031, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741887_1063, duration: 5949520
2019-08-24 18:56:24,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741888_1064 src: /192.168.0.240:44850 dest: /192.168.0.162:50010
2019-08-24 18:56:24,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44850, dest: /192.168.0.162:50010, bytes: 2039, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741888_1064, duration: 2299916
2019-08-24 18:56:24,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741888_1064, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741889_1065 src: /192.168.0.240:44854 dest: /192.168.0.162:50010
2019-08-24 18:56:24,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44854, dest: /192.168.0.162:50010, bytes: 2039, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741889_1065, duration: 2329515
2019-08-24 18:56:24,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741889_1065, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741890_1066 src: /192.168.0.240:44858 dest: /192.168.0.162:50010
2019-08-24 18:56:24,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44858, dest: /192.168.0.162:50010, bytes: 1412178, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741890_1066, duration: 27822077
2019-08-24 18:56:24,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741890_1066, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741891_1067 src: /192.168.0.240:44862 dest: /192.168.0.162:50010
2019-08-24 18:56:24,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44862, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741891_1067, duration: 3345401
2019-08-24 18:56:24,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741891_1067, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:24,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741892_1068 src: /192.168.0.240:44866 dest: /192.168.0.162:50010
2019-08-24 18:56:24,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44866, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741892_1068, duration: 7706015
2019-08-24 18:56:24,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741892_1068, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741893_1069 src: /192.168.0.240:44870 dest: /192.168.0.162:50010
2019-08-24 18:56:25,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44870, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741893_1069, duration: 3279928
2019-08-24 18:56:25,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741893_1069, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741894_1070 src: /192.168.0.240:44874 dest: /192.168.0.162:50010
2019-08-24 18:56:25,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44874, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741894_1070, duration: 5908492
2019-08-24 18:56:25,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741894_1070, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741895_1071 src: /192.168.0.240:44878 dest: /192.168.0.162:50010
2019-08-24 18:56:25,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44878, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741895_1071, duration: 1603260
2019-08-24 18:56:25,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741895_1071, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741896_1072 src: /192.168.0.240:44882 dest: /192.168.0.162:50010
2019-08-24 18:56:25,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44882, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741896_1072, duration: 2051893
2019-08-24 18:56:25,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741896_1072, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741897_1073 src: /192.168.0.240:44886 dest: /192.168.0.162:50010
2019-08-24 18:56:25,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44886, dest: /192.168.0.162:50010, bytes: 2616445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741897_1073, duration: 51536889
2019-08-24 18:56:25,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741897_1073, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741898_1074 src: /192.168.0.240:44890 dest: /192.168.0.162:50010
2019-08-24 18:56:25,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44890, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741898_1074, duration: 5441763
2019-08-24 18:56:25,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741898_1074, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741899_1075 src: /192.168.0.240:44894 dest: /192.168.0.162:50010
2019-08-24 18:56:25,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44894, dest: /192.168.0.162:50010, bytes: 2078, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741899_1075, duration: 2487997
2019-08-24 18:56:25,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741899_1075, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741900_1076 src: /192.168.0.240:44898 dest: /192.168.0.162:50010
2019-08-24 18:56:25,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44898, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741900_1076, duration: 1618853
2019-08-24 18:56:25,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741900_1076, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741901_1077 src: /192.168.0.240:44902 dest: /192.168.0.162:50010
2019-08-24 18:56:25,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44902, dest: /192.168.0.162:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741901_1077, duration: 2292428
2019-08-24 18:56:25,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741901_1077, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741902_1078 src: /192.168.0.240:44906 dest: /192.168.0.162:50010
2019-08-24 18:56:25,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44906, dest: /192.168.0.162:50010, bytes: 2086, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741902_1078, duration: 1807871
2019-08-24 18:56:25,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741902_1078, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:25,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741903_1079 src: /192.168.0.240:44910 dest: /192.168.0.162:50010
2019-08-24 18:56:25,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44910, dest: /192.168.0.162:50010, bytes: 2086, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-789433466_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741903_1079, duration: 10641054
2019-08-24 18:56:25,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741903_1079, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 18:56:31,811 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741885_1061
2019-08-24 18:56:31,811 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741874_1050
2019-08-24 18:56:31,811 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741882_1058
2019-08-24 18:56:31,812 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741884_1060
2019-08-24 18:56:33,820 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741875_1051
2019-08-24 18:56:33,821 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741883_1059
2019-08-24 18:56:33,827 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741870_1046
2019-08-24 18:56:33,827 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741896_1072
2019-08-24 18:56:35,214 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741868_1044
2019-08-24 19:06:27,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741904_1080 src: /192.168.0.240:44944 dest: /192.168.0.162:50010
2019-08-24 19:06:27,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.240:44944, dest: /192.168.0.162:50010, bytes: 1492141, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-629272240_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741904_1080, duration: 55845785
2019-08-24 19:06:27,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741904_1080, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-08-24 19:06:37,111 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741904_1080
2019-08-24 19:07:10,895 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.0.162"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-08-24 19:07:14,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:15,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:16,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:17,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:18,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:19,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:20,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:21,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:22,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.0.240:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-08-24 19:07:23,096 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: RemoteException in offerService
org.apache.hadoop.ipc.RemoteException(java.io.IOException): NameNode still not started
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup(NameNodeRpcServer.java:1632)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendHeartbeat(NameNodeRpcServer.java:1151)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.sendHeartbeat(DatanodeProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:27331)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.call(Client.java:1470)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
2019-08-24 19:07:24,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from um1/192.168.0.240:9000 with active state
2019-08-24 19:07:24,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.240:9000 beginning handshake with NN
2019-08-24 19:07:24,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.240:9000 successfully registered with NN
2019-08-24 19:07:24,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8628bb1b693,  containing 1 storage report(s), of which we sent 1. The reports had 52 total blocks and used 1 RPC(s). This took 0 msec to generate and 9 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-08-24 19:07:24,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-08 06:37:53,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.163
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-08 06:37:53,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-08 06:37:53,705 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-08 06:37:54,117 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-08 06:37:54,223 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-08 06:37:54,223 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-08 06:37:54,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-08 06:37:54,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-08 06:37:54,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-08 06:37:54,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-08 06:37:54,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-08 06:37:54,396 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-08 06:37:54,401 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-08 06:37:54,411 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-08 06:37:54,417 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-08 06:37:54,417 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-08 06:37:54,417 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-08 06:37:54,441 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-08 06:37:54,444 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-08 06:37:54,444 INFO org.mortbay.log: jetty-6.1.26
2019-09-08 06:37:54,713 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-08 06:37:55,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-08 06:37:55,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-08 06:37:55,182 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-08 06:37:55,207 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-08 06:37:55,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-08 06:37:55,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-08 06:37:55,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-08 06:37:55,307 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-08 06:37:55,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.240:9000 starting to offer service
2019-09-08 06:37:55,345 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-08 06:37:55,346 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-08 06:37:55,628 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode2/in_use.lock acquired by nodename 17268@um2
2019-09-08 06:37:55,677 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-8011334-192.168.0.240-1566658908132
2019-09-08 06:37:55,677 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132
2019-09-08 06:37:55,677 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-08 06:37:55,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=322677011;bpid=BP-8011334-192.168.0.240-1566658908132;lv=-56;nsInfo=lv=-60;cid=CID-403a0251-1604-44a0-b205-ccebae382a38;nsid=322677011;c=0;bpid=BP-8011334-192.168.0.240-1566658908132;dnuuid=66e04227-1fc5-44d2-a340-3770e34fbbb7
2019-09-08 06:37:55,691 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-08 06:37:55,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode2/current
2019-09-08 06:37:55,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode2/current, StorageType: DISK
2019-09-08 06:37:55,744 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-08 06:37:55,744 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-8011334-192.168.0.240-1566658908132
2019-09-08 06:37:55,747 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-09-08 06:37:55,771 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-8011334-192.168.0.240-1566658908132 on /abc/datanode2/current: 24ms
2019-09-08 06:37:55,771 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-8011334-192.168.0.240-1566658908132: 27ms
2019-09-08 06:37:55,772 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-09-08 06:37:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current: 10ms
2019-09-08 06:37:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 11ms
2019-09-08 06:37:55,786 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1567931227786 with interval 21600000
2019-09-08 06:37:55,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.240:9000 beginning handshake with NN
2019-09-08 06:37:55,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.240:9000 successfully registered with NN
2019-09-08 06:37:55,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.0.240:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-08 06:37:55,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.240:9000 trying to claim ACTIVE state with txid=611
2019-09-08 06:37:55,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.240:9000
2019-09-08 06:37:55,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x363662f2301,  containing 1 storage report(s), of which we sent 1. The reports had 52 total blocks and used 1 RPC(s). This took 2 msec to generate and 35 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-08 06:37:55,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-08 06:37:55,914 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-08 06:37:55,914 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-08 06:37:55,916 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-08 06:37:55,917 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-08 06:37:55,917 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-08 06:37:55,922 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-8011334-192.168.0.240-1566658908132 to blockPoolScannerMap, new size=1
2019-09-08 06:38:00,843 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741867_1043
2019-09-08 06:38:00,849 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741895_1071
2019-09-08 06:38:00,855 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741900_1076
2019-09-08 06:38:00,856 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741891_1067
2019-09-08 06:38:00,858 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741887_1063
2019-09-08 06:38:00,859 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741892_1068
2019-09-08 06:38:00,861 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741903_1079
2019-09-08 06:38:28,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.0.163, datanodeUuid=66e04227-1fc5-44d2-a340-3770e34fbbb7, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-403a0251-1604-44a0-b205-ccebae382a38;nsid=322677011;c=0) Starting thread to transfer BP-8011334-192.168.0.240-1566658908132:blk_1073741826_1002 to 192.168.0.240:50010 
2019-09-08 06:38:28,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.0.163, datanodeUuid=66e04227-1fc5-44d2-a340-3770e34fbbb7, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-403a0251-1604-44a0-b205-ccebae382a38;nsid=322677011;c=0) Starting thread to transfer BP-8011334-192.168.0.240-1566658908132:blk_1073741825_1001 to 192.168.0.240:50010 
2019-09-08 06:38:28,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-8011334-192.168.0.240-1566658908132:blk_1073741825_1001 (numBytes=66) to /192.168.0.240:50010
2019-09-08 06:38:28,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-8011334-192.168.0.240-1566658908132:blk_1073741826_1002 (numBytes=5812) to /192.168.0.240:50010
2019-09-08 06:40:07,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2019-09-08 06:40:07,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2019-09-08 06:40:07,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2019-09-08 06:40:07,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741888_1064 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741888
2019-09-08 06:40:07,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2019-09-08 06:40:07,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741889_1065 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741889
2019-09-08 06:40:07,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2019-09-08 06:40:07,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741890_1066 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741890
2019-09-08 06:40:07,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741891_1067 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741891
2019-09-08 06:40:07,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741892_1068 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741892
2019-09-08 06:40:07,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2019-09-08 06:40:07,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741893_1069 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741893
2019-09-08 06:40:07,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2019-09-08 06:40:07,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2019-09-08 06:40:07,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741894_1070 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741894
2019-09-08 06:40:07,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741896_1072 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741896 for deletion
2019-09-08 06:40:07,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741895_1071 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741895
2019-09-08 06:40:07,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741896_1072 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741896
2019-09-08 06:40:07,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741897_1073 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741897 for deletion
2019-09-08 06:40:07,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741898_1074 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741898 for deletion
2019-09-08 06:40:07,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2019-09-08 06:40:07,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741900_1076 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741900 for deletion
2019-09-08 06:40:07,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2019-09-08 06:40:07,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2019-09-08 06:40:07,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2019-09-08 06:40:07,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741897_1073 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741897
2019-09-08 06:40:07,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741898_1074 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741898
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741899_1075 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741899
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741900_1076 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741900
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741901_1077 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741901
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741902_1078 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741902
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741903_1079 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741903
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741868_1044 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741868
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741869_1045 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741869
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741870_1046 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741870
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741871_1047 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741871
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2019-09-08 06:40:07,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741872_1048 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741872
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741873_1049 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741873
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741874_1050 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741874
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741875_1051 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741875
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741876_1052 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741876
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741877_1053 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741877
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2019-09-08 06:40:07,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741878_1054 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741878
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741879_1055 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741879
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741882_1058 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741882 for deletion
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741880_1056 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741880
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741881_1057 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741881
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741882_1058 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741882
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2019-09-08 06:40:07,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741883_1059 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741883
2019-09-08 06:40:07,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741884_1060 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741884
2019-09-08 06:40:08,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2019-09-08 06:40:08,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2019-09-08 06:40:08,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2019-09-08 06:40:08,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741885_1061 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741885
2019-09-08 06:40:08,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741886_1062 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741886
2019-09-08 06:40:08,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-8011334-192.168.0.240-1566658908132 blk_1073741887_1063 file /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current/finalized/subdir0/subdir0/blk_1073741887
2019-09-13 22:19:22,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.19
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-13 22:19:22,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-13 22:19:22,680 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-13 22:19:23,135 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-13 22:19:23,336 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-13 22:19:23,336 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-13 22:19:23,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-13 22:19:23,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-13 22:19:23,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-13 22:19:23,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-13 22:19:23,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-13 22:19:23,943 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-13 22:19:23,946 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-13 22:19:23,954 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-13 22:19:23,957 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-13 22:19:23,957 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-13 22:19:23,957 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-13 22:19:23,971 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-13 22:19:23,973 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-13 22:19:23,974 INFO org.mortbay.log: jetty-6.1.26
2019-09-13 22:19:24,241 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-13 22:19:24,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-13 22:19:24,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-13 22:19:24,677 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-13 22:19:24,694 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-13 22:19:24,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-13 22:19:24,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-13 22:19:24,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-13 22:19:24,775 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-13 22:19:24,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.18:9000 starting to offer service
2019-09-13 22:19:24,799 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-13 22:19:24,800 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-13 22:19:25,110 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode2/in_use.lock acquired by nodename 17254@um2
2019-09-13 22:19:25,158 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-8011334-192.168.0.240-1566658908132
2019-09-13 22:19:25,158 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132
2019-09-13 22:19:25,160 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-13 22:19:25,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=322677011;bpid=BP-8011334-192.168.0.240-1566658908132;lv=-56;nsInfo=lv=-60;cid=CID-403a0251-1604-44a0-b205-ccebae382a38;nsid=322677011;c=0;bpid=BP-8011334-192.168.0.240-1566658908132;dnuuid=66e04227-1fc5-44d2-a340-3770e34fbbb7
2019-09-13 22:19:25,172 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-13 22:19:25,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode2/current
2019-09-13 22:19:25,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode2/current, StorageType: DISK
2019-09-13 22:19:25,254 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-13 22:19:25,254 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-8011334-192.168.0.240-1566658908132
2019-09-13 22:19:25,255 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-09-13 22:19:25,269 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-8011334-192.168.0.240-1566658908132 on /abc/datanode2/current: 14ms
2019-09-13 22:19:25,270 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-8011334-192.168.0.240-1566658908132: 15ms
2019-09-13 22:19:25,270 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-09-13 22:19:25,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current: 5ms
2019-09-13 22:19:25,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2019-09-13 22:19:25,278 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1568420055278 with interval 21600000
2019-09-13 22:19:25,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.18:9000 beginning handshake with NN
2019-09-13 22:19:25,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.18:9000 successfully registered with NN
2019-09-13 22:19:25,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.0.18:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-13 22:19:25,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.18:9000 trying to claim ACTIVE state with txid=629
2019-09-13 22:19:25,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.18:9000
2019-09-13 22:19:25,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x44814f25db,  containing 1 storage report(s), of which we sent 1. The reports had 16 total blocks and used 1 RPC(s). This took 1 msec to generate and 28 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-13 22:19:25,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-13 22:19:25,385 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-13 22:19:25,385 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-13 22:19:25,387 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-13 22:19:25,387 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-13 22:19:25,387 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-13 22:19:25,392 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-8011334-192.168.0.240-1566658908132 to blockPoolScannerMap, new size=1
2019-09-13 22:19:30,347 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741861_1037
2019-09-14 00:38:29,977 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.0.19"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-09-14 00:38:32,725 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-14 00:38:32,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.0.19
************************************************************/
2019-09-14 00:39:45,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.19
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-14 00:39:45,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-14 00:39:45,785 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-14 00:39:46,056 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-14 00:39:46,114 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-14 00:39:46,114 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-14 00:39:46,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-14 00:39:46,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-14 00:39:46,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-14 00:39:46,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-14 00:39:46,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-14 00:39:46,208 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-14 00:39:46,211 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-14 00:39:46,219 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-14 00:39:46,221 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-14 00:39:46,222 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-14 00:39:46,222 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-14 00:39:46,233 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-14 00:39:46,235 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-14 00:39:46,235 INFO org.mortbay.log: jetty-6.1.26
2019-09-14 00:39:46,425 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-14 00:39:46,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-14 00:39:46,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-14 00:39:46,808 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-14 00:39:46,834 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-14 00:39:46,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-14 00:39:46,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-14 00:39:46,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-14 00:39:46,910 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-14 00:39:46,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.18:9000 starting to offer service
2019-09-14 00:39:46,931 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-14 00:39:46,931 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-14 00:39:47,233 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode2/in_use.lock acquired by nodename 20355@um2
2019-09-14 00:39:47,283 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-8011334-192.168.0.240-1566658908132
2019-09-14 00:39:47,283 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132
2019-09-14 00:39:47,283 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-14 00:39:47,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=322677011;bpid=BP-8011334-192.168.0.240-1566658908132;lv=-56;nsInfo=lv=-60;cid=CID-403a0251-1604-44a0-b205-ccebae382a38;nsid=322677011;c=0;bpid=BP-8011334-192.168.0.240-1566658908132;dnuuid=66e04227-1fc5-44d2-a340-3770e34fbbb7
2019-09-14 00:39:47,292 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-14 00:39:47,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode2/current
2019-09-14 00:39:47,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode2/current, StorageType: DISK
2019-09-14 00:39:47,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-14 00:39:47,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-8011334-192.168.0.240-1566658908132
2019-09-14 00:39:47,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-09-14 00:39:47,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132/current: 5574656
2019-09-14 00:39:47,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-8011334-192.168.0.240-1566658908132 on /abc/datanode2/current: 11ms
2019-09-14 00:39:47,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-8011334-192.168.0.240-1566658908132: 12ms
2019-09-14 00:39:47,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-09-14 00:39:47,358 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current: 4ms
2019-09-14 00:39:47,358 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2019-09-14 00:39:47,361 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1568418413361 with interval 21600000
2019-09-14 00:39:47,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.18:9000 beginning handshake with NN
2019-09-14 00:39:47,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.18:9000 successfully registered with NN
2019-09-14 00:39:47,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.0.18:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-14 00:39:47,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.18:9000 trying to claim ACTIVE state with txid=680
2019-09-14 00:39:47,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.18:9000
2019-09-14 00:39:47,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7ed6f7b5a47,  containing 1 storage report(s), of which we sent 1. The reports had 16 total blocks and used 1 RPC(s). This took 1 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-14 00:39:47,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-14 00:39:47,536 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-14 00:39:47,536 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-14 00:39:47,537 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-14 00:39:47,537 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-14 00:39:47,537 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-14 00:39:47,541 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-8011334-192.168.0.240-1566658908132 to blockPoolScannerMap, new size=1
2019-09-14 00:46:53,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x850bc81d865,  containing 1 storage report(s), of which we sent 1. The reports had 16 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-14 00:46:53,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-14 01:00:28,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741905_1081 src: /192.168.0.18:60860 dest: /192.168.0.19:50010
2019-09-14 01:00:28,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.18:60860, dest: /192.168.0.19:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1740172404_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741905_1081, duration: 111543591
2019-09-14 01:00:28,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741905_1081, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-14 01:00:34,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741906_1082 src: /192.168.0.18:60864 dest: /192.168.0.19:50010
2019-09-14 01:00:34,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.18:60864, dest: /192.168.0.19:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1740172404_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741906_1082, duration: 7633988
2019-09-14 01:00:34,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741906_1082, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-14 05:47:15,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.19
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-14 05:47:15,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-14 05:47:15,541 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-14 05:47:16,081 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-14 05:47:16,229 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-14 05:47:16,229 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-14 05:47:16,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-14 05:47:16,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-14 05:47:16,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-14 05:47:16,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-14 05:47:16,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-14 05:47:16,401 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-14 05:47:16,404 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-14 05:47:16,413 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-14 05:47:16,415 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-14 05:47:16,415 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-14 05:47:16,416 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-14 05:47:16,437 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-14 05:47:16,440 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-14 05:47:16,440 INFO org.mortbay.log: jetty-6.1.26
2019-09-14 05:47:16,727 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-14 05:47:17,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-14 05:47:17,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-14 05:47:17,243 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-14 05:47:17,265 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-14 05:47:17,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-14 05:47:17,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-14 05:47:17,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-14 05:47:17,343 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-14 05:47:17,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.18:9000 starting to offer service
2019-09-14 05:47:17,374 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-14 05:47:17,378 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-14 05:47:17,799 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /abc/datanode2/in_use.lock acquired by nodename 2304@um2
2019-09-14 05:47:17,877 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-8011334-192.168.0.240-1566658908132
2019-09-14 05:47:17,877 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /abc/datanode2/current/BP-8011334-192.168.0.240-1566658908132
2019-09-14 05:47:17,878 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-14 05:47:17,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=322677011;bpid=BP-8011334-192.168.0.240-1566658908132;lv=-56;nsInfo=lv=-60;cid=CID-403a0251-1604-44a0-b205-ccebae382a38;nsid=322677011;c=0;bpid=BP-8011334-192.168.0.240-1566658908132;dnuuid=66e04227-1fc5-44d2-a340-3770e34fbbb7
2019-09-14 05:47:17,895 WARN org.apache.hadoop.hdfs.server.common.Util: Path /abc/datanode2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-14 05:47:17,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /abc/datanode2/current
2019-09-14 05:47:17,932 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /abc/datanode2/current, StorageType: DISK
2019-09-14 05:47:17,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-14 05:47:17,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-8011334-192.168.0.240-1566658908132
2019-09-14 05:47:17,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-09-14 05:47:18,030 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-8011334-192.168.0.240-1566658908132 on /abc/datanode2/current: 37ms
2019-09-14 05:47:18,030 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-8011334-192.168.0.240-1566658908132: 38ms
2019-09-14 05:47:18,030 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current...
2019-09-14 05:47:18,038 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-8011334-192.168.0.240-1566658908132 on volume /abc/datanode2/current: 7ms
2019-09-14 05:47:18,038 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2019-09-14 05:47:18,041 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1568436745041 with interval 21600000
2019-09-14 05:47:18,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.18:9000 beginning handshake with NN
2019-09-14 05:47:18,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid null) service to um1/192.168.0.18:9000 successfully registered with NN
2019-09-14 05:47:18,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.0.18:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-14 05:47:18,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.18:9000 trying to claim ACTIVE state with txid=719
2019-09-14 05:47:18,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-8011334-192.168.0.240-1566658908132 (Datanode Uuid 66e04227-1fc5-44d2-a340-3770e34fbbb7) service to um1/192.168.0.18:9000
2019-09-14 05:47:18,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x30aa000112,  containing 1 storage report(s), of which we sent 1. The reports had 18 total blocks and used 1 RPC(s). This took 1 msec to generate and 34 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-14 05:47:18,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-14 05:47:18,202 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-14 05:47:18,202 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-14 05:47:18,204 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-14 05:47:18,204 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-14 05:47:18,205 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-8011334-192.168.0.240-1566658908132
2019-09-14 05:47:18,211 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-8011334-192.168.0.240-1566658908132 to blockPoolScannerMap, new size=1
2019-09-14 05:47:23,069 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-8011334-192.168.0.240-1566658908132:blk_1073741867_1043
2019-09-14 06:01:14,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741907_1083 src: /192.168.0.18:60586 dest: /192.168.0.19:50010
2019-09-14 06:01:15,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.18:60586, dest: /192.168.0.19:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1858332151_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741907_1083, duration: 117759880
2019-09-14 06:01:15,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741907_1083, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-14 06:18:19,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-8011334-192.168.0.240-1566658908132:blk_1073741908_1084 src: /192.168.0.18:60728 dest: /192.168.0.19:50010
2019-09-14 06:18:19,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.0.18:60728, dest: /192.168.0.19:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_533262011_1, offset: 0, srvID: 66e04227-1fc5-44d2-a340-3770e34fbbb7, blockid: BP-8011334-192.168.0.240-1566658908132:blk_1073741908_1084, duration: 52384460
2019-09-14 06:18:19,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-8011334-192.168.0.240-1566658908132:blk_1073741908_1084, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-14 06:52:25,068 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-8011334-192.168.0.240-1566658908132 Total blocks: 20, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-09-22 17:00:38,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.19
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-22 17:00:38,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-22 17:00:38,787 WARN org.apache.hadoop.hdfs.server.common.Util: Path /org/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-22 17:00:39,360 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-22 17:00:39,520 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-22 17:00:39,520 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-22 17:00:39,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-22 17:00:39,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-22 17:00:39,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-22 17:00:39,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-22 17:00:39,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-22 17:00:39,727 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-22 17:00:39,731 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-22 17:00:39,742 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-22 17:00:39,745 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-22 17:00:39,745 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-22 17:00:39,745 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-22 17:00:39,765 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-22 17:00:39,769 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-22 17:00:39,770 INFO org.mortbay.log: jetty-6.1.26
2019-09-22 17:00:40,142 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-22 17:00:40,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-22 17:00:40,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-22 17:00:40,712 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-22 17:00:40,727 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-22 17:00:40,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-22 17:00:40,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-22 17:00:40,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-22 17:00:40,826 WARN org.apache.hadoop.hdfs.server.common.Util: Path /org/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-22 17:00:40,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.0.242:9000 starting to offer service
2019-09-22 17:00:40,851 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-22 17:00:40,852 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-22 17:00:41,237 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /org/data2/in_use.lock acquired by nodename 2443@um2
2019-09-22 17:00:41,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /org/data2 is not formatted for BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:00:41,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-09-22 17:00:41,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:00:41,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /org/data2/current/BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:00:41,328 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /org/data2/current/BP-141445087-192.168.0.242-1569164288494 is not formatted for BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:00:41,328 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-09-22 17:00:41,328 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-141445087-192.168.0.242-1569164288494 directory /org/data2/current/BP-141445087-192.168.0.242-1569164288494/current
2019-09-22 17:00:41,330 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-22 17:00:41,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1637254800;bpid=BP-141445087-192.168.0.242-1569164288494;lv=-56;nsInfo=lv=-60;cid=CID-8a4da663-132b-4526-a234-04cd4d0a14a2;nsid=1637254800;c=0;bpid=BP-141445087-192.168.0.242-1569164288494;dnuuid=null
2019-09-22 17:00:41,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID ed9d1ea8-4d9a-4b7c-b7b8-1b7bcec5b74f
2019-09-22 17:00:41,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /org/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-22 17:00:41,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /org/data2/current
2019-09-22 17:00:41,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /org/data2/current, StorageType: DISK
2019-09-22 17:00:41,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-22 17:00:41,381 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:00:41,381 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-141445087-192.168.0.242-1569164288494 on volume /org/data2/current...
2019-09-22 17:00:41,400 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-141445087-192.168.0.242-1569164288494 on /org/data2/current: 18ms
2019-09-22 17:00:41,400 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-141445087-192.168.0.242-1569164288494: 19ms
2019-09-22 17:00:41,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-141445087-192.168.0.242-1569164288494 on volume /org/data2/current...
2019-09-22 17:00:41,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-141445087-192.168.0.242-1569164288494 on volume /org/data2/current: 0ms
2019-09-22 17:00:41,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2019-09-22 17:00:41,404 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569172801403 with interval 21600000
2019-09-22 17:00:41,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-141445087-192.168.0.242-1569164288494 (Datanode Uuid null) service to um1/192.168.0.242:9000 beginning handshake with NN
2019-09-22 17:00:41,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-141445087-192.168.0.242-1569164288494 (Datanode Uuid null) service to um1/192.168.0.242:9000 successfully registered with NN
2019-09-22 17:00:41,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.0.242:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-22 17:00:41,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-141445087-192.168.0.242-1569164288494 (Datanode Uuid ed9d1ea8-4d9a-4b7c-b7b8-1b7bcec5b74f) service to um1/192.168.0.242:9000 trying to claim ACTIVE state with txid=1
2019-09-22 17:00:41,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-141445087-192.168.0.242-1569164288494 (Datanode Uuid ed9d1ea8-4d9a-4b7c-b7b8-1b7bcec5b74f) service to um1/192.168.0.242:9000
2019-09-22 17:00:41,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x27569f97bcd,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 46 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-22 17:00:41,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:00:41,551 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-22 17:00:41,551 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-22 17:00:41,554 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-22 17:00:41,554 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-22 17:00:41,555 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:00:41,558 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-141445087-192.168.0.242-1569164288494 to blockPoolScannerMap, new size=1
2019-09-22 17:14:05,763 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-22 17:14:05,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.0.19
************************************************************/
2019-09-22 17:14:16,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.0.19
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-22 17:14:16,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-22 17:14:16,731 WARN org.apache.hadoop.hdfs.server.common.Util: Path /org/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-22 17:14:17,055 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-22 17:14:17,159 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-22 17:14:17,159 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-22 17:14:17,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-22 17:14:17,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-22 17:14:17,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-22 17:14:17,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-22 17:14:17,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-22 17:14:17,255 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-22 17:14:17,258 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-22 17:14:17,266 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-22 17:14:17,269 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-22 17:14:17,269 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-22 17:14:17,269 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-22 17:14:17,283 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-22 17:14:17,285 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-22 17:14:17,285 INFO org.mortbay.log: jetty-6.1.26
2019-09-22 17:14:17,476 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-22 17:14:17,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-22 17:14:17,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-22 17:14:17,844 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-22 17:14:17,859 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-22 17:14:17,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-22 17:14:17,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-22 17:14:17,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-22 17:14:17,921 WARN org.apache.hadoop.hdfs.server.common.Util: Path /org/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-22 17:14:17,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um2/192.168.0.19:9000 starting to offer service
2019-09-22 17:14:17,945 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-22 17:14:17,945 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-22 17:14:18,158 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /org/data2/in_use.lock acquired by nodename 3582@um2
2019-09-22 17:14:18,225 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:14:18,225 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /org/data2/current/BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:14:18,225 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-22 17:14:18,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1637254800;bpid=BP-141445087-192.168.0.242-1569164288494;lv=-56;nsInfo=lv=-60;cid=CID-8a4da663-132b-4526-a234-04cd4d0a14a2;nsid=1637254800;c=0;bpid=BP-141445087-192.168.0.242-1569164288494;dnuuid=ed9d1ea8-4d9a-4b7c-b7b8-1b7bcec5b74f
2019-09-22 17:14:18,238 WARN org.apache.hadoop.hdfs.server.common.Util: Path /org/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-22 17:14:18,255 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /org/data2/current
2019-09-22 17:14:18,255 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /org/data2/current, StorageType: DISK
2019-09-22 17:14:18,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-22 17:14:18,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:14:18,287 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-141445087-192.168.0.242-1569164288494 on volume /org/data2/current...
2019-09-22 17:14:18,298 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /org/data2/current/BP-141445087-192.168.0.242-1569164288494/current: 24576
2019-09-22 17:14:18,305 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-141445087-192.168.0.242-1569164288494 on /org/data2/current: 18ms
2019-09-22 17:14:18,305 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-141445087-192.168.0.242-1569164288494: 21ms
2019-09-22 17:14:18,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-141445087-192.168.0.242-1569164288494 on volume /org/data2/current...
2019-09-22 17:14:18,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-141445087-192.168.0.242-1569164288494 on volume /org/data2/current: 0ms
2019-09-22 17:14:18,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2019-09-22 17:14:18,314 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569186245314 with interval 21600000
2019-09-22 17:14:18,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-141445087-192.168.0.242-1569164288494 (Datanode Uuid null) service to um2/192.168.0.19:9000 beginning handshake with NN
2019-09-22 17:14:18,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-141445087-192.168.0.242-1569164288494 (Datanode Uuid null) service to um2/192.168.0.19:9000 successfully registered with NN
2019-09-22 17:14:18,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um2/192.168.0.19:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-22 17:14:18,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-141445087-192.168.0.242-1569164288494 (Datanode Uuid ed9d1ea8-4d9a-4b7c-b7b8-1b7bcec5b74f) service to um2/192.168.0.19:9000 trying to claim ACTIVE state with txid=3
2019-09-22 17:14:18,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-141445087-192.168.0.242-1569164288494 (Datanode Uuid ed9d1ea8-4d9a-4b7c-b7b8-1b7bcec5b74f) service to um2/192.168.0.19:9000
2019-09-22 17:14:18,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3339fd6f8b1,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 35 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-22 17:14:18,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:14:18,483 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-22 17:14:18,483 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-22 17:14:18,484 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-22 17:14:18,484 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-22 17:14:18,484 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-141445087-192.168.0.242-1569164288494
2019-09-22 17:14:18,488 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-141445087-192.168.0.242-1569164288494 to blockPoolScannerMap, new size=1
2019-09-28 17:58:32,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-28 17:58:32,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-28 17:58:32,859 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 17:58:33,645 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-28 17:58:33,843 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-28 17:58:33,843 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-28 17:58:33,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-28 17:58:33,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-28 17:58:33,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-28 17:58:33,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-28 17:58:33,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-28 17:58:34,106 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-28 17:58:34,111 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-28 17:58:34,122 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 17:58:34,125 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-28 17:58:34,125 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 17:58:34,125 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 17:58:34,144 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-28 17:58:34,146 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-28 17:58:34,147 INFO org.mortbay.log: jetty-6.1.26
2019-09-28 17:58:34,699 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-28 17:58:35,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-28 17:58:35,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-28 17:58:35,599 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-28 17:58:35,616 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-28 17:58:35,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-28 17:58:35,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-28 17:58:35,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-28 17:58:35,742 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 17:58:35,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-28 17:58:35,843 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-28 17:58:35,843 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-28 17:58:37,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:38,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:39,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:40,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:41,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:42,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:43,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:44,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:45,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:46,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:46,132 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 17:58:52,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:53,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:54,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:55,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:56,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:57,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:58,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:58:59,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:00,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:01,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:01,147 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 17:59:07,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:08,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:09,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:10,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:11,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:12,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:13,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:14,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:15,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:16,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:16,205 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 17:59:22,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:23,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:24,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:25,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:26,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:27,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:28,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:29,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:30,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:31,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:31,231 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 17:59:37,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:38,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:39,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:40,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:41,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:42,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:43,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:44,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:45,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:46,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:46,247 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 17:59:52,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:53,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:54,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:55,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:56,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:57,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:58,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 17:59:59,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:00,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:01,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:01,275 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:00:07,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:08,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:09,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:10,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:11,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:12,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:13,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:14,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:15,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:16,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:16,326 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:00:22,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:23,334 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:24,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:25,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:26,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:27,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:28,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:29,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:30,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:31,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:31,364 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:00:37,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:38,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:39,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:40,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:41,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:42,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:43,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:44,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:45,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:46,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:46,397 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:00:52,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:53,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:54,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:55,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:56,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:57,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:58,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:00:59,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:00,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:01,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:01,436 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:01:07,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:08,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:09,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:10,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:11,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:12,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:13,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:14,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:15,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:16,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:16,485 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:01:22,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:23,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:24,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:25,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:26,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:27,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:28,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:29,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:30,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:31,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:31,514 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:01:37,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:38,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:39,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:40,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:41,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:42,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:43,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:44,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:45,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:46,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:46,565 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:01:52,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:53,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:54,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:55,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:56,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:57,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:58,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:01:59,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:00,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:01,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:01,590 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:02:07,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:08,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:09,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:10,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:11,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:12,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:13,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:14,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:15,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:16,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:16,623 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:02:22,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:23,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:24,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:25,644 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:26,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:27,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:28,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:29,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:30,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:31,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:31,673 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-28 18:02:37,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:38,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:39,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:40,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:41,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:42,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:02:42,893 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-28 18:02:42,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.182.5
************************************************************/
2019-09-28 18:05:29,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-28 18:05:29,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-28 18:05:29,804 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:05:30,236 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-28 18:05:30,330 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-28 18:05:30,331 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-28 18:05:30,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-28 18:05:30,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-28 18:05:30,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-28 18:05:30,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-28 18:05:30,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-28 18:05:30,458 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-28 18:05:30,463 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-28 18:05:30,472 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 18:05:30,474 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-28 18:05:30,475 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 18:05:30,475 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 18:05:30,501 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-28 18:05:30,504 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-28 18:05:30,504 INFO org.mortbay.log: jetty-6.1.26
2019-09-28 18:05:30,833 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-28 18:05:31,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-28 18:05:31,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-28 18:05:31,319 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-28 18:05:31,338 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-28 18:05:31,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-28 18:05:31,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-28 18:05:31,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-28 18:05:31,403 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:05:31,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-28 18:05:31,422 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-28 18:05:31,431 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-28 18:05:31,732 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 6423@um2
2019-09-28 18:05:31,733 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /orgz/data2 is not formatted for BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:05:31,733 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-09-28 18:05:31,837 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:05:31,837 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:05:31,838 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /orgz/data2/current/BP-1953541993-192.168.182.3-1569686638162 is not formatted for BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:05:31,838 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-09-28 18:05:31,838 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1953541993-192.168.182.3-1569686638162 directory /orgz/data2/current/BP-1953541993-192.168.182.3-1569686638162/current
2019-09-28 18:05:31,857 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-28 18:05:31,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=991233186;bpid=BP-1953541993-192.168.182.3-1569686638162;lv=-56;nsInfo=lv=-60;cid=CID-754a4d87-2b19-4139-88e6-04e15df49eae;nsid=991233186;c=0;bpid=BP-1953541993-192.168.182.3-1569686638162;dnuuid=null
2019-09-28 18:05:31,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 045cc2e5-0565-4c47-8b70-045defcf3bbf
2019-09-28 18:05:31,876 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:05:31,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-09-28 18:05:31,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-09-28 18:05:31,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-28 18:05:31,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:05:31,942 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1953541993-192.168.182.3-1569686638162 on volume /orgz/data2/current...
2019-09-28 18:05:31,951 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1953541993-192.168.182.3-1569686638162 on /orgz/data2/current: 9ms
2019-09-28 18:05:31,951 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1953541993-192.168.182.3-1569686638162: 10ms
2019-09-28 18:05:31,952 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1953541993-192.168.182.3-1569686638162 on volume /orgz/data2/current...
2019-09-28 18:05:31,952 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1953541993-192.168.182.3-1569686638162 on volume /orgz/data2/current: 1ms
2019-09-28 18:05:31,952 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-09-28 18:05:31,955 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569701840955 with interval 21600000
2019-09-28 18:05:31,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1953541993-192.168.182.3-1569686638162 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-28 18:05:32,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1953541993-192.168.182.3-1569686638162 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-28 18:05:32,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-28 18:05:32,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1953541993-192.168.182.3-1569686638162 (Datanode Uuid 045cc2e5-0565-4c47-8b70-045defcf3bbf) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=1
2019-09-28 18:05:32,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1953541993-192.168.182.3-1569686638162 (Datanode Uuid 045cc2e5-0565-4c47-8b70-045defcf3bbf) service to um1/192.168.182.3:9000
2019-09-28 18:05:32,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x27f3434080d,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 67 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-28 18:05:32,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:05:32,255 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-28 18:05:32,255 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-28 18:05:32,256 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-28 18:05:32,256 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-28 18:05:32,257 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:05:32,262 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1953541993-192.168.182.3-1569686638162 to blockPoolScannerMap, new size=1
2019-09-28 18:09:56,453 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.182.5"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-09-28 18:10:00,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:10:01,338 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-28 18:10:01,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.182.5
************************************************************/
2019-09-28 18:11:11,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-28 18:11:11,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-28 18:11:11,828 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:11:12,235 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-28 18:11:12,358 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-28 18:11:12,358 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-28 18:11:12,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-28 18:11:12,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-28 18:11:12,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-28 18:11:12,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-28 18:11:12,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-28 18:11:12,462 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-28 18:11:12,467 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-28 18:11:12,477 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 18:11:12,479 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-28 18:11:12,480 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 18:11:12,480 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 18:11:12,492 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-28 18:11:12,494 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-28 18:11:12,494 INFO org.mortbay.log: jetty-6.1.26
2019-09-28 18:11:12,768 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-28 18:11:13,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-28 18:11:13,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-28 18:11:13,262 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-28 18:11:13,278 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-28 18:11:13,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-28 18:11:13,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-28 18:11:13,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-28 18:11:13,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:11:13,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-28 18:11:13,367 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-28 18:11:13,371 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-28 18:11:13,851 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 7623@um2
2019-09-28 18:11:13,929 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:11:13,930 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:11:13,931 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-28 18:11:13,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=991233186;bpid=BP-1953541993-192.168.182.3-1569686638162;lv=-56;nsInfo=lv=-60;cid=CID-754a4d87-2b19-4139-88e6-04e15df49eae;nsid=991233186;c=0;bpid=BP-1953541993-192.168.182.3-1569686638162;dnuuid=045cc2e5-0565-4c47-8b70-045defcf3bbf
2019-09-28 18:11:13,949 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:11:13,976 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-09-28 18:11:13,976 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-09-28 18:11:14,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-28 18:11:14,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:11:14,041 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1953541993-192.168.182.3-1569686638162 on volume /orgz/data2/current...
2019-09-28 18:11:14,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /orgz/data2/current/BP-1953541993-192.168.182.3-1569686638162/current: 24576
2019-09-28 18:11:14,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1953541993-192.168.182.3-1569686638162 on /orgz/data2/current: 15ms
2019-09-28 18:11:14,061 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1953541993-192.168.182.3-1569686638162: 20ms
2019-09-28 18:11:14,064 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1953541993-192.168.182.3-1569686638162 on volume /orgz/data2/current...
2019-09-28 18:11:14,064 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1953541993-192.168.182.3-1569686638162 on volume /orgz/data2/current: 0ms
2019-09-28 18:11:14,064 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2019-09-28 18:11:14,069 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569696948069 with interval 21600000
2019-09-28 18:11:14,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1953541993-192.168.182.3-1569686638162 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-28 18:11:14,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1953541993-192.168.182.3-1569686638162 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-28 18:11:14,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-28 18:11:14,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1953541993-192.168.182.3-1569686638162 (Datanode Uuid 045cc2e5-0565-4c47-8b70-045defcf3bbf) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=4
2019-09-28 18:11:14,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1953541993-192.168.182.3-1569686638162 (Datanode Uuid 045cc2e5-0565-4c47-8b70-045defcf3bbf) service to um1/192.168.182.3:9000
2019-09-28 18:11:14,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2cee274f1dc,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 149 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-28 18:11:14,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:11:14,565 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-28 18:11:14,565 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-28 18:11:14,567 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-28 18:11:14,567 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-28 18:11:14,567 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1953541993-192.168.182.3-1569686638162
2019-09-28 18:11:14,573 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1953541993-192.168.182.3-1569686638162 to blockPoolScannerMap, new size=1
2019-09-28 18:12:08,355 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.182.5"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-09-28 18:12:12,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 18:12:12,608 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-28 18:12:12,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.182.5
************************************************************/
2019-09-28 18:42:18,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-28 18:42:18,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-28 18:42:18,929 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:42:19,360 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-28 18:42:19,478 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-28 18:42:19,478 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-28 18:42:19,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-28 18:42:19,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-28 18:42:19,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-28 18:42:19,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-28 18:42:19,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-28 18:42:19,648 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-28 18:42:19,653 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-28 18:42:19,667 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 18:42:19,669 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-28 18:42:19,670 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 18:42:19,670 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 18:42:19,689 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-28 18:42:19,693 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-28 18:42:19,693 INFO org.mortbay.log: jetty-6.1.26
2019-09-28 18:42:20,178 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-28 18:42:20,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-28 18:42:20,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-28 18:42:21,015 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-28 18:42:21,045 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-28 18:42:21,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-28 18:42:21,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-28 18:42:21,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-28 18:42:21,252 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:42:21,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-28 18:42:21,266 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-28 18:42:21,268 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-28 18:42:21,813 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 8361@um2
2019-09-28 18:42:21,815 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /orgz/data2: namenode clusterID = CID-52b20ea5-265a-4894-9ae4-b6de307aaed4; datanode clusterID = CID-754a4d87-2b19-4139-88e6-04e15df49eae
2019-09-28 18:42:21,815 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:478)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1342)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1308)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:226)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:867)
	at java.lang.Thread.run(Thread.java:748)
2019-09-28 18:42:21,816 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000
2019-09-28 18:42:21,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2019-09-28 18:42:23,822 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-09-28 18:42:23,823 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-09-28 18:42:23,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.182.5
************************************************************/
2019-09-28 18:47:18,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-28 18:47:18,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-28 18:47:18,356 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:47:18,781 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-28 18:47:18,870 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-28 18:47:18,870 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-28 18:47:18,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-28 18:47:18,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-28 18:47:18,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-28 18:47:18,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-28 18:47:18,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-28 18:47:18,952 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-28 18:47:18,956 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-28 18:47:18,964 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 18:47:18,966 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-28 18:47:18,967 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 18:47:18,967 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 18:47:18,981 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-28 18:47:18,983 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-28 18:47:18,984 INFO org.mortbay.log: jetty-6.1.26
2019-09-28 18:47:19,174 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-28 18:47:19,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-28 18:47:19,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-28 18:47:19,552 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-28 18:47:19,566 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-28 18:47:19,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-28 18:47:19,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-28 18:47:19,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-28 18:47:19,625 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:47:19,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-28 18:47:19,639 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-28 18:47:19,651 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-28 18:47:19,909 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 9207@um2
2019-09-28 18:47:19,911 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /orgz/data2 is not formatted for BP-278062902-192.168.182.3-1569688905540
2019-09-28 18:47:19,911 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-09-28 18:47:19,960 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-09-28 18:47:19,960 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-09-28 18:47:19,961 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540 is not formatted for BP-278062902-192.168.182.3-1569688905540
2019-09-28 18:47:19,961 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2019-09-28 18:47:19,961 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-278062902-192.168.182.3-1569688905540 directory /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current
2019-09-28 18:47:19,963 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-28 18:47:19,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=null
2019-09-28 18:47:19,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-09-28 18:47:19,982 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-28 18:47:20,006 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-09-28 18:47:20,006 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-09-28 18:47:20,012 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-28 18:47:20,013 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-09-28 18:47:20,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-28 18:47:20,024 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 9ms
2019-09-28 18:47:20,024 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 11ms
2019-09-28 18:47:20,025 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-28 18:47:20,025 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 0ms
2019-09-28 18:47:20,025 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2019-09-28 18:47:20,028 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569693818028 with interval 21600000
2019-09-28 18:47:20,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-28 18:47:20,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-28 18:47:20,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-28 18:47:20,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=9
2019-09-28 18:47:20,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-09-28 18:47:20,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4c71e60c9db,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 18 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-28 18:47:20,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-28 18:47:20,099 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-28 18:47:20,099 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-28 18:47:20,101 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-28 18:47:20,101 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-28 18:47:20,101 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-28 18:47:20,105 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-09-28 19:05:26,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741826_1002 src: /192.168.182.3:45266 dest: /192.168.182.5:50010
2019-09-28 19:05:26,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45266, dest: /192.168.182.5:50010, bytes: 66, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1935528628_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741826_1002, duration: 56509923
2019-09-28 19:05:26,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:05:35,495 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741826_1002
2019-09-28 19:06:43,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741828_1004 src: /192.168.182.3:45278 dest: /192.168.182.5:50010
2019-09-28 19:06:43,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45278, dest: /192.168.182.5:50010, bytes: 3751306, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1740878836_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741828_1004, duration: 91237806
2019-09-28 19:06:43,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:06:48,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741828_1004 to 192.168.182.4:50010 
2019-09-28 19:06:52,606 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741828_1004
2019-09-28 19:07:07,239 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0):Failed to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741828_1004 to 192.168.182.4:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2045)
	at java.lang.Thread.run(Thread.java:748)
2019-09-28 19:07:07,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
2019-09-28 19:12:39,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741828_1004 to 192.168.182.4:50010 
2019-09-28 19:12:40,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741828_1004 (numBytes=3751306) to /192.168.182.4:50010
2019-09-28 19:33:53,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741829_1005 src: /192.168.182.3:45502 dest: /192.168.182.5:50010
2019-09-28 19:33:53,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45502, dest: /192.168.182.5:50010, bytes: 3751306, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741829_1005, duration: 142886317
2019-09-28 19:33:53,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:33:53,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741830_1006 src: /192.168.182.3:45506 dest: /192.168.182.5:50010
2019-09-28 19:33:53,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45506, dest: /192.168.182.5:50010, bytes: 3048, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741830_1006, duration: 5596163
2019-09-28 19:33:53,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:33:53,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741831_1007 src: /192.168.182.3:45510 dest: /192.168.182.5:50010
2019-09-28 19:33:53,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45510, dest: /192.168.182.5:50010, bytes: 123637, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741831_1007, duration: 2741625
2019-09-28 19:33:53,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:33:53,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741834_1010 src: /192.168.182.3:45522 dest: /192.168.182.5:50010
2019-09-28 19:33:53,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45522, dest: /192.168.182.5:50010, bytes: 130, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741834_1010, duration: 5666195
2019-09-28 19:33:53,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:33:53,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741835_1011 src: /192.168.182.3:45526 dest: /192.168.182.5:50010
2019-09-28 19:33:53,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45526, dest: /192.168.182.5:50010, bytes: 240, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741835_1011, duration: 6106664
2019-09-28 19:33:53,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:33:53,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741837_1013 src: /192.168.182.3:45534 dest: /192.168.182.5:50010
2019-09-28 19:33:53,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45534, dest: /192.168.182.5:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741837_1013, duration: 3732071
2019-09-28 19:33:53,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:33:53,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741839_1015 src: /192.168.182.3:45542 dest: /192.168.182.5:50010
2019-09-28 19:33:53,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45542, dest: /192.168.182.5:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741839_1015, duration: 8377307
2019-09-28 19:33:53,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:33:53,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741841_1017 src: /192.168.182.3:45550 dest: /192.168.182.5:50010
2019-09-28 19:33:53,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45550, dest: /192.168.182.5:50010, bytes: 334, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741841_1017, duration: 3346064
2019-09-28 19:33:53,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:33:53,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741842_1018 src: /192.168.182.3:45554 dest: /192.168.182.5:50010
2019-09-28 19:33:53,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:45554, dest: /192.168.182.5:50010, bytes: 615, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1539573792_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741842_1018, duration: 2775260
2019-09-28 19:33:53,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-28 19:34:01,924 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741829_1005
2019-09-28 19:34:01,925 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741831_1007
2019-09-28 19:34:01,925 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741830_1006
2019-09-28 19:34:06,934 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741834_1010
2019-09-28 19:34:06,934 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741841_1017
2019-09-28 19:34:06,935 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741839_1015
2019-09-28 19:34:06,935 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741837_1013
2019-09-28 19:34:06,935 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741842_1018
2019-09-28 19:34:06,936 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741835_1011
2019-09-28 19:47:51,129 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2019-09-28 19:47:51,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741826_1002 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741826
2019-09-28 19:47:51,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2019-09-28 19:47:51,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741828_1004 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741828
2019-09-28 19:48:39,257 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.182.5"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-09-28 19:48:43,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:44,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:45,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:46,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:47,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:48,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:49,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:50,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:51,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:52,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:52,282 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From um2/192.168.182.5 to um1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2019-09-28 19:48:53,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:54,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:55,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:56,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:57,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:58,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:48:59,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:49:00,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 19:49:00,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from um1/192.168.182.3:9000 with active state
2019-09-28 19:49:00,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-28 19:49:00,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-28 19:49:00,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x824a8dcd864,  containing 1 storage report(s), of which we sent 1. The reports had 9 total blocks and used 1 RPC(s). This took 1 msec to generate and 21 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-28 19:49:00,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:05:09,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-29 16:05:09,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-29 16:05:10,506 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:05:11,271 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-29 16:05:11,495 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-29 16:05:11,495 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-29 16:05:11,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-29 16:05:11,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-29 16:05:11,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-29 16:05:11,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-29 16:05:11,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-29 16:05:11,739 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-29 16:05:11,745 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-29 16:05:11,757 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-29 16:05:11,767 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-29 16:05:11,767 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-29 16:05:11,767 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-29 16:05:11,808 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-29 16:05:11,812 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-29 16:05:11,812 INFO org.mortbay.log: jetty-6.1.26
2019-09-29 16:05:12,315 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-29 16:05:13,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-29 16:05:13,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-29 16:05:13,129 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-29 16:05:13,152 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-29 16:05:13,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-29 16:05:13,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-29 16:05:13,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-29 16:05:13,262 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:05:13,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-29 16:05:13,323 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-29 16:05:13,339 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-29 16:05:14,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:05:15,659 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.182.3:9000
2019-09-29 16:05:21,109 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2938@um2
2019-09-29 16:05:21,313 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:05:21,313 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:05:21,315 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-29 16:05:21,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-09-29 16:05:21,348 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:05:21,389 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-09-29 16:05:21,389 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-09-29 16:05:21,474 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-29 16:05:21,474 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:05:21,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 16:05:21,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 15ms
2019-09-29 16:05:21,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 17ms
2019-09-29 16:05:21,497 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 16:05:21,501 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 4ms
2019-09-29 16:05:21,501 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 10ms
2019-09-29 16:05:21,505 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569771994505 with interval 21600000
2019-09-29 16:05:21,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-29 16:05:21,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-29 16:05:21,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-29 16:05:21,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=245
2019-09-29 16:05:21,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-09-29 16:05:21,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2bc695b356,  containing 1 storage report(s), of which we sent 1. The reports had 9 total blocks and used 1 RPC(s). This took 2 msec to generate and 32 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-29 16:05:21,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:05:21,664 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-29 16:05:21,664 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-29 16:05:21,666 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-29 16:05:21,666 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-29 16:05:21,666 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:05:21,672 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-09-29 16:05:51,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741833_1009 src: /192.168.182.3:34646 dest: /192.168.182.5:50010
2019-09-29 16:05:51,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741832_1008 src: /192.168.182.3:34644 dest: /192.168.182.5:50010
2019-09-29 16:05:51,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073741833_1009 src: /192.168.182.3:34646 dest: /192.168.182.5:50010 of size 4043
2019-09-29 16:05:51,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073741832_1008 src: /192.168.182.3:34644 dest: /192.168.182.5:50010 of size 66
2019-09-29 16:05:54,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741838_1014 src: /192.168.182.3:34650 dest: /192.168.182.5:50010
2019-09-29 16:05:54,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741836_1012 src: /192.168.182.3:34652 dest: /192.168.182.5:50010
2019-09-29 16:05:54,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073741838_1014 src: /192.168.182.3:34650 dest: /192.168.182.5:50010 of size 73
2019-09-29 16:05:54,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073741836_1012 src: /192.168.182.3:34652 dest: /192.168.182.5:50010 of size 5812
2019-09-29 16:05:57,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741840_1016 src: /192.168.182.3:34662 dest: /192.168.182.5:50010
2019-09-29 16:05:57,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073741840_1016 src: /192.168.182.3:34662 dest: /192.168.182.5:50010 of size 185
2019-09-29 16:10:03,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2019-09-29 16:10:03,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2019-09-29 16:10:03,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2019-09-29 16:10:03,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2019-09-29 16:10:03,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741829_1005 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741829
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741830_1006 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741830
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741831_1007 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741831
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741832_1008 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741832
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741833_1009 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741833
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2019-09-29 16:10:03,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741834_1010 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741834
2019-09-29 16:10:03,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741835_1011 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741835
2019-09-29 16:10:03,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741836_1012 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741836
2019-09-29 16:10:03,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741837_1013 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741837
2019-09-29 16:10:03,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2019-09-29 16:10:03,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2019-09-29 16:10:03,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741838_1014 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741838
2019-09-29 16:10:03,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2019-09-29 16:10:03,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741839_1015 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741839
2019-09-29 16:10:03,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2019-09-29 16:10:03,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741840_1016 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741840
2019-09-29 16:10:03,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741841_1017 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741841
2019-09-29 16:10:03,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2019-09-29 16:10:03,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741842_1018 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741842
2019-09-29 16:10:12,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741843_1019 src: /192.168.182.3:34680 dest: /192.168.182.5:50010
2019-09-29 16:10:13,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34680, dest: /192.168.182.5:50010, bytes: 3751306, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741843_1019, duration: 379095285
2019-09-29 16:10:13,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741844_1020 src: /192.168.182.3:34684 dest: /192.168.182.5:50010
2019-09-29 16:10:13,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34684, dest: /192.168.182.5:50010, bytes: 3048, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741844_1020, duration: 5652621
2019-09-29 16:10:13,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741845_1021 src: /192.168.182.3:34688 dest: /192.168.182.5:50010
2019-09-29 16:10:13,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34688, dest: /192.168.182.5:50010, bytes: 123637, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741845_1021, duration: 15419097
2019-09-29 16:10:13,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741846_1022 src: /192.168.182.3:34692 dest: /192.168.182.5:50010
2019-09-29 16:10:13,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34692, dest: /192.168.182.5:50010, bytes: 66, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741846_1022, duration: 5401562
2019-09-29 16:10:13,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741847_1023 src: /192.168.182.3:34696 dest: /192.168.182.5:50010
2019-09-29 16:10:13,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34696, dest: /192.168.182.5:50010, bytes: 4043, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741847_1023, duration: 4724568
2019-09-29 16:10:13,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741848_1024 src: /192.168.182.3:34700 dest: /192.168.182.5:50010
2019-09-29 16:10:13,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34700, dest: /192.168.182.5:50010, bytes: 130, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741848_1024, duration: 16192336
2019-09-29 16:10:13,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741849_1025 src: /192.168.182.3:34704 dest: /192.168.182.5:50010
2019-09-29 16:10:13,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34704, dest: /192.168.182.5:50010, bytes: 240, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741849_1025, duration: 3701936
2019-09-29 16:10:13,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741850_1026 src: /192.168.182.3:34708 dest: /192.168.182.5:50010
2019-09-29 16:10:13,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34708, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741850_1026, duration: 3531907
2019-09-29 16:10:13,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741851_1027 src: /192.168.182.3:34712 dest: /192.168.182.5:50010
2019-09-29 16:10:13,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34712, dest: /192.168.182.5:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741851_1027, duration: 2016840
2019-09-29 16:10:13,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741852_1028 src: /192.168.182.3:34716 dest: /192.168.182.5:50010
2019-09-29 16:10:13,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34716, dest: /192.168.182.5:50010, bytes: 73, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741852_1028, duration: 5869191
2019-09-29 16:10:13,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741853_1029 src: /192.168.182.3:34720 dest: /192.168.182.5:50010
2019-09-29 16:10:13,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34720, dest: /192.168.182.5:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741853_1029, duration: 3626571
2019-09-29 16:10:13,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741854_1030 src: /192.168.182.3:34724 dest: /192.168.182.5:50010
2019-09-29 16:10:13,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34724, dest: /192.168.182.5:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741854_1030, duration: 5728692
2019-09-29 16:10:13,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:13,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741855_1031 src: /192.168.182.3:34728 dest: /192.168.182.5:50010
2019-09-29 16:10:13,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34728, dest: /192.168.182.5:50010, bytes: 334, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741855_1031, duration: 5231592
2019-09-29 16:10:13,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:14,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741856_1032 src: /192.168.182.3:34732 dest: /192.168.182.5:50010
2019-09-29 16:10:14,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:34732, dest: /192.168.182.5:50010, bytes: 615, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610501529_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741856_1032, duration: 4791430
2019-09-29 16:10:14,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-09-29 16:10:21,697 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741850_1026
2019-09-29 16:18:43,222 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.182.5"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-09-29 16:18:47,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:18:48,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:18:49,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:18:50,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:18:51,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:18:52,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:18:53,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:18:53,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from um1/192.168.182.3:9000 with active state
2019-09-29 16:18:53,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-29 16:18:53,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-29 16:18:53,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe8c839e966,  containing 1 storage report(s), of which we sent 1. The reports had 14 total blocks and used 1 RPC(s). This took 0 msec to generate and 21 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-29 16:18:53,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:40:18,932 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.182.5"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-09-29 16:40:22,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.182.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-29 16:40:22,993 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-29 16:40:22,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.182.5
************************************************************/
2019-09-29 16:55:18,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-29 16:55:18,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-29 16:55:18,499 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:55:18,875 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-29 16:55:19,007 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-29 16:55:19,007 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-29 16:55:19,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-29 16:55:19,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-29 16:55:19,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-29 16:55:19,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-29 16:55:19,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-29 16:55:19,119 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-29 16:55:19,122 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-29 16:55:19,130 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-29 16:55:19,132 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-29 16:55:19,132 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-29 16:55:19,132 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-29 16:55:19,148 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-29 16:55:19,151 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-29 16:55:19,151 INFO org.mortbay.log: jetty-6.1.26
2019-09-29 16:55:19,433 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-29 16:55:19,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-29 16:55:19,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-29 16:55:19,821 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-29 16:55:19,836 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-29 16:55:19,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-29 16:55:19,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-29 16:55:19,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-29 16:55:19,888 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:55:19,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-29 16:55:19,908 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-29 16:55:19,908 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-29 16:55:20,505 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 5252@um2
2019-09-29 16:55:20,611 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:55:20,611 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:55:20,612 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-29 16:55:20,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-09-29 16:55:20,647 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:55:20,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-09-29 16:55:20,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-09-29 16:55:20,731 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-29 16:55:20,731 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:55:20,732 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 16:55:20,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 18ms
2019-09-29 16:55:20,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 20ms
2019-09-29 16:55:20,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 16:55:20,757 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 5ms
2019-09-29 16:55:20,758 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2019-09-29 16:55:20,761 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569778588761 with interval 21600000
2019-09-29 16:55:20,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-29 16:55:20,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-29 16:55:20,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-29 16:55:21,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=349
2019-09-29 16:55:21,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-09-29 16:55:21,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2e61fbcbb72,  containing 1 storage report(s), of which we sent 1. The reports had 14 total blocks and used 1 RPC(s). This took 2 msec to generate and 111 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-29 16:55:21,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:55:21,126 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-29 16:55:21,126 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-29 16:55:21,127 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-29 16:55:21,127 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-29 16:55:21,127 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:55:21,133 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-09-29 16:55:25,795 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741844_1020
2019-09-29 16:57:39,017 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.182.5"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2019-09-29 16:57:41,516 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-29 16:57:41,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.182.5
************************************************************/
2019-09-29 16:59:00,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-29 16:59:00,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-29 16:59:00,685 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:59:01,186 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-29 16:59:01,295 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-29 16:59:01,295 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-29 16:59:01,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-29 16:59:01,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-29 16:59:01,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-29 16:59:01,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-29 16:59:01,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-29 16:59:01,449 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-29 16:59:01,455 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-29 16:59:01,467 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-29 16:59:01,473 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-29 16:59:01,474 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-29 16:59:01,474 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-29 16:59:01,499 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-29 16:59:01,503 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-29 16:59:01,503 INFO org.mortbay.log: jetty-6.1.26
2019-09-29 16:59:01,883 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-29 16:59:02,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-29 16:59:02,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-29 16:59:02,471 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-29 16:59:02,494 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-29 16:59:02,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-29 16:59:02,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-29 16:59:02,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-29 16:59:02,581 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:59:02,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-29 16:59:02,593 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-29 16:59:02,595 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-29 16:59:03,226 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 6812@um2
2019-09-29 16:59:03,407 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:59:03,407 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:59:03,408 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-29 16:59:03,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-09-29 16:59:03,449 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 16:59:03,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-09-29 16:59:03,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-09-29 16:59:03,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-29 16:59:03,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:59:03,544 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 16:59:03,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current: 4059136
2019-09-29 16:59:03,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 18ms
2019-09-29 16:59:03,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 19ms
2019-09-29 16:59:03,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 16:59:03,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 11ms
2019-09-29 16:59:03,576 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 14ms
2019-09-29 16:59:03,581 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569778937581 with interval 21600000
2019-09-29 16:59:03,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-29 16:59:03,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-29 16:59:03,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-29 16:59:03,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=352
2019-09-29 16:59:03,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-09-29 16:59:03,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x319fffc5a73,  containing 1 storage report(s), of which we sent 1. The reports had 14 total blocks and used 1 RPC(s). This took 2 msec to generate and 91 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-29 16:59:03,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:59:03,914 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-29 16:59:03,914 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-29 16:59:03,916 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-29 16:59:03,916 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-29 16:59:03,916 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 16:59:03,922 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-09-29 16:59:08,626 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741854_1030
2019-09-29 17:07:49,107 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 is shutting down
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException): Datanode denied communication with namenode because the host is not in the include-list: 192.168.182.5:50010
	at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.handleHeartbeat(DatanodeManager.java:1378)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.handleHeartbeat(FSNamesystem.java:5087)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendHeartbeat(NameNodeRpcServer.java:1153)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.sendHeartbeat(DatanodeProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:27331)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.call(Client.java:1470)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
2019-09-29 17:07:49,108 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-09-29 17:07:49,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5)
2019-09-29 17:07:49,209 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Removed bpid=BP-278062902-192.168.182.3-1569688905540 from blockPoolScannerMap
2019-09-29 17:07:49,209 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removing block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:07:51,210 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-09-29 17:07:51,212 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-09-29 17:07:51,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.182.5
************************************************************/
2019-09-29 17:21:22,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-29 17:21:22,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-29 17:21:22,870 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 17:21:23,218 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-29 17:21:23,291 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-29 17:21:23,291 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-29 17:21:23,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-29 17:21:23,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-29 17:21:23,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-29 17:21:23,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-29 17:21:23,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-29 17:21:23,400 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-29 17:21:23,404 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-29 17:21:23,413 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-29 17:21:23,415 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-29 17:21:23,415 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-29 17:21:23,415 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-29 17:21:23,427 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-29 17:21:23,430 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-29 17:21:23,431 INFO org.mortbay.log: jetty-6.1.26
2019-09-29 17:21:23,669 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-29 17:21:23,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-29 17:21:23,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-29 17:21:24,016 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-29 17:21:24,031 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-29 17:21:24,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-29 17:21:24,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-29 17:21:24,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-29 17:21:24,097 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 17:21:24,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-29 17:21:24,104 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-29 17:21:24,104 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-29 17:21:24,479 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 8493@um2
2019-09-29 17:21:24,555 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:21:24,555 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:21:24,555 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-29 17:21:24,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-09-29 17:21:24,565 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 17:21:24,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-09-29 17:21:24,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-09-29 17:21:24,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-29 17:21:24,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:21:24,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 17:21:24,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 14ms
2019-09-29 17:21:24,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 14ms
2019-09-29 17:21:24,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 17:21:24,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 4ms
2019-09-29 17:21:24,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2019-09-29 17:21:24,644 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569776977644 with interval 21600000
2019-09-29 17:21:24,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-29 17:21:24,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-29 17:21:24,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-29 17:21:24,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=357
2019-09-29 17:21:24,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-09-29 17:21:24,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x45239a9479c,  containing 1 storage report(s), of which we sent 1. The reports had 14 total blocks and used 1 RPC(s). This took 1 msec to generate and 78 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-29 17:21:24,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:21:24,895 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-29 17:21:24,895 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-29 17:21:24,897 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-29 17:21:24,897 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-29 17:21:24,897 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:21:24,902 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-09-29 17:28:19,133 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 is shutting down
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException): Datanode denied communication with namenode because the host is not in the include-list: 192.168.182.5:50010
	at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.handleHeartbeat(DatanodeManager.java:1378)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.handleHeartbeat(FSNamesystem.java:5087)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendHeartbeat(NameNodeRpcServer.java:1153)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.sendHeartbeat(DatanodeProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:27331)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.call(Client.java:1470)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
2019-09-29 17:28:19,135 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-09-29 17:28:19,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5)
2019-09-29 17:28:19,236 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Removed bpid=BP-278062902-192.168.182.3-1569688905540 from blockPoolScannerMap
2019-09-29 17:28:19,237 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removing block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:28:21,238 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-09-29 17:28:21,240 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-09-29 17:28:21,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.182.5
************************************************************/
2019-09-29 17:45:24,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-09-29 17:45:24,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-29 17:45:25,313 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 17:45:25,906 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-29 17:45:26,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-29 17:45:26,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-29 17:45:26,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-09-29 17:45:26,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-29 17:45:26,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-29 17:45:26,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-09-29 17:45:26,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-09-29 17:45:26,202 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-29 17:45:26,207 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-29 17:45:26,221 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-29 17:45:26,224 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-29 17:45:26,224 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-29 17:45:26,225 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-29 17:45:26,250 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-09-29 17:45:26,253 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-09-29 17:45:26,253 INFO org.mortbay.log: jetty-6.1.26
2019-09-29 17:45:26,627 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-09-29 17:45:27,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-09-29 17:45:27,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-29 17:45:27,258 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-09-29 17:45:27,281 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-29 17:45:27,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-29 17:45:27,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-29 17:45:27,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-29 17:45:27,394 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 17:45:27,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-09-29 17:45:27,424 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-29 17:45:27,432 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-29 17:45:27,852 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 10767@um2
2019-09-29 17:45:28,008 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:45:28,009 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:45:28,009 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-09-29 17:45:28,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-09-29 17:45:28,027 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-09-29 17:45:28,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-09-29 17:45:28,058 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-09-29 17:45:28,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-29 17:45:28,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:45:28,115 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 17:45:28,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 21ms
2019-09-29 17:45:28,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 22ms
2019-09-29 17:45:28,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-09-29 17:45:28,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 10ms
2019-09-29 17:45:28,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2019-09-29 17:45:28,152 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1569787099152 with interval 21600000
2019-09-29 17:45:28,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-09-29 17:45:28,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-09-29 17:45:28,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-29 17:45:28,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=372
2019-09-29 17:45:28,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-09-29 17:45:28,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5a2571c11ff,  containing 1 storage report(s), of which we sent 1. The reports had 14 total blocks and used 1 RPC(s). This took 2 msec to generate and 139 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-29 17:45:28,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:45:28,560 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-09-29 17:45:28,560 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-09-29 17:45:28,561 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-09-29 17:45:28,561 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-09-29 17:45:28,562 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-09-29 17:45:28,568 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-09-29 17:45:33,192 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741847_1023
2019-10-09 21:36:29,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-10-09 21:36:29,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-09 21:36:29,787 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-09 21:36:30,278 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-09 21:36:30,372 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-10-09 21:36:30,372 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-09 21:36:30,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-10-09 21:36:30,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-09 21:36:30,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-09 21:36:30,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-10-09 21:36:30,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-10-09 21:36:30,568 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-09 21:36:30,571 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-09 21:36:30,580 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-09 21:36:30,583 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-09 21:36:30,583 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-09 21:36:30,583 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-09 21:36:30,603 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-10-09 21:36:30,607 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-10-09 21:36:30,607 INFO org.mortbay.log: jetty-6.1.26
2019-10-09 21:36:30,886 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-10-09 21:36:31,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-10-09 21:36:31,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-09 21:36:31,329 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-10-09 21:36:31,344 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-09 21:36:31,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-09 21:36:31,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-09 21:36:31,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-09 21:36:31,414 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-09 21:36:31,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-10-09 21:36:31,432 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-09 21:36:31,433 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-09 21:36:31,722 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 19908@um2
2019-10-09 21:36:31,780 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-10-09 21:36:31,781 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-10-09 21:36:31,781 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-10-09 21:36:31,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-10-09 21:36:31,798 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-09 21:36:31,827 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-10-09 21:36:31,827 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-10-09 21:36:31,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-09 21:36:31,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-10-09 21:36:31,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-10-09 21:36:31,893 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 25ms
2019-10-09 21:36:31,893 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 25ms
2019-10-09 21:36:31,894 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-10-09 21:36:31,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 5ms
2019-10-09 21:36:31,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2019-10-09 21:36:31,902 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1570655447902 with interval 21600000
2019-10-09 21:36:31,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-10-09 21:36:31,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-10-09 21:36:31,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-09 21:36:31,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=377
2019-10-09 21:36:31,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-10-09 21:36:32,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x24b9c1051ce,  containing 1 storage report(s), of which we sent 1. The reports had 14 total blocks and used 1 RPC(s). This took 0 msec to generate and 72 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-09 21:36:32,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-10-09 21:36:32,058 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-10-09 21:36:32,058 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-09 21:36:32,063 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-10-09 21:36:32,063 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-10-09 21:36:32,064 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-10-09 21:36:32,077 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-10-09 21:36:36,936 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741854_1030
2019-10-09 21:36:36,940 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741844_1020
2019-10-09 21:36:36,941 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741849_1025
2019-10-09 21:36:36,943 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741851_1027
2019-10-09 21:36:36,955 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741845_1021
2019-10-09 21:37:05,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741855_1031 to 192.168.182.3:50010 
2019-10-09 21:37:05,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741843_1019 to 192.168.182.3:50010 
2019-10-09 21:37:05,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741855_1031 (numBytes=334) to /192.168.182.3:50010
2019-10-09 21:37:05,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741843_1019 (numBytes=3751306) to /192.168.182.3:50010
2019-10-09 21:37:07,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741844_1020 to 192.168.182.3:50010 
2019-10-09 21:37:07,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741845_1021 to 192.168.182.3:50010 
2019-10-09 21:37:08,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741845_1021 (numBytes=123637) to /192.168.182.3:50010
2019-10-09 21:37:08,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741844_1020 (numBytes=3048) to /192.168.182.3:50010
2019-10-09 21:37:10,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741846_1022 to 192.168.182.3:50010 
2019-10-09 21:37:10,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741847_1023 to 192.168.182.3:50010 
2019-10-09 21:37:11,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741846_1022 (numBytes=66) to /192.168.182.3:50010
2019-10-09 21:37:11,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741847_1023 (numBytes=4043) to /192.168.182.3:50010
2019-10-09 21:37:13,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741850_1026 to 192.168.182.3:50010 
2019-10-09 21:37:13,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741851_1027 to 192.168.182.3:50010 
2019-10-09 21:37:14,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741851_1027 (numBytes=28) to /192.168.182.3:50010
2019-10-09 21:37:14,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741850_1026 (numBytes=5812) to /192.168.182.3:50010
2019-10-09 21:37:17,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741854_1030 to 192.168.182.3:50010 
2019-10-09 21:37:17,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741856_1032 to 192.168.182.3:50010 
2019-10-09 21:37:17,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741856_1032 (numBytes=615) to /192.168.182.3:50010
2019-10-09 21:37:17,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741854_1030 (numBytes=185) to /192.168.182.3:50010
2019-10-09 21:37:20,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741848_1024 to 192.168.182.3:50010 
2019-10-09 21:37:20,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741849_1025 to 192.168.182.3:50010 
2019-10-09 21:37:20,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741849_1025 (numBytes=240) to /192.168.182.3:50010
2019-10-09 21:37:20,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741848_1024 (numBytes=130) to /192.168.182.3:50010
2019-10-09 21:37:23,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741852_1028 to 192.168.182.3:50010 
2019-10-09 21:37:23,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.182.5, datanodeUuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0) Starting thread to transfer BP-278062902-192.168.182.3-1569688905540:blk_1073741853_1029 to 192.168.182.3:50010 
2019-10-09 21:37:23,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741852_1028 (numBytes=73) to /192.168.182.3:50010
2019-10-09 21:37:23,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-278062902-192.168.182.3-1569688905540:blk_1073741853_1029 (numBytes=32) to /192.168.182.3:50010
2019-10-09 21:40:00,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741858_1034 src: /192.168.182.3:43108 dest: /192.168.182.5:50010
2019-10-09 21:40:00,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43108, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1059159123_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741858_1034, duration: 205301963
2019-10-09 21:40:00,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:40:07,047 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741858_1034
2019-10-09 21:43:27,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741859_1035 src: /192.168.182.3:43158 dest: /192.168.182.5:50010
2019-10-09 21:43:27,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43158, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_321436657_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741859_1035, duration: 36410345
2019-10-09 21:43:27,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:43:37,126 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741859_1035
2019-10-09 21:44:54,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741860_1036 src: /192.168.182.3:43188 dest: /192.168.182.5:50010
2019-10-09 21:44:54,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43188, dest: /192.168.182.5:50010, bytes: 3415, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237256286_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741860_1036, duration: 58977912
2019-10-09 21:44:54,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:44:54,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741861_1037 src: /192.168.182.3:43192 dest: /192.168.182.5:50010
2019-10-09 21:44:54,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43192, dest: /192.168.182.5:50010, bytes: 3614, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237256286_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741861_1037, duration: 6456931
2019-10-09 21:44:54,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:44:55,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741862_1038 src: /192.168.182.3:43198 dest: /192.168.182.5:50010
2019-10-09 21:44:55,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43198, dest: /192.168.182.5:50010, bytes: 32441258, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237256286_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741862_1038, duration: 503612085
2019-10-09 21:44:55,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:44:55,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741863_1039 src: /192.168.182.3:43202 dest: /192.168.182.5:50010
2019-10-09 21:44:55,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43202, dest: /192.168.182.5:50010, bytes: 222, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237256286_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741863_1039, duration: 6241762
2019-10-09 21:44:55,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:44:55,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741864_1040 src: /192.168.182.3:43206 dest: /192.168.182.5:50010
2019-10-09 21:44:55,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43206, dest: /192.168.182.5:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237256286_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741864_1040, duration: 5883204
2019-10-09 21:44:55,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:44:56,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741865_1041 src: /192.168.182.3:43210 dest: /192.168.182.5:50010
2019-10-09 21:44:56,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43210, dest: /192.168.182.5:50010, bytes: 233928, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237256286_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741865_1041, duration: 105066178
2019-10-09 21:44:56,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:45:02,181 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741860_1036
2019-10-09 21:45:02,182 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741861_1037
2019-10-09 21:45:02,182 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741864_1040
2019-10-09 21:45:02,182 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741863_1039
2019-10-09 21:45:02,184 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741865_1041
2019-10-09 21:45:03,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741866_1042 src: /192.168.182.5:45578 dest: /192.168.182.5:50010
2019-10-09 21:45:03,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.5:45578, dest: /192.168.182.5:50010, bytes: 267620, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1018881432_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741866_1042, duration: 112924057
2019-10-09 21:45:03,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741866_1042, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-10-09 21:45:10,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741867_1043 src: /192.168.182.5:45586 dest: /192.168.182.5:50010
2019-10-09 21:45:12,209 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741866_1042
2019-10-09 21:45:17,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741868_1044 src: /192.168.182.3:43236 dest: /192.168.182.5:50010
2019-10-09 21:45:17,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:43236, dest: /192.168.182.5:50010, bytes: 327, op: HDFS_WRITE, cliID: DFSClient_attempt_1570649817867_0001_r_000000_0_23365144_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741868_1044, duration: 32072422
2019-10-09 21:45:17,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741868_1044, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-09 21:45:17,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.5:45586, dest: /192.168.182.5:50010, bytes: 35306, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1018881432_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741867_1043, duration: 6668123271
2019-10-09 21:45:17,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741867_1043, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-10-09 21:45:17,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741869_1045 src: /192.168.182.5:45594 dest: /192.168.182.5:50010
2019-10-09 21:45:17,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.5:45594, dest: /192.168.182.5:50010, bytes: 389, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1018881432_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741869_1045, duration: 5647418
2019-10-09 21:45:17,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741869_1045, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-10-09 21:45:17,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741870_1046 src: /192.168.182.5:45600 dest: /192.168.182.5:50010
2019-10-09 21:45:17,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.5:45600, dest: /192.168.182.5:50010, bytes: 35306, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1018881432_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741870_1046, duration: 15220521
2019-10-09 21:45:17,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741870_1046, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-10-09 21:45:17,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741871_1047 src: /192.168.182.5:45604 dest: /192.168.182.5:50010
2019-10-09 21:45:17,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.5:45604, dest: /192.168.182.5:50010, bytes: 267620, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1018881432_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741871_1047, duration: 10352376
2019-10-09 21:45:17,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741871_1047, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2019-10-09 21:45:22,218 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741868_1044
2019-10-09 21:45:23,447 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2019-10-09 21:45:23,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741860_1036 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741860
2019-10-09 21:45:23,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741861_1037 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741861
2019-10-09 21:45:23,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741862_1038 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741862
2019-10-09 21:45:23,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741863_1039 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741863
2019-10-09 21:45:23,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741864_1040 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741864
2019-10-09 21:45:23,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741865_1041 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741865
2019-10-09 21:45:23,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741866_1042 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741866
2019-10-09 21:45:23,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741867_1043 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741867
2019-10-09 21:45:23,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741868_1044 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741868
2019-10-09 21:45:27,225 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741869_1045
2019-10-09 21:45:27,226 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741870_1046
2019-10-09 21:45:27,227 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741871_1047
2019-10-13 17:28:56,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-10-13 17:28:56,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-13 17:28:56,704 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-13 17:28:57,349 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-13 17:28:57,539 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-10-13 17:28:57,539 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-13 17:28:57,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-10-13 17:28:57,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-13 17:28:57,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-13 17:28:57,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-10-13 17:28:57,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-10-13 17:28:57,883 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-13 17:28:57,888 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-13 17:28:57,900 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-13 17:28:57,903 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-13 17:28:57,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-13 17:28:57,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-13 17:28:57,922 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-10-13 17:28:57,925 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-10-13 17:28:57,925 INFO org.mortbay.log: jetty-6.1.26
2019-10-13 17:28:58,287 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-10-13 17:28:58,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-10-13 17:28:58,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-13 17:28:58,956 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-10-13 17:28:58,975 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-13 17:28:59,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-13 17:28:59,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-13 17:28:59,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-13 17:28:59,091 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-13 17:28:59,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-10-13 17:28:59,154 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-13 17:28:59,165 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-13 17:28:59,569 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2761@um2
2019-10-13 17:28:59,647 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-10-13 17:28:59,647 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-10-13 17:28:59,649 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-10-13 17:28:59,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-10-13 17:28:59,669 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-13 17:28:59,709 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-10-13 17:28:59,709 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-10-13 17:28:59,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-13 17:28:59,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-10-13 17:28:59,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-10-13 17:28:59,816 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 45ms
2019-10-13 17:28:59,816 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 52ms
2019-10-13 17:28:59,817 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-10-13 17:28:59,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 7ms
2019-10-13 17:28:59,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2019-10-13 17:28:59,829 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1570985768829 with interval 21600000
2019-10-13 17:28:59,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-10-13 17:28:59,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-10-13 17:28:59,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-13 17:28:59,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=555
2019-10-13 17:28:59,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-10-13 17:28:59,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x15d7443c95,  containing 1 storage report(s), of which we sent 1. The reports had 19 total blocks and used 1 RPC(s). This took 1 msec to generate and 42 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-13 17:28:59,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-10-13 17:29:00,005 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-10-13 17:29:00,006 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-13 17:29:00,008 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-10-13 17:29:00,008 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-10-13 17:29:00,008 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-10-13 17:29:00,017 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-10-13 17:29:04,883 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741850_1026
2019-10-19 06:53:55,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.182.5
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-10-19 06:53:55,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-19 06:53:55,487 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-19 06:53:55,994 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-19 06:53:56,071 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-10-19 06:53:56,071 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-19 06:53:56,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-10-19 06:53:56,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-19 06:53:56,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-19 06:53:56,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-10-19 06:53:56,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-10-19 06:53:56,262 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-19 06:53:56,267 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-19 06:53:56,277 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-19 06:53:56,280 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-19 06:53:56,281 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-19 06:53:56,281 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-19 06:53:56,299 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-10-19 06:53:56,301 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-10-19 06:53:56,301 INFO org.mortbay.log: jetty-6.1.26
2019-10-19 06:53:56,625 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-10-19 06:53:57,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-10-19 06:53:57,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-19 06:53:57,316 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-10-19 06:53:57,333 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-19 06:53:57,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-19 06:53:57,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-19 06:53:57,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-19 06:53:57,449 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-19 06:53:57,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.182.3:9000 starting to offer service
2019-10-19 06:53:57,470 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-19 06:53:57,470 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-19 06:53:57,932 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 4888@um2
2019-10-19 06:53:58,092 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-10-19 06:53:58,092 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-10-19 06:53:58,093 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-10-19 06:53:58,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-10-19 06:53:58,125 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-10-19 06:53:58,161 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-10-19 06:53:58,161 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-10-19 06:53:58,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-19 06:53:58,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-10-19 06:53:58,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-10-19 06:53:58,297 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 45ms
2019-10-19 06:53:58,298 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 53ms
2019-10-19 06:53:58,316 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-10-19 06:53:58,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 11ms
2019-10-19 06:53:58,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 29ms
2019-10-19 06:53:58,332 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1571481265332 with interval 21600000
2019-10-19 06:53:58,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 beginning handshake with NN
2019-10-19 06:53:58,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.182.3:9000 successfully registered with NN
2019-10-19 06:53:58,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.182.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-19 06:53:58,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000 trying to claim ACTIVE state with txid=573
2019-10-19 06:53:58,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.182.3:9000
2019-10-19 06:53:58,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4585716a963,  containing 1 storage report(s), of which we sent 1. The reports had 19 total blocks and used 1 RPC(s). This took 1 msec to generate and 42 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-19 06:53:58,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-10-19 06:53:58,511 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-10-19 06:53:58,511 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-19 06:53:58,513 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-10-19 06:53:58,513 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-10-19 06:53:58,514 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-10-19 06:53:58,520 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-10-19 06:57:18,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741872_1048 src: /192.168.182.3:58244 dest: /192.168.182.5:50010
2019-10-19 06:57:18,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58244, dest: /192.168.182.5:50010, bytes: 3511, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-773874989_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741872_1048, duration: 132024964
2019-10-19 06:57:18,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 06:57:18,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741873_1049 src: /192.168.182.3:58248 dest: /192.168.182.5:50010
2019-10-19 06:57:18,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58248, dest: /192.168.182.5:50010, bytes: 3884, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-773874989_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741873_1049, duration: 7055785
2019-10-19 06:57:18,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 06:57:19,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741874_1050 src: /192.168.182.3:58254 dest: /192.168.182.5:50010
2019-10-19 06:57:19,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58254, dest: /192.168.182.5:50010, bytes: 32441258, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-773874989_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741874_1050, duration: 483675547
2019-10-19 06:57:19,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 06:57:19,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741875_1051 src: /192.168.182.3:58258 dest: /192.168.182.5:50010
2019-10-19 06:57:19,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58258, dest: /192.168.182.5:50010, bytes: 221, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-773874989_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741875_1051, duration: 4159574
2019-10-19 06:57:19,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 06:57:20,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741876_1052 src: /192.168.182.3:58262 dest: /192.168.182.5:50010
2019-10-19 06:57:20,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58262, dest: /192.168.182.5:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-773874989_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741876_1052, duration: 4573050
2019-10-19 06:57:20,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 06:57:20,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741877_1053 src: /192.168.182.3:58266 dest: /192.168.182.5:50010
2019-10-19 06:57:20,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58266, dest: /192.168.182.5:50010, bytes: 233945, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-773874989_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741877_1053, duration: 32627901
2019-10-19 06:57:20,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 06:58:34,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2019-10-19 06:58:34,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2019-10-19 06:58:34,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741872_1048 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741872
2019-10-19 06:58:34,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741873_1049 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741873
2019-10-19 07:38:15,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741878_1054 src: /192.168.182.3:58446 dest: /192.168.182.5:50010
2019-10-19 07:38:15,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58446, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2135127316_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741878_1054, duration: 54361410
2019-10-19 07:38:15,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741878_1054, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 07:40:24,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741879_1055 src: /192.168.182.3:58470 dest: /192.168.182.5:50010
2019-10-19 07:40:24,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58470, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2135127316_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741879_1055, duration: 3257038
2019-10-19 07:40:24,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 07:40:30,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741880_1056 src: /192.168.182.3:58474 dest: /192.168.182.5:50010
2019-10-19 07:40:30,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58474, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2135127316_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741880_1056, duration: 3320776
2019-10-19 07:40:30,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 07:44:41,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741881_1057 src: /192.168.182.3:58488 dest: /192.168.182.5:50010
2019-10-19 07:44:41,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58488, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2135127316_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741881_1057, duration: 4419452
2019-10-19 07:44:41,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 07:44:48,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741882_1058 src: /192.168.182.3:58492 dest: /192.168.182.5:50010
2019-10-19 07:44:48,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58492, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2135127316_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741882_1058, duration: 4372905
2019-10-19 07:44:48,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 07:44:49,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741883_1059 src: /192.168.182.3:58496 dest: /192.168.182.5:50010
2019-10-19 07:44:49,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58496, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2135127316_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741883_1059, duration: 3335589
2019-10-19 07:44:49,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741883_1059, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 07:44:50,807 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741881_1057
2019-10-19 07:44:50,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741884_1060 src: /192.168.182.3:58500 dest: /192.168.182.5:50010
2019-10-19 07:44:51,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58500, dest: /192.168.182.5:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2135127316_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741884_1060, duration: 12181164
2019-10-19 07:44:51,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 07:44:55,810 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741882_1058
2019-10-19 07:45:00,814 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741884_1060
2019-10-19 08:01:12,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741885_1061 src: /192.168.182.3:58544 dest: /192.168.182.5:50010
2019-10-19 08:01:13,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58544, dest: /192.168.182.5:50010, bytes: 57263, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741885_1061, duration: 79269077
2019-10-19 08:01:13,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741885_1061, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741886_1062 src: /192.168.182.3:58638 dest: /192.168.182.5:50010
2019-10-19 08:17:18,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58638, dest: /192.168.182.5:50010, bytes: 2559, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741886_1062, duration: 6915809
2019-10-19 08:17:18,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741886_1062, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741887_1063 src: /192.168.182.3:58642 dest: /192.168.182.5:50010
2019-10-19 08:17:18,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58642, dest: /192.168.182.5:50010, bytes: 3396, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741887_1063, duration: 5060054
2019-10-19 08:17:18,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741888_1064 src: /192.168.182.3:58646 dest: /192.168.182.5:50010
2019-10-19 08:17:18,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58646, dest: /192.168.182.5:50010, bytes: 3214, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741888_1064, duration: 7527986
2019-10-19 08:17:18,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741888_1064, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741889_1065 src: /192.168.182.3:58650 dest: /192.168.182.5:50010
2019-10-19 08:17:18,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58650, dest: /192.168.182.5:50010, bytes: 2703, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741889_1065, duration: 2485699
2019-10-19 08:17:18,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741889_1065, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741890_1066 src: /192.168.182.3:58654 dest: /192.168.182.5:50010
2019-10-19 08:17:18,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58654, dest: /192.168.182.5:50010, bytes: 3033, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741890_1066, duration: 9077656
2019-10-19 08:17:18,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741890_1066, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741891_1067 src: /192.168.182.3:58658 dest: /192.168.182.5:50010
2019-10-19 08:17:18,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58658, dest: /192.168.182.5:50010, bytes: 3590, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741891_1067, duration: 7093759
2019-10-19 08:17:18,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741891_1067, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741892_1068 src: /192.168.182.3:58662 dest: /192.168.182.5:50010
2019-10-19 08:17:18,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58662, dest: /192.168.182.5:50010, bytes: 2736, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741892_1068, duration: 3918276
2019-10-19 08:17:18,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741892_1068, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741893_1069 src: /192.168.182.3:58666 dest: /192.168.182.5:50010
2019-10-19 08:17:18,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58666, dest: /192.168.182.5:50010, bytes: 2647, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741893_1069, duration: 23072311
2019-10-19 08:17:18,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741893_1069, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741894_1070 src: /192.168.182.3:58670 dest: /192.168.182.5:50010
2019-10-19 08:17:18,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58670, dest: /192.168.182.5:50010, bytes: 2561, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741894_1070, duration: 5025313
2019-10-19 08:17:18,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741894_1070, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741895_1071 src: /192.168.182.3:58674 dest: /192.168.182.5:50010
2019-10-19 08:17:18,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58674, dest: /192.168.182.5:50010, bytes: 3676, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741895_1071, duration: 2361901
2019-10-19 08:17:18,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741895_1071, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741896_1072 src: /192.168.182.3:58678 dest: /192.168.182.5:50010
2019-10-19 08:17:18,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58678, dest: /192.168.182.5:50010, bytes: 2551, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741896_1072, duration: 3347113
2019-10-19 08:17:18,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741896_1072, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741897_1073 src: /192.168.182.3:58682 dest: /192.168.182.5:50010
2019-10-19 08:17:18,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58682, dest: /192.168.182.5:50010, bytes: 2581, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741897_1073, duration: 2750586
2019-10-19 08:17:18,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741897_1073, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741898_1074 src: /192.168.182.3:58686 dest: /192.168.182.5:50010
2019-10-19 08:17:18,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58686, dest: /192.168.182.5:50010, bytes: 2550, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741898_1074, duration: 2134628
2019-10-19 08:17:18,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741898_1074, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741899_1075 src: /192.168.182.3:58690 dest: /192.168.182.5:50010
2019-10-19 08:17:18,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58690, dest: /192.168.182.5:50010, bytes: 2571, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741899_1075, duration: 4165651
2019-10-19 08:17:18,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741899_1075, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:18,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741900_1076 src: /192.168.182.3:58694 dest: /192.168.182.5:50010
2019-10-19 08:17:18,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58694, dest: /192.168.182.5:50010, bytes: 2549, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741900_1076, duration: 2483275
2019-10-19 08:17:18,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741900_1076, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741901_1077 src: /192.168.182.3:58698 dest: /192.168.182.5:50010
2019-10-19 08:17:19,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58698, dest: /192.168.182.5:50010, bytes: 2549, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741901_1077, duration: 2677576
2019-10-19 08:17:19,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741901_1077, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741902_1078 src: /192.168.182.3:58702 dest: /192.168.182.5:50010
2019-10-19 08:17:19,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58702, dest: /192.168.182.5:50010, bytes: 2929, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741902_1078, duration: 3966290
2019-10-19 08:17:19,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741902_1078, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741903_1079 src: /192.168.182.3:58706 dest: /192.168.182.5:50010
2019-10-19 08:17:19,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58706, dest: /192.168.182.5:50010, bytes: 2654, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741903_1079, duration: 5215163
2019-10-19 08:17:19,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741903_1079, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741904_1080 src: /192.168.182.3:58710 dest: /192.168.182.5:50010
2019-10-19 08:17:19,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58710, dest: /192.168.182.5:50010, bytes: 3050, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741904_1080, duration: 2750504
2019-10-19 08:17:19,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741904_1080, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741905_1081 src: /192.168.182.3:58714 dest: /192.168.182.5:50010
2019-10-19 08:17:19,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58714, dest: /192.168.182.5:50010, bytes: 2750, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741905_1081, duration: 6729990
2019-10-19 08:17:19,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741905_1081, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741906_1082 src: /192.168.182.3:58718 dest: /192.168.182.5:50010
2019-10-19 08:17:19,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58718, dest: /192.168.182.5:50010, bytes: 2566, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741906_1082, duration: 2907505
2019-10-19 08:17:19,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741906_1082, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741907_1083 src: /192.168.182.3:58722 dest: /192.168.182.5:50010
2019-10-19 08:17:19,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58722, dest: /192.168.182.5:50010, bytes: 3028, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741907_1083, duration: 5887591
2019-10-19 08:17:19,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741907_1083, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741908_1084 src: /192.168.182.3:58726 dest: /192.168.182.5:50010
2019-10-19 08:17:19,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58726, dest: /192.168.182.5:50010, bytes: 2596, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741908_1084, duration: 2855915
2019-10-19 08:17:19,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741908_1084, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741909_1085 src: /192.168.182.3:58730 dest: /192.168.182.5:50010
2019-10-19 08:17:19,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58730, dest: /192.168.182.5:50010, bytes: 2566, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741909_1085, duration: 3924346
2019-10-19 08:17:19,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741909_1085, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741910_1086 src: /192.168.182.3:58734 dest: /192.168.182.5:50010
2019-10-19 08:17:19,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58734, dest: /192.168.182.5:50010, bytes: 2594, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741910_1086, duration: 2080413
2019-10-19 08:17:19,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741910_1086, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741911_1087 src: /192.168.182.3:58738 dest: /192.168.182.5:50010
2019-10-19 08:17:19,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58738, dest: /192.168.182.5:50010, bytes: 2551, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741911_1087, duration: 2802183
2019-10-19 08:17:19,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741911_1087, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741912_1088 src: /192.168.182.3:58742 dest: /192.168.182.5:50010
2019-10-19 08:17:19,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58742, dest: /192.168.182.5:50010, bytes: 3898, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741912_1088, duration: 3551406
2019-10-19 08:17:19,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741912_1088, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741913_1089 src: /192.168.182.3:58746 dest: /192.168.182.5:50010
2019-10-19 08:17:19,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58746, dest: /192.168.182.5:50010, bytes: 2665, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741913_1089, duration: 2154549
2019-10-19 08:17:19,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741913_1089, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741914_1090 src: /192.168.182.3:58750 dest: /192.168.182.5:50010
2019-10-19 08:17:19,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58750, dest: /192.168.182.5:50010, bytes: 3066, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741914_1090, duration: 10652278
2019-10-19 08:17:19,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741914_1090, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741915_1091 src: /192.168.182.3:58754 dest: /192.168.182.5:50010
2019-10-19 08:17:19,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58754, dest: /192.168.182.5:50010, bytes: 2607, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741915_1091, duration: 8007766
2019-10-19 08:17:19,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741915_1091, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741916_1092 src: /192.168.182.3:58758 dest: /192.168.182.5:50010
2019-10-19 08:17:19,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58758, dest: /192.168.182.5:50010, bytes: 2575, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741916_1092, duration: 2019576
2019-10-19 08:17:19,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741916_1092, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741917_1093 src: /192.168.182.3:58762 dest: /192.168.182.5:50010
2019-10-19 08:17:19,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58762, dest: /192.168.182.5:50010, bytes: 2565, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741917_1093, duration: 4573869
2019-10-19 08:17:19,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741917_1093, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741918_1094 src: /192.168.182.3:58766 dest: /192.168.182.5:50010
2019-10-19 08:17:19,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58766, dest: /192.168.182.5:50010, bytes: 2539, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741918_1094, duration: 2235636
2019-10-19 08:17:19,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741918_1094, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741919_1095 src: /192.168.182.3:58770 dest: /192.168.182.5:50010
2019-10-19 08:17:19,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58770, dest: /192.168.182.5:50010, bytes: 7184, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741919_1095, duration: 1781688
2019-10-19 08:17:19,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741919_1095, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741920_1096 src: /192.168.182.3:58774 dest: /192.168.182.5:50010
2019-10-19 08:17:19,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58774, dest: /192.168.182.5:50010, bytes: 3663, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741920_1096, duration: 2432681
2019-10-19 08:17:19,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741920_1096, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741921_1097 src: /192.168.182.3:58778 dest: /192.168.182.5:50010
2019-10-19 08:17:19,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58778, dest: /192.168.182.5:50010, bytes: 3449, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741921_1097, duration: 1935146
2019-10-19 08:17:19,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741921_1097, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741922_1098 src: /192.168.182.3:58782 dest: /192.168.182.5:50010
2019-10-19 08:17:19,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58782, dest: /192.168.182.5:50010, bytes: 2922, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741922_1098, duration: 3667798
2019-10-19 08:17:19,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741922_1098, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741923_1099 src: /192.168.182.3:58786 dest: /192.168.182.5:50010
2019-10-19 08:17:19,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58786, dest: /192.168.182.5:50010, bytes: 2602, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741923_1099, duration: 4319148
2019-10-19 08:17:19,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741923_1099, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:19,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741924_1100 src: /192.168.182.3:58790 dest: /192.168.182.5:50010
2019-10-19 08:17:19,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58790, dest: /192.168.182.5:50010, bytes: 2576, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741924_1100, duration: 1612620
2019-10-19 08:17:19,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741924_1100, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741925_1101 src: /192.168.182.3:58794 dest: /192.168.182.5:50010
2019-10-19 08:17:20,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58794, dest: /192.168.182.5:50010, bytes: 2569, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741925_1101, duration: 1582662
2019-10-19 08:17:20,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741925_1101, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741926_1102 src: /192.168.182.3:58798 dest: /192.168.182.5:50010
2019-10-19 08:17:20,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58798, dest: /192.168.182.5:50010, bytes: 3405, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741926_1102, duration: 3477701
2019-10-19 08:17:20,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741926_1102, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741927_1103 src: /192.168.182.3:58802 dest: /192.168.182.5:50010
2019-10-19 08:17:20,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58802, dest: /192.168.182.5:50010, bytes: 2989, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741927_1103, duration: 3116059
2019-10-19 08:17:20,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741927_1103, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741928_1104 src: /192.168.182.3:58806 dest: /192.168.182.5:50010
2019-10-19 08:17:20,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58806, dest: /192.168.182.5:50010, bytes: 2726, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741928_1104, duration: 1516203
2019-10-19 08:17:20,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741928_1104, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741929_1105 src: /192.168.182.3:58810 dest: /192.168.182.5:50010
2019-10-19 08:17:20,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58810, dest: /192.168.182.5:50010, bytes: 2673, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741929_1105, duration: 14033333
2019-10-19 08:17:20,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741929_1105, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741930_1106 src: /192.168.182.3:58814 dest: /192.168.182.5:50010
2019-10-19 08:17:20,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58814, dest: /192.168.182.5:50010, bytes: 3052, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741930_1106, duration: 3651069
2019-10-19 08:17:20,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741930_1106, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741931_1107 src: /192.168.182.3:58818 dest: /192.168.182.5:50010
2019-10-19 08:17:20,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58818, dest: /192.168.182.5:50010, bytes: 4243, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741931_1107, duration: 3656899
2019-10-19 08:17:20,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741931_1107, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741932_1108 src: /192.168.182.3:58822 dest: /192.168.182.5:50010
2019-10-19 08:17:20,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58822, dest: /192.168.182.5:50010, bytes: 2618, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741932_1108, duration: 1256857
2019-10-19 08:17:20,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741932_1108, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741933_1109 src: /192.168.182.3:58826 dest: /192.168.182.5:50010
2019-10-19 08:17:20,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58826, dest: /192.168.182.5:50010, bytes: 7556, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741933_1109, duration: 3657178
2019-10-19 08:17:20,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741933_1109, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741934_1110 src: /192.168.182.3:58830 dest: /192.168.182.5:50010
2019-10-19 08:17:20,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58830, dest: /192.168.182.5:50010, bytes: 2561, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741934_1110, duration: 3192228
2019-10-19 08:17:20,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741934_1110, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741935_1111 src: /192.168.182.3:58834 dest: /192.168.182.5:50010
2019-10-19 08:17:20,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58834, dest: /192.168.182.5:50010, bytes: 4842, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741935_1111, duration: 1996041
2019-10-19 08:17:20,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741935_1111, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741936_1112 src: /192.168.182.3:58838 dest: /192.168.182.5:50010
2019-10-19 08:17:20,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58838, dest: /192.168.182.5:50010, bytes: 2739, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741936_1112, duration: 2218074
2019-10-19 08:17:20,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741936_1112, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741937_1113 src: /192.168.182.3:58842 dest: /192.168.182.5:50010
2019-10-19 08:17:20,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58842, dest: /192.168.182.5:50010, bytes: 2879, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741937_1113, duration: 3074221
2019-10-19 08:17:20,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741937_1113, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741938_1114 src: /192.168.182.3:58846 dest: /192.168.182.5:50010
2019-10-19 08:17:20,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58846, dest: /192.168.182.5:50010, bytes: 2608, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741938_1114, duration: 1452713
2019-10-19 08:17:20,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741938_1114, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741939_1115 src: /192.168.182.3:58850 dest: /192.168.182.5:50010
2019-10-19 08:17:20,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58850, dest: /192.168.182.5:50010, bytes: 4253, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741939_1115, duration: 5110137
2019-10-19 08:17:20,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741939_1115, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741940_1116 src: /192.168.182.3:58854 dest: /192.168.182.5:50010
2019-10-19 08:17:20,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58854, dest: /192.168.182.5:50010, bytes: 2898, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741940_1116, duration: 2466909
2019-10-19 08:17:20,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741940_1116, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741941_1117 src: /192.168.182.3:58858 dest: /192.168.182.5:50010
2019-10-19 08:17:20,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58858, dest: /192.168.182.5:50010, bytes: 2561, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741941_1117, duration: 9926280
2019-10-19 08:17:20,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741941_1117, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741942_1118 src: /192.168.182.3:58862 dest: /192.168.182.5:50010
2019-10-19 08:17:20,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58862, dest: /192.168.182.5:50010, bytes: 2613, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741942_1118, duration: 1672491
2019-10-19 08:17:20,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741942_1118, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741943_1119 src: /192.168.182.3:58866 dest: /192.168.182.5:50010
2019-10-19 08:17:20,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58866, dest: /192.168.182.5:50010, bytes: 3244, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741943_1119, duration: 7495042
2019-10-19 08:17:20,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741943_1119, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741944_1120 src: /192.168.182.3:58870 dest: /192.168.182.5:50010
2019-10-19 08:17:20,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58870, dest: /192.168.182.5:50010, bytes: 2586, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741944_1120, duration: 1797313
2019-10-19 08:17:20,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741944_1120, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741945_1121 src: /192.168.182.3:58874 dest: /192.168.182.5:50010
2019-10-19 08:17:20,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58874, dest: /192.168.182.5:50010, bytes: 2794, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741945_1121, duration: 10890237
2019-10-19 08:17:20,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741945_1121, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741946_1122 src: /192.168.182.3:58878 dest: /192.168.182.5:50010
2019-10-19 08:17:20,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58878, dest: /192.168.182.5:50010, bytes: 2549, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741946_1122, duration: 1516492
2019-10-19 08:17:20,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741946_1122, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741947_1123 src: /192.168.182.3:58882 dest: /192.168.182.5:50010
2019-10-19 08:17:20,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58882, dest: /192.168.182.5:50010, bytes: 2571, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741947_1123, duration: 2039392
2019-10-19 08:17:20,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741947_1123, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741948_1124 src: /192.168.182.3:58886 dest: /192.168.182.5:50010
2019-10-19 08:17:20,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58886, dest: /192.168.182.5:50010, bytes: 3503, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741948_1124, duration: 3934138
2019-10-19 08:17:20,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741948_1124, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741949_1125 src: /192.168.182.3:58890 dest: /192.168.182.5:50010
2019-10-19 08:17:20,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58890, dest: /192.168.182.5:50010, bytes: 2551, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741949_1125, duration: 2063795
2019-10-19 08:17:20,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741949_1125, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741950_1126 src: /192.168.182.3:58894 dest: /192.168.182.5:50010
2019-10-19 08:17:20,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58894, dest: /192.168.182.5:50010, bytes: 2571, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741950_1126, duration: 1827741
2019-10-19 08:17:20,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741950_1126, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741951_1127 src: /192.168.182.3:58898 dest: /192.168.182.5:50010
2019-10-19 08:17:20,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58898, dest: /192.168.182.5:50010, bytes: 2575, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741951_1127, duration: 5018576
2019-10-19 08:17:20,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741951_1127, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:20,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741952_1128 src: /192.168.182.3:58902 dest: /192.168.182.5:50010
2019-10-19 08:17:21,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58902, dest: /192.168.182.5:50010, bytes: 2519, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741952_1128, duration: 15080042
2019-10-19 08:17:21,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741952_1128, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741953_1129 src: /192.168.182.3:58906 dest: /192.168.182.5:50010
2019-10-19 08:17:21,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58906, dest: /192.168.182.5:50010, bytes: 2618, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741953_1129, duration: 5334554
2019-10-19 08:17:21,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741953_1129, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741954_1130 src: /192.168.182.3:58910 dest: /192.168.182.5:50010
2019-10-19 08:17:21,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58910, dest: /192.168.182.5:50010, bytes: 2780, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741954_1130, duration: 3012568
2019-10-19 08:17:21,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741954_1130, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741955_1131 src: /192.168.182.3:58914 dest: /192.168.182.5:50010
2019-10-19 08:17:21,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58914, dest: /192.168.182.5:50010, bytes: 2743, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741955_1131, duration: 2518437
2019-10-19 08:17:21,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741955_1131, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741956_1132 src: /192.168.182.3:58918 dest: /192.168.182.5:50010
2019-10-19 08:17:21,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58918, dest: /192.168.182.5:50010, bytes: 2980, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741956_1132, duration: 1389624
2019-10-19 08:17:21,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741956_1132, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741957_1133 src: /192.168.182.3:58922 dest: /192.168.182.5:50010
2019-10-19 08:17:21,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58922, dest: /192.168.182.5:50010, bytes: 2844, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741957_1133, duration: 1761641
2019-10-19 08:17:21,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741957_1133, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741958_1134 src: /192.168.182.3:58926 dest: /192.168.182.5:50010
2019-10-19 08:17:21,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58926, dest: /192.168.182.5:50010, bytes: 3903, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741958_1134, duration: 1702567
2019-10-19 08:17:21,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741958_1134, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741959_1135 src: /192.168.182.3:58930 dest: /192.168.182.5:50010
2019-10-19 08:17:21,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58930, dest: /192.168.182.5:50010, bytes: 2858, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741959_1135, duration: 2009336
2019-10-19 08:17:21,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741959_1135, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741960_1136 src: /192.168.182.3:58934 dest: /192.168.182.5:50010
2019-10-19 08:17:21,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58934, dest: /192.168.182.5:50010, bytes: 3178, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741960_1136, duration: 3345867
2019-10-19 08:17:21,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741960_1136, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741961_1137 src: /192.168.182.3:58938 dest: /192.168.182.5:50010
2019-10-19 08:17:21,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58938, dest: /192.168.182.5:50010, bytes: 2986, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741961_1137, duration: 1772616
2019-10-19 08:17:21,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741961_1137, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741962_1138 src: /192.168.182.3:58942 dest: /192.168.182.5:50010
2019-10-19 08:17:21,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58942, dest: /192.168.182.5:50010, bytes: 2566, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741962_1138, duration: 1863257
2019-10-19 08:17:21,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741962_1138, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741963_1139 src: /192.168.182.3:58946 dest: /192.168.182.5:50010
2019-10-19 08:17:21,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58946, dest: /192.168.182.5:50010, bytes: 2534, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741963_1139, duration: 1631711
2019-10-19 08:17:21,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741963_1139, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741964_1140 src: /192.168.182.3:58950 dest: /192.168.182.5:50010
2019-10-19 08:17:21,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58950, dest: /192.168.182.5:50010, bytes: 2839, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741964_1140, duration: 1109299
2019-10-19 08:17:21,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741964_1140, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741965_1141 src: /192.168.182.3:58954 dest: /192.168.182.5:50010
2019-10-19 08:17:21,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58954, dest: /192.168.182.5:50010, bytes: 2885, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741965_1141, duration: 2316862
2019-10-19 08:17:21,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741965_1141, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741966_1142 src: /192.168.182.3:58958 dest: /192.168.182.5:50010
2019-10-19 08:17:21,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58958, dest: /192.168.182.5:50010, bytes: 2559, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741966_1142, duration: 900972
2019-10-19 08:17:21,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741966_1142, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741967_1143 src: /192.168.182.3:58962 dest: /192.168.182.5:50010
2019-10-19 08:17:21,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58962, dest: /192.168.182.5:50010, bytes: 2534, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741967_1143, duration: 1970275
2019-10-19 08:17:21,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741967_1143, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741968_1144 src: /192.168.182.3:58966 dest: /192.168.182.5:50010
2019-10-19 08:17:21,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58966, dest: /192.168.182.5:50010, bytes: 2571, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741968_1144, duration: 2743739
2019-10-19 08:17:21,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741968_1144, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741969_1145 src: /192.168.182.3:58970 dest: /192.168.182.5:50010
2019-10-19 08:17:21,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58970, dest: /192.168.182.5:50010, bytes: 3217, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741969_1145, duration: 1346024
2019-10-19 08:17:21,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741969_1145, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741970_1146 src: /192.168.182.3:58974 dest: /192.168.182.5:50010
2019-10-19 08:17:21,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58974, dest: /192.168.182.5:50010, bytes: 2623, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741970_1146, duration: 1210908
2019-10-19 08:17:21,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741970_1146, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741971_1147 src: /192.168.182.3:58978 dest: /192.168.182.5:50010
2019-10-19 08:17:21,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58978, dest: /192.168.182.5:50010, bytes: 2623, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741971_1147, duration: 12383032
2019-10-19 08:17:21,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741971_1147, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741972_1148 src: /192.168.182.3:58982 dest: /192.168.182.5:50010
2019-10-19 08:17:21,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58982, dest: /192.168.182.5:50010, bytes: 2765, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741972_1148, duration: 3343285
2019-10-19 08:17:21,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741972_1148, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741973_1149 src: /192.168.182.3:58986 dest: /192.168.182.5:50010
2019-10-19 08:17:21,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58986, dest: /192.168.182.5:50010, bytes: 2926, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741973_1149, duration: 1468376
2019-10-19 08:17:21,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741973_1149, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741974_1150 src: /192.168.182.3:58990 dest: /192.168.182.5:50010
2019-10-19 08:17:21,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58990, dest: /192.168.182.5:50010, bytes: 2586, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741974_1150, duration: 1983938
2019-10-19 08:17:21,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741974_1150, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741975_1151 src: /192.168.182.3:58994 dest: /192.168.182.5:50010
2019-10-19 08:17:21,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58994, dest: /192.168.182.5:50010, bytes: 2754, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741975_1151, duration: 3431123
2019-10-19 08:17:21,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741975_1151, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741976_1152 src: /192.168.182.3:58998 dest: /192.168.182.5:50010
2019-10-19 08:17:21,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:58998, dest: /192.168.182.5:50010, bytes: 2909, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741976_1152, duration: 1366640
2019-10-19 08:17:21,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741976_1152, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741977_1153 src: /192.168.182.3:59002 dest: /192.168.182.5:50010
2019-10-19 08:17:21,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59002, dest: /192.168.182.5:50010, bytes: 2544, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741977_1153, duration: 1499600
2019-10-19 08:17:21,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741977_1153, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741978_1154 src: /192.168.182.3:59006 dest: /192.168.182.5:50010
2019-10-19 08:17:21,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59006, dest: /192.168.182.5:50010, bytes: 2564, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741978_1154, duration: 2888749
2019-10-19 08:17:21,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741978_1154, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741979_1155 src: /192.168.182.3:59010 dest: /192.168.182.5:50010
2019-10-19 08:17:21,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59010, dest: /192.168.182.5:50010, bytes: 2775, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741979_1155, duration: 1347162
2019-10-19 08:17:21,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741979_1155, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741980_1156 src: /192.168.182.3:59014 dest: /192.168.182.5:50010
2019-10-19 08:17:21,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59014, dest: /192.168.182.5:50010, bytes: 2886, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741980_1156, duration: 2430760
2019-10-19 08:17:21,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741980_1156, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741981_1157 src: /192.168.182.3:59018 dest: /192.168.182.5:50010
2019-10-19 08:17:21,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59018, dest: /192.168.182.5:50010, bytes: 2577, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741981_1157, duration: 7107658
2019-10-19 08:17:21,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741981_1157, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741982_1158 src: /192.168.182.3:59022 dest: /192.168.182.5:50010
2019-10-19 08:17:21,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59022, dest: /192.168.182.5:50010, bytes: 3788, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741982_1158, duration: 1308351
2019-10-19 08:17:21,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741982_1158, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741983_1159 src: /192.168.182.3:59026 dest: /192.168.182.5:50010
2019-10-19 08:17:21,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59026, dest: /192.168.182.5:50010, bytes: 4027, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741983_1159, duration: 1873371
2019-10-19 08:17:21,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741983_1159, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:21,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741984_1160 src: /192.168.182.3:59030 dest: /192.168.182.5:50010
2019-10-19 08:17:21,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59030, dest: /192.168.182.5:50010, bytes: 2893, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741984_1160, duration: 3989567
2019-10-19 08:17:21,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741984_1160, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741985_1161 src: /192.168.182.3:59034 dest: /192.168.182.5:50010
2019-10-19 08:17:22,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59034, dest: /192.168.182.5:50010, bytes: 3676, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741985_1161, duration: 2039170
2019-10-19 08:17:22,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741985_1161, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741986_1162 src: /192.168.182.3:59038 dest: /192.168.182.5:50010
2019-10-19 08:17:22,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59038, dest: /192.168.182.5:50010, bytes: 3574, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741986_1162, duration: 1525099
2019-10-19 08:17:22,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741986_1162, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741987_1163 src: /192.168.182.3:59042 dest: /192.168.182.5:50010
2019-10-19 08:17:22,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59042, dest: /192.168.182.5:50010, bytes: 2658, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741987_1163, duration: 6874306
2019-10-19 08:17:22,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741987_1163, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741988_1164 src: /192.168.182.3:59046 dest: /192.168.182.5:50010
2019-10-19 08:17:22,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59046, dest: /192.168.182.5:50010, bytes: 2867, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741988_1164, duration: 4953094
2019-10-19 08:17:22,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741988_1164, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741989_1165 src: /192.168.182.3:59050 dest: /192.168.182.5:50010
2019-10-19 08:17:22,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59050, dest: /192.168.182.5:50010, bytes: 2891, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741989_1165, duration: 1982575
2019-10-19 08:17:22,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741989_1165, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741990_1166 src: /192.168.182.3:59054 dest: /192.168.182.5:50010
2019-10-19 08:17:22,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59054, dest: /192.168.182.5:50010, bytes: 3003, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741990_1166, duration: 7084708
2019-10-19 08:17:22,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741990_1166, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741991_1167 src: /192.168.182.3:59058 dest: /192.168.182.5:50010
2019-10-19 08:17:22,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59058, dest: /192.168.182.5:50010, bytes: 2596, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741991_1167, duration: 1287889
2019-10-19 08:17:22,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741991_1167, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741992_1168 src: /192.168.182.3:59062 dest: /192.168.182.5:50010
2019-10-19 08:17:22,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59062, dest: /192.168.182.5:50010, bytes: 2565, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741992_1168, duration: 2682528
2019-10-19 08:17:22,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741992_1168, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741993_1169 src: /192.168.182.3:59066 dest: /192.168.182.5:50010
2019-10-19 08:17:22,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59066, dest: /192.168.182.5:50010, bytes: 2574, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741993_1169, duration: 2490352
2019-10-19 08:17:22,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741993_1169, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741994_1170 src: /192.168.182.3:59070 dest: /192.168.182.5:50010
2019-10-19 08:17:22,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59070, dest: /192.168.182.5:50010, bytes: 2973, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741994_1170, duration: 2159247
2019-10-19 08:17:22,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741994_1170, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741995_1171 src: /192.168.182.3:59074 dest: /192.168.182.5:50010
2019-10-19 08:17:22,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59074, dest: /192.168.182.5:50010, bytes: 2608, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741995_1171, duration: 1417666
2019-10-19 08:17:22,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741995_1171, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741996_1172 src: /192.168.182.3:59078 dest: /192.168.182.5:50010
2019-10-19 08:17:22,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59078, dest: /192.168.182.5:50010, bytes: 2593, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741996_1172, duration: 13266193
2019-10-19 08:17:22,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741996_1172, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741997_1173 src: /192.168.182.3:59082 dest: /192.168.182.5:50010
2019-10-19 08:17:22,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59082, dest: /192.168.182.5:50010, bytes: 2551, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741997_1173, duration: 3079227
2019-10-19 08:17:22,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741997_1173, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741998_1174 src: /192.168.182.3:59086 dest: /192.168.182.5:50010
2019-10-19 08:17:22,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59086, dest: /192.168.182.5:50010, bytes: 2747, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741998_1174, duration: 1458317
2019-10-19 08:17:22,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741998_1174, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073741999_1175 src: /192.168.182.3:59090 dest: /192.168.182.5:50010
2019-10-19 08:17:22,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59090, dest: /192.168.182.5:50010, bytes: 3364, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073741999_1175, duration: 3603832
2019-10-19 08:17:22,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073741999_1175, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742000_1176 src: /192.168.182.3:59094 dest: /192.168.182.5:50010
2019-10-19 08:17:22,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59094, dest: /192.168.182.5:50010, bytes: 2703, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742000_1176, duration: 4698470
2019-10-19 08:17:22,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742000_1176, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742001_1177 src: /192.168.182.3:59098 dest: /192.168.182.5:50010
2019-10-19 08:17:22,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59098, dest: /192.168.182.5:50010, bytes: 2710, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742001_1177, duration: 2475994
2019-10-19 08:17:22,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742001_1177, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742002_1178 src: /192.168.182.3:59102 dest: /192.168.182.5:50010
2019-10-19 08:17:22,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59102, dest: /192.168.182.5:50010, bytes: 4212, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742002_1178, duration: 1986174
2019-10-19 08:17:22,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742002_1178, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742003_1179 src: /192.168.182.3:59106 dest: /192.168.182.5:50010
2019-10-19 08:17:22,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59106, dest: /192.168.182.5:50010, bytes: 2703, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742003_1179, duration: 1498188
2019-10-19 08:17:22,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742003_1179, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742004_1180 src: /192.168.182.3:59110 dest: /192.168.182.5:50010
2019-10-19 08:17:22,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59110, dest: /192.168.182.5:50010, bytes: 3118, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742004_1180, duration: 1811737
2019-10-19 08:17:22,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742004_1180, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742005_1181 src: /192.168.182.3:59114 dest: /192.168.182.5:50010
2019-10-19 08:17:22,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59114, dest: /192.168.182.5:50010, bytes: 5373, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742005_1181, duration: 2841803
2019-10-19 08:17:22,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742005_1181, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742006_1182 src: /192.168.182.3:59118 dest: /192.168.182.5:50010
2019-10-19 08:17:22,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59118, dest: /192.168.182.5:50010, bytes: 2587, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742006_1182, duration: 11328656
2019-10-19 08:17:22,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742006_1182, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742007_1183 src: /192.168.182.3:59122 dest: /192.168.182.5:50010
2019-10-19 08:17:22,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59122, dest: /192.168.182.5:50010, bytes: 2592, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742007_1183, duration: 1407216
2019-10-19 08:17:22,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742007_1183, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742008_1184 src: /192.168.182.3:59126 dest: /192.168.182.5:50010
2019-10-19 08:17:22,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59126, dest: /192.168.182.5:50010, bytes: 3532, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742008_1184, duration: 1598604
2019-10-19 08:17:22,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742008_1184, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742009_1185 src: /192.168.182.3:59130 dest: /192.168.182.5:50010
2019-10-19 08:17:22,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59130, dest: /192.168.182.5:50010, bytes: 2562, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742009_1185, duration: 3444502
2019-10-19 08:17:22,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742009_1185, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742010_1186 src: /192.168.182.3:59134 dest: /192.168.182.5:50010
2019-10-19 08:17:22,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59134, dest: /192.168.182.5:50010, bytes: 3262, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742010_1186, duration: 1525835
2019-10-19 08:17:22,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742010_1186, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742011_1187 src: /192.168.182.3:59138 dest: /192.168.182.5:50010
2019-10-19 08:17:22,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59138, dest: /192.168.182.5:50010, bytes: 2701, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742011_1187, duration: 1889157
2019-10-19 08:17:22,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742011_1187, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:22,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742012_1188 src: /192.168.182.3:59142 dest: /192.168.182.5:50010
2019-10-19 08:17:22,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59142, dest: /192.168.182.5:50010, bytes: 2897, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742012_1188, duration: 4278456
2019-10-19 08:17:22,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742012_1188, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742013_1189 src: /192.168.182.3:59146 dest: /192.168.182.5:50010
2019-10-19 08:17:23,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59146, dest: /192.168.182.5:50010, bytes: 2576, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742013_1189, duration: 4365091
2019-10-19 08:17:23,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742013_1189, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742014_1190 src: /192.168.182.3:59150 dest: /192.168.182.5:50010
2019-10-19 08:17:23,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59150, dest: /192.168.182.5:50010, bytes: 3350, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742014_1190, duration: 1207889
2019-10-19 08:17:23,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742014_1190, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742015_1191 src: /192.168.182.3:59154 dest: /192.168.182.5:50010
2019-10-19 08:17:23,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59154, dest: /192.168.182.5:50010, bytes: 2908, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742015_1191, duration: 1855494
2019-10-19 08:17:23,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742015_1191, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742016_1192 src: /192.168.182.3:59158 dest: /192.168.182.5:50010
2019-10-19 08:17:23,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59158, dest: /192.168.182.5:50010, bytes: 4397, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742016_1192, duration: 1365687
2019-10-19 08:17:23,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742016_1192, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742017_1193 src: /192.168.182.3:59162 dest: /192.168.182.5:50010
2019-10-19 08:17:23,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59162, dest: /192.168.182.5:50010, bytes: 3094, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742017_1193, duration: 1219310
2019-10-19 08:17:23,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742017_1193, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742018_1194 src: /192.168.182.3:59166 dest: /192.168.182.5:50010
2019-10-19 08:17:23,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59166, dest: /192.168.182.5:50010, bytes: 2581, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742018_1194, duration: 1176504
2019-10-19 08:17:23,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742018_1194, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742019_1195 src: /192.168.182.3:59170 dest: /192.168.182.5:50010
2019-10-19 08:17:23,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59170, dest: /192.168.182.5:50010, bytes: 3635, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742019_1195, duration: 1394769
2019-10-19 08:17:23,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742019_1195, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742020_1196 src: /192.168.182.3:59174 dest: /192.168.182.5:50010
2019-10-19 08:17:23,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59174, dest: /192.168.182.5:50010, bytes: 2555, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742020_1196, duration: 1416936
2019-10-19 08:17:23,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742020_1196, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742021_1197 src: /192.168.182.3:59178 dest: /192.168.182.5:50010
2019-10-19 08:17:23,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59178, dest: /192.168.182.5:50010, bytes: 2529, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742021_1197, duration: 1613419
2019-10-19 08:17:23,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742021_1197, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742022_1198 src: /192.168.182.3:59182 dest: /192.168.182.5:50010
2019-10-19 08:17:23,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59182, dest: /192.168.182.5:50010, bytes: 2539, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742022_1198, duration: 1801647
2019-10-19 08:17:23,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742022_1198, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742023_1199 src: /192.168.182.3:59186 dest: /192.168.182.5:50010
2019-10-19 08:17:23,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59186, dest: /192.168.182.5:50010, bytes: 2559, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742023_1199, duration: 11323947
2019-10-19 08:17:23,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742023_1199, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742024_1200 src: /192.168.182.3:59190 dest: /192.168.182.5:50010
2019-10-19 08:17:23,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59190, dest: /192.168.182.5:50010, bytes: 2529, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742024_1200, duration: 2125183
2019-10-19 08:17:23,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742024_1200, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742025_1201 src: /192.168.182.3:59194 dest: /192.168.182.5:50010
2019-10-19 08:17:23,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2019-10-19 08:17:23,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741885_1061 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741885
2019-10-19 08:17:23,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59194, dest: /192.168.182.5:50010, bytes: 2601, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742025_1201, duration: 1087639
2019-10-19 08:17:23,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742025_1201, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742026_1202 src: /192.168.182.3:59198 dest: /192.168.182.5:50010
2019-10-19 08:17:23,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59198, dest: /192.168.182.5:50010, bytes: 2962, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742026_1202, duration: 2853594
2019-10-19 08:17:23,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742026_1202, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742027_1203 src: /192.168.182.3:59202 dest: /192.168.182.5:50010
2019-10-19 08:17:23,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59202, dest: /192.168.182.5:50010, bytes: 3329, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742027_1203, duration: 4731008
2019-10-19 08:17:23,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742027_1203, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742028_1204 src: /192.168.182.3:59206 dest: /192.168.182.5:50010
2019-10-19 08:17:23,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59206, dest: /192.168.182.5:50010, bytes: 3393, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742028_1204, duration: 1501031
2019-10-19 08:17:23,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742028_1204, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742029_1205 src: /192.168.182.3:59210 dest: /192.168.182.5:50010
2019-10-19 08:17:23,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59210, dest: /192.168.182.5:50010, bytes: 2960, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742029_1205, duration: 2018886
2019-10-19 08:17:23,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742029_1205, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742030_1206 src: /192.168.182.3:59214 dest: /192.168.182.5:50010
2019-10-19 08:17:23,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59214, dest: /192.168.182.5:50010, bytes: 2874, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742030_1206, duration: 1524453
2019-10-19 08:17:23,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742030_1206, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742031_1207 src: /192.168.182.3:59218 dest: /192.168.182.5:50010
2019-10-19 08:17:23,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59218, dest: /192.168.182.5:50010, bytes: 3251, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742031_1207, duration: 1386620
2019-10-19 08:17:23,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742031_1207, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742032_1208 src: /192.168.182.3:59222 dest: /192.168.182.5:50010
2019-10-19 08:17:23,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59222, dest: /192.168.182.5:50010, bytes: 2588, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742032_1208, duration: 1665205
2019-10-19 08:17:23,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742032_1208, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742033_1209 src: /192.168.182.3:59226 dest: /192.168.182.5:50010
2019-10-19 08:17:23,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59226, dest: /192.168.182.5:50010, bytes: 2576, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742033_1209, duration: 1796894
2019-10-19 08:17:23,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742033_1209, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742034_1210 src: /192.168.182.3:59230 dest: /192.168.182.5:50010
2019-10-19 08:17:23,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59230, dest: /192.168.182.5:50010, bytes: 2941, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742034_1210, duration: 1194512
2019-10-19 08:17:23,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742034_1210, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742035_1211 src: /192.168.182.3:59234 dest: /192.168.182.5:50010
2019-10-19 08:17:23,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59234, dest: /192.168.182.5:50010, bytes: 3079, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742035_1211, duration: 1718851
2019-10-19 08:17:23,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742035_1211, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742036_1212 src: /192.168.182.3:59238 dest: /192.168.182.5:50010
2019-10-19 08:17:23,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59238, dest: /192.168.182.5:50010, bytes: 2591, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742036_1212, duration: 1317987
2019-10-19 08:17:23,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742036_1212, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742037_1213 src: /192.168.182.3:59242 dest: /192.168.182.5:50010
2019-10-19 08:17:23,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59242, dest: /192.168.182.5:50010, bytes: 2549, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742037_1213, duration: 915683
2019-10-19 08:17:23,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742037_1213, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742038_1214 src: /192.168.182.3:59246 dest: /192.168.182.5:50010
2019-10-19 08:17:23,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59246, dest: /192.168.182.5:50010, bytes: 2576, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742038_1214, duration: 1742891
2019-10-19 08:17:23,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742038_1214, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742039_1215 src: /192.168.182.3:59250 dest: /192.168.182.5:50010
2019-10-19 08:17:23,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59250, dest: /192.168.182.5:50010, bytes: 2566, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742039_1215, duration: 3474944
2019-10-19 08:17:23,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742039_1215, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742040_1216 src: /192.168.182.3:59254 dest: /192.168.182.5:50010
2019-10-19 08:17:23,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59254, dest: /192.168.182.5:50010, bytes: 2744, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742040_1216, duration: 1493933
2019-10-19 08:17:23,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742040_1216, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742041_1217 src: /192.168.182.3:59258 dest: /192.168.182.5:50010
2019-10-19 08:17:23,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59258, dest: /192.168.182.5:50010, bytes: 2586, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742041_1217, duration: 2049266
2019-10-19 08:17:23,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742041_1217, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742042_1218 src: /192.168.182.3:59262 dest: /192.168.182.5:50010
2019-10-19 08:17:23,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59262, dest: /192.168.182.5:50010, bytes: 2593, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742042_1218, duration: 742963
2019-10-19 08:17:23,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742042_1218, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742043_1219 src: /192.168.182.3:59266 dest: /192.168.182.5:50010
2019-10-19 08:17:23,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59266, dest: /192.168.182.5:50010, bytes: 3331, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742043_1219, duration: 1538399
2019-10-19 08:17:23,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742043_1219, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742044_1220 src: /192.168.182.3:59270 dest: /192.168.182.5:50010
2019-10-19 08:17:23,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59270, dest: /192.168.182.5:50010, bytes: 2836, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742044_1220, duration: 1213751
2019-10-19 08:17:23,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742044_1220, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742045_1221 src: /192.168.182.3:59274 dest: /192.168.182.5:50010
2019-10-19 08:17:23,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59274, dest: /192.168.182.5:50010, bytes: 2529, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742045_1221, duration: 1335317
2019-10-19 08:17:23,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742045_1221, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742046_1222 src: /192.168.182.3:59278 dest: /192.168.182.5:50010
2019-10-19 08:17:23,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59278, dest: /192.168.182.5:50010, bytes: 2539, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742046_1222, duration: 1319446
2019-10-19 08:17:23,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742046_1222, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742047_1223 src: /192.168.182.3:59282 dest: /192.168.182.5:50010
2019-10-19 08:17:23,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59282, dest: /192.168.182.5:50010, bytes: 2603, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742047_1223, duration: 1769574
2019-10-19 08:17:23,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742047_1223, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742048_1224 src: /192.168.182.3:59286 dest: /192.168.182.5:50010
2019-10-19 08:17:23,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59286, dest: /192.168.182.5:50010, bytes: 2580, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742048_1224, duration: 4219676
2019-10-19 08:17:23,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742048_1224, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:23,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742049_1225 src: /192.168.182.3:59290 dest: /192.168.182.5:50010
2019-10-19 08:17:23,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59290, dest: /192.168.182.5:50010, bytes: 2603, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742049_1225, duration: 11169668
2019-10-19 08:17:23,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742049_1225, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742050_1226 src: /192.168.182.3:59294 dest: /192.168.182.5:50010
2019-10-19 08:17:24,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59294, dest: /192.168.182.5:50010, bytes: 2561, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742050_1226, duration: 11683041
2019-10-19 08:17:24,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742050_1226, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742051_1227 src: /192.168.182.3:59298 dest: /192.168.182.5:50010
2019-10-19 08:17:24,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59298, dest: /192.168.182.5:50010, bytes: 2561, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742051_1227, duration: 3549507
2019-10-19 08:17:24,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742051_1227, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742052_1228 src: /192.168.182.3:59302 dest: /192.168.182.5:50010
2019-10-19 08:17:24,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59302, dest: /192.168.182.5:50010, bytes: 3324, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742052_1228, duration: 3971910
2019-10-19 08:17:24,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742052_1228, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742053_1229 src: /192.168.182.3:59306 dest: /192.168.182.5:50010
2019-10-19 08:17:24,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59306, dest: /192.168.182.5:50010, bytes: 2780, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742053_1229, duration: 9619064
2019-10-19 08:17:24,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742053_1229, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742054_1230 src: /192.168.182.3:59310 dest: /192.168.182.5:50010
2019-10-19 08:17:24,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59310, dest: /192.168.182.5:50010, bytes: 3689, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742054_1230, duration: 1370758
2019-10-19 08:17:24,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742054_1230, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742055_1231 src: /192.168.182.3:59314 dest: /192.168.182.5:50010
2019-10-19 08:17:24,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59314, dest: /192.168.182.5:50010, bytes: 5141, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742055_1231, duration: 10797898
2019-10-19 08:17:24,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742055_1231, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742056_1232 src: /192.168.182.3:59318 dest: /192.168.182.5:50010
2019-10-19 08:17:24,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59318, dest: /192.168.182.5:50010, bytes: 2776, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742056_1232, duration: 4627079
2019-10-19 08:17:24,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742056_1232, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742057_1233 src: /192.168.182.3:59322 dest: /192.168.182.5:50010
2019-10-19 08:17:24,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59322, dest: /192.168.182.5:50010, bytes: 2623, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742057_1233, duration: 1768290
2019-10-19 08:17:24,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742057_1233, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742058_1234 src: /192.168.182.3:59326 dest: /192.168.182.5:50010
2019-10-19 08:17:24,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59326, dest: /192.168.182.5:50010, bytes: 2628, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742058_1234, duration: 4032653
2019-10-19 08:17:24,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742058_1234, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742059_1235 src: /192.168.182.3:59330 dest: /192.168.182.5:50010
2019-10-19 08:17:24,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59330, dest: /192.168.182.5:50010, bytes: 2779, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742059_1235, duration: 1799567
2019-10-19 08:17:24,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742059_1235, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742060_1236 src: /192.168.182.3:59334 dest: /192.168.182.5:50010
2019-10-19 08:17:24,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59334, dest: /192.168.182.5:50010, bytes: 2730, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742060_1236, duration: 3986166
2019-10-19 08:17:24,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742060_1236, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742061_1237 src: /192.168.182.3:59338 dest: /192.168.182.5:50010
2019-10-19 08:17:24,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59338, dest: /192.168.182.5:50010, bytes: 2717, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742061_1237, duration: 1112710
2019-10-19 08:17:24,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742061_1237, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742062_1238 src: /192.168.182.3:59342 dest: /192.168.182.5:50010
2019-10-19 08:17:24,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59342, dest: /192.168.182.5:50010, bytes: 2718, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742062_1238, duration: 6008854
2019-10-19 08:17:24,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742062_1238, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742063_1239 src: /192.168.182.3:59346 dest: /192.168.182.5:50010
2019-10-19 08:17:24,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59346, dest: /192.168.182.5:50010, bytes: 2953, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742063_1239, duration: 2805169
2019-10-19 08:17:24,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742063_1239, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742064_1240 src: /192.168.182.3:59350 dest: /192.168.182.5:50010
2019-10-19 08:17:24,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59350, dest: /192.168.182.5:50010, bytes: 2655, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742064_1240, duration: 2789997
2019-10-19 08:17:24,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742064_1240, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742065_1241 src: /192.168.182.3:59354 dest: /192.168.182.5:50010
2019-10-19 08:17:24,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59354, dest: /192.168.182.5:50010, bytes: 3200, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742065_1241, duration: 1579392
2019-10-19 08:17:24,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742065_1241, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742066_1242 src: /192.168.182.3:59358 dest: /192.168.182.5:50010
2019-10-19 08:17:24,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59358, dest: /192.168.182.5:50010, bytes: 2555, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742066_1242, duration: 1892601
2019-10-19 08:17:24,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742066_1242, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742067_1243 src: /192.168.182.3:59362 dest: /192.168.182.5:50010
2019-10-19 08:17:24,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59362, dest: /192.168.182.5:50010, bytes: 2551, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742067_1243, duration: 1476626
2019-10-19 08:17:24,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742067_1243, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742068_1244 src: /192.168.182.3:59366 dest: /192.168.182.5:50010
2019-10-19 08:17:24,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59366, dest: /192.168.182.5:50010, bytes: 2613, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742068_1244, duration: 1903436
2019-10-19 08:17:24,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742068_1244, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:24,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742069_1245 src: /192.168.182.3:59370 dest: /192.168.182.5:50010
2019-10-19 08:17:25,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59370, dest: /192.168.182.5:50010, bytes: 4775, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742069_1245, duration: 1739864
2019-10-19 08:17:25,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742069_1245, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742070_1246 src: /192.168.182.3:59374 dest: /192.168.182.5:50010
2019-10-19 08:17:25,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59374, dest: /192.168.182.5:50010, bytes: 2683, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742070_1246, duration: 1371548
2019-10-19 08:17:25,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742070_1246, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742071_1247 src: /192.168.182.3:59378 dest: /192.168.182.5:50010
2019-10-19 08:17:25,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59378, dest: /192.168.182.5:50010, bytes: 2910, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742071_1247, duration: 1711914
2019-10-19 08:17:25,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742071_1247, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742072_1248 src: /192.168.182.3:59382 dest: /192.168.182.5:50010
2019-10-19 08:17:25,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59382, dest: /192.168.182.5:50010, bytes: 2544, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742072_1248, duration: 1800351
2019-10-19 08:17:25,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742072_1248, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742073_1249 src: /192.168.182.3:59386 dest: /192.168.182.5:50010
2019-10-19 08:17:25,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59386, dest: /192.168.182.5:50010, bytes: 3292, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742073_1249, duration: 1392284
2019-10-19 08:17:25,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742073_1249, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742074_1250 src: /192.168.182.3:59390 dest: /192.168.182.5:50010
2019-10-19 08:17:25,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59390, dest: /192.168.182.5:50010, bytes: 2721, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742074_1250, duration: 1523090
2019-10-19 08:17:25,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742074_1250, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742075_1251 src: /192.168.182.3:59394 dest: /192.168.182.5:50010
2019-10-19 08:17:25,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59394, dest: /192.168.182.5:50010, bytes: 2766, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742075_1251, duration: 1422004
2019-10-19 08:17:25,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742075_1251, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742076_1252 src: /192.168.182.3:59398 dest: /192.168.182.5:50010
2019-10-19 08:17:25,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59398, dest: /192.168.182.5:50010, bytes: 2529, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742076_1252, duration: 3085844
2019-10-19 08:17:25,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742076_1252, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742077_1253 src: /192.168.182.3:59402 dest: /192.168.182.5:50010
2019-10-19 08:17:25,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59402, dest: /192.168.182.5:50010, bytes: 3343, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742077_1253, duration: 1468927
2019-10-19 08:17:25,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742077_1253, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742078_1254 src: /192.168.182.3:59406 dest: /192.168.182.5:50010
2019-10-19 08:17:25,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59406, dest: /192.168.182.5:50010, bytes: 2993, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742078_1254, duration: 1203393
2019-10-19 08:17:25,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742078_1254, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742079_1255 src: /192.168.182.3:59410 dest: /192.168.182.5:50010
2019-10-19 08:17:25,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59410, dest: /192.168.182.5:50010, bytes: 2603, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742079_1255, duration: 1367668
2019-10-19 08:17:25,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742079_1255, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742080_1256 src: /192.168.182.3:59414 dest: /192.168.182.5:50010
2019-10-19 08:17:25,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59414, dest: /192.168.182.5:50010, bytes: 3718, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742080_1256, duration: 4884144
2019-10-19 08:17:25,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742080_1256, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742081_1257 src: /192.168.182.3:59418 dest: /192.168.182.5:50010
2019-10-19 08:17:25,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59418, dest: /192.168.182.5:50010, bytes: 3156, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742081_1257, duration: 1247275
2019-10-19 08:17:25,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742081_1257, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742082_1258 src: /192.168.182.3:59422 dest: /192.168.182.5:50010
2019-10-19 08:17:25,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59422, dest: /192.168.182.5:50010, bytes: 2748, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742082_1258, duration: 842580
2019-10-19 08:17:25,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742082_1258, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742083_1259 src: /192.168.182.3:59426 dest: /192.168.182.5:50010
2019-10-19 08:17:25,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59426, dest: /192.168.182.5:50010, bytes: 2757, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742083_1259, duration: 20214196
2019-10-19 08:17:25,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742083_1259, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742084_1260 src: /192.168.182.3:59430 dest: /192.168.182.5:50010
2019-10-19 08:17:25,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59430, dest: /192.168.182.5:50010, bytes: 3108, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742084_1260, duration: 1245800
2019-10-19 08:17:25,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742084_1260, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742085_1261 src: /192.168.182.3:59434 dest: /192.168.182.5:50010
2019-10-19 08:17:25,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59434, dest: /192.168.182.5:50010, bytes: 2545, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742085_1261, duration: 1900385
2019-10-19 08:17:25,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742085_1261, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:25,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742086_1262 src: /192.168.182.3:59438 dest: /192.168.182.5:50010
2019-10-19 08:17:25,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59438, dest: /192.168.182.5:50010, bytes: 2735, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742086_1262, duration: 1688148
2019-10-19 08:17:25,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742086_1262, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:26,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742087_1263 src: /192.168.182.3:59442 dest: /192.168.182.5:50010
2019-10-19 08:17:26,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59442, dest: /192.168.182.5:50010, bytes: 2591, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742087_1263, duration: 3121315
2019-10-19 08:17:26,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742087_1263, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:26,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742088_1264 src: /192.168.182.3:59446 dest: /192.168.182.5:50010
2019-10-19 08:17:26,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59446, dest: /192.168.182.5:50010, bytes: 2767, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742088_1264, duration: 1307532
2019-10-19 08:17:26,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742088_1264, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:26,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742089_1265 src: /192.168.182.3:59450 dest: /192.168.182.5:50010
2019-10-19 08:17:26,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59450, dest: /192.168.182.5:50010, bytes: 3362, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742089_1265, duration: 2525524
2019-10-19 08:17:26,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742089_1265, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:17:26,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742090_1266 src: /192.168.182.3:59454 dest: /192.168.182.5:50010
2019-10-19 08:17:26,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.182.3:59454, dest: /192.168.182.5:50010, bytes: 3296, op: HDFS_WRITE, cliID: DFSClient_attempt_20191019080110_0005_m_000000_5_-1855583467_42, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742090_1266, duration: 1531414
2019-10-19 08:17:26,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742090_1266, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-10-19 08:58:40,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb2660aca2a8,  containing 1 storage report(s), of which we sent 1. The reports had 235 total blocks and used 1 RPC(s). This took 1 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-19 08:58:40,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-02 05:12:10,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-11-02 05:12:10,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-02 05:12:10,565 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-02 05:12:11,048 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-11-02 05:12:11,229 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-11-02 05:12:11,229 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-11-02 05:12:11,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-11-02 05:12:11,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-11-02 05:12:11,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-11-02 05:12:11,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-11-02 05:12:11,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-11-02 05:12:11,396 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-11-02 05:12:11,400 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-11-02 05:12:11,409 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-11-02 05:12:11,411 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-11-02 05:12:11,411 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-11-02 05:12:11,412 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-11-02 05:12:11,435 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-11-02 05:12:11,437 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-11-02 05:12:11,437 INFO org.mortbay.log: jetty-6.1.26
2019-11-02 05:12:11,761 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-11-02 05:12:12,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-11-02 05:12:12,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-11-02 05:12:12,390 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-11-02 05:12:12,407 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-11-02 05:12:12,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-11-02 05:12:12,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-11-02 05:12:12,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-11-02 05:12:12,518 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-02 05:12:12,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2019-11-02 05:12:12,540 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-11-02 05:12:12,540 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-11-02 05:12:12,931 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 501@um2
2019-11-02 05:12:13,004 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-11-02 05:12:13,004 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-11-02 05:12:13,005 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-11-02 05:12:13,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-11-02 05:12:13,020 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-02 05:12:13,060 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-11-02 05:12:13,060 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-11-02 05:12:13,099 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-11-02 05:12:13,099 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-11-02 05:12:13,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-02 05:12:13,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 47ms
2019-11-02 05:12:13,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 49ms
2019-11-02 05:12:13,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-02 05:12:13,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 42ms
2019-11-02 05:12:13,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 43ms
2019-11-02 05:12:13,198 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1572670393198 with interval 21600000
2019-11-02 05:12:13,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-11-02 05:12:13,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2019-11-02 05:12:13,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-11-02 05:12:13,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=2251
2019-11-02 05:12:13,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000
2019-11-02 05:12:13,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe293133183,  containing 1 storage report(s), of which we sent 1. The reports had 235 total blocks and used 1 RPC(s). This took 5 msec to generate and 40 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-02 05:12:13,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-02 05:12:13,376 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-11-02 05:12:13,376 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-11-02 05:12:13,379 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-11-02 05:12:13,379 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-11-02 05:12:13,379 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-02 05:12:13,386 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-11-02 05:12:18,240 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741882_1058
2019-11-02 05:12:18,244 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741924_1100
2019-11-02 05:25:14,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742091_1267 src: /192.168.251.3:46914 dest: /192.168.251.4:50010
2019-11-02 05:25:14,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:46914, dest: /192.168.251.4:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-589348507_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742091_1267, duration: 122097145
2019-11-02 05:25:14,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742091_1267, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-02 05:25:23,624 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742091_1267
2019-11-02 05:38:08,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742092_1268 src: /192.168.251.3:47000 dest: /192.168.251.4:50010
2019-11-02 05:38:08,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:47000, dest: /192.168.251.4:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_59377606_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742092_1268, duration: 59421147
2019-11-02 05:38:08,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742092_1268, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-02 05:38:13,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742093_1269 src: /192.168.251.3:47004 dest: /192.168.251.4:50010
2019-11-02 05:38:13,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:47004, dest: /192.168.251.4:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_59377606_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742093_1269, duration: 2069505
2019-11-02 05:38:13,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742093_1269, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-02 05:38:14,441 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742092_1268
2019-11-02 05:38:19,450 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742093_1269
2019-11-02 05:40:02,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2019-11-02 05:40:02,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073741858_1034 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir0/blk_1073741858
2019-11-02 05:50:06,285 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742091_1267 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir1/blk_1073742091 for deletion
2019-11-02 05:50:06,285 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073742091_1267 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir1/blk_1073742091
2019-11-02 05:53:13,238 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-278062902-192.168.182.3-1569688905540 Total blocks: 236, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-11-02 06:00:52,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742094_1270 src: /192.168.251.3:47220 dest: /192.168.251.4:50010
2019-11-02 06:00:52,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:47220, dest: /192.168.251.4:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1116601547_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742094_1270, duration: 124292777
2019-11-02 06:00:52,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742094_1270, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-02 06:01:00,298 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742094_1270
2019-11-02 06:01:02,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742095_1271 src: /192.168.251.3:47224 dest: /192.168.251.4:50010
2019-11-02 06:01:02,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:47224, dest: /192.168.251.4:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1116601547_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742095_1271, duration: 3234248
2019-11-02 06:01:02,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742095_1271, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-02 06:01:10,324 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742095_1271
2019-11-02 06:04:53,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742096_1272 src: /192.168.251.3:47322 dest: /192.168.251.4:50010
2019-11-02 06:04:53,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:47322, dest: /192.168.251.4:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1116601547_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742096_1272, duration: 16612156
2019-11-02 06:04:53,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742096_1272, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-02 06:04:58,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742097_1273 src: /192.168.251.3:47326 dest: /192.168.251.4:50010
2019-11-02 06:04:58,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:47326, dest: /192.168.251.4:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1116601547_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742097_1273, duration: 4259524
2019-11-02 06:04:58,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742097_1273, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-02 06:05:00,414 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742096_1272
2019-11-02 06:05:05,421 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742097_1273
2019-11-02 09:24:26,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from um1/192.168.251.3:9000 with active state
2019-11-02 09:24:26,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-11-02 09:24:26,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 successfully registered with NN
2019-11-02 09:24:26,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xea60c28967a,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 2 msec to generate and 19 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-02 09:24:26,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-02 17:47:18,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xea6bed26bdc,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 1 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-02 17:47:18,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-03 09:48:03,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x15003f6d341a,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-03 09:48:03,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-03 10:20:25,504 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-278062902-192.168.182.3-1569688905540 Total blocks: 240, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-11-11 18:59:52,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-11-11 18:59:52,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-11 18:59:53,138 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-11 18:59:53,862 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-11-11 18:59:54,073 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-11-11 18:59:54,073 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-11-11 18:59:54,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-11-11 18:59:54,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-11-11 18:59:54,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-11-11 18:59:54,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-11-11 18:59:54,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-11-11 18:59:54,350 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-11-11 18:59:54,356 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-11-11 18:59:54,365 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-11-11 18:59:54,368 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-11-11 18:59:54,368 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-11-11 18:59:54,368 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-11-11 18:59:54,390 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-11-11 18:59:54,399 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-11-11 18:59:54,400 INFO org.mortbay.log: jetty-6.1.26
2019-11-11 18:59:54,831 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-11-11 18:59:55,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-11-11 18:59:55,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-11-11 18:59:55,450 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-11-11 18:59:55,468 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-11-11 18:59:55,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-11-11 18:59:55,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-11-11 18:59:55,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-11-11 18:59:55,593 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-11 18:59:55,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2019-11-11 18:59:55,620 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-11-11 18:59:55,626 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-11-11 18:59:56,088 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2565@um2
2019-11-11 18:59:56,158 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-11-11 18:59:56,158 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-11-11 18:59:56,160 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-11-11 18:59:56,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-11-11 18:59:56,209 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-11 18:59:56,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-11-11 18:59:56,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-11-11 18:59:56,287 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-11-11 18:59:56,287 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-11-11 18:59:56,288 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-11 18:59:56,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 37ms
2019-11-11 18:59:56,330 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 43ms
2019-11-11 18:59:56,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-11 18:59:56,386 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 52ms
2019-11-11 18:59:56,386 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 55ms
2019-11-11 18:59:56,390 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1573503417390 with interval 21600000
2019-11-11 18:59:56,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-11-11 18:59:56,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2019-11-11 18:59:56,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-11-11 18:59:56,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=2450
2019-11-11 18:59:56,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000
2019-11-11 18:59:56,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x39938c099e,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 2 msec to generate and 168 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-11 18:59:56,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-11 18:59:56,815 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-11-11 18:59:56,815 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-11-11 18:59:56,818 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-11-11 18:59:56,818 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-11-11 18:59:56,818 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-11 18:59:56,828 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-11-11 19:00:01,426 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741912_1088
2019-11-11 19:39:03,899 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5329ms
No GCs detected
2019-11-11 19:39:04,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from um1/192.168.251.3:9000 with active state
2019-11-11 19:39:04,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-11-11 19:39:04,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 successfully registered with NN
2019-11-11 19:39:04,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x25c21fb4187,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 1 msec to generate and 48 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-11 19:39:04,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-11 22:55:22,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x25cd54437b7,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 1 msec to generate and 14 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-11 22:55:22,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-12 00:33:13,026 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-278062902-192.168.182.3-1569688905540 Total blocks: 240, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-11-12 03:43:40,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x12183fb6ea08,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 0 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-12 03:43:40,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-12 06:22:29,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from um1/192.168.251.3:9000 with active state
2019-11-12 06:22:29,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-11-12 06:22:29,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 successfully registered with NN
2019-11-12 06:22:29,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1947b4bac8b5,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 1 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-12 06:22:29,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-12 07:50:51,671 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-278062902-192.168.182.3-1569688905540 Total blocks: 240, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-11-12 13:13:38,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-11-12 13:13:38,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-12 13:13:39,317 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-12 13:13:39,930 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-11-12 13:13:40,131 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-11-12 13:13:40,132 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-11-12 13:13:40,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-11-12 13:13:40,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-11-12 13:13:40,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-11-12 13:13:40,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-11-12 13:13:40,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-11-12 13:13:40,370 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-11-12 13:13:40,379 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-11-12 13:13:40,389 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-11-12 13:13:40,392 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-11-12 13:13:40,392 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-11-12 13:13:40,392 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-11-12 13:13:40,417 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-11-12 13:13:40,428 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-11-12 13:13:40,428 INFO org.mortbay.log: jetty-6.1.26
2019-11-12 13:13:40,823 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-11-12 13:13:41,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-11-12 13:13:41,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-11-12 13:13:41,411 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-11-12 13:13:41,428 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-11-12 13:13:41,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-11-12 13:13:41,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-11-12 13:13:41,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-11-12 13:13:41,544 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-12 13:13:41,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2019-11-12 13:13:41,573 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-11-12 13:13:41,574 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-11-12 13:13:42,047 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2981@um2
2019-11-12 13:13:42,162 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-11-12 13:13:42,162 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-11-12 13:13:42,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-11-12 13:13:42,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-11-12 13:13:42,181 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-12 13:13:42,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-11-12 13:13:42,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-11-12 13:13:42,271 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-11-12 13:13:42,271 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-11-12 13:13:42,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-12 13:13:42,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 56ms
2019-11-12 13:13:42,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 57ms
2019-11-12 13:13:42,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-12 13:13:42,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 29ms
2019-11-12 13:13:42,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 29ms
2019-11-12 13:13:42,360 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1573565935360 with interval 21600000
2019-11-12 13:13:42,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-11-12 13:13:42,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2019-11-12 13:13:42,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-11-12 13:13:42,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=2563
2019-11-12 13:13:42,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000
2019-11-12 13:13:42,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8af55eaaeea,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 2 msec to generate and 71 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-12 13:13:42,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-12 13:13:42,545 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-11-12 13:13:42,545 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-11-12 13:13:42,548 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-11-12 13:13:42,548 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-11-12 13:13:42,548 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-12 13:13:42,565 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-11-12 14:38:55,403 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-278062902-192.168.182.3-1569688905540 Total blocks: 240, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-11-13 11:05:16,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-11-13 11:05:16,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-13 11:05:16,698 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-13 11:05:17,300 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-11-13 11:05:17,484 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-11-13 11:05:17,484 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-11-13 11:05:17,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-11-13 11:05:17,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-11-13 11:05:17,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-11-13 11:05:17,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-11-13 11:05:17,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-11-13 11:05:17,788 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-11-13 11:05:17,793 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-11-13 11:05:17,803 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-11-13 11:05:17,805 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-11-13 11:05:17,805 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-11-13 11:05:17,805 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-11-13 11:05:17,820 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-11-13 11:05:17,823 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-11-13 11:05:17,823 INFO org.mortbay.log: jetty-6.1.26
2019-11-13 11:05:18,188 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-11-13 11:05:19,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-11-13 11:05:19,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-11-13 11:05:19,512 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-11-13 11:05:19,563 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-11-13 11:05:19,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-11-13 11:05:19,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-11-13 11:05:19,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-11-13 11:05:19,897 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-13 11:05:19,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2019-11-13 11:05:19,922 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-11-13 11:05:19,925 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-11-13 11:05:20,980 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2364@um2
2019-11-13 11:05:21,122 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-11-13 11:05:21,122 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-11-13 11:05:21,125 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-11-13 11:05:21,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-11-13 11:05:21,142 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-13 11:05:21,173 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-11-13 11:05:21,173 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-11-13 11:05:21,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-11-13 11:05:21,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-11-13 11:05:21,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-13 11:05:21,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 274ms
2019-11-13 11:05:21,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 293ms
2019-11-13 11:05:21,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-13 11:05:21,687 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 91ms
2019-11-13 11:05:21,696 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 101ms
2019-11-13 11:05:21,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1573650327699 with interval 21600000
2019-11-13 11:05:21,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-11-13 11:05:21,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2019-11-13 11:05:21,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-11-13 11:05:21,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=2599
2019-11-13 11:05:21,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000
2019-11-13 11:05:22,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1bb947e9c8,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 6 msec to generate and 126 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-13 11:05:22,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-13 11:05:22,129 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-11-13 11:05:22,129 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-11-13 11:05:22,167 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-11-13 11:05:22,167 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-11-13 11:05:22,167 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-13 11:05:22,184 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-11-13 11:05:26,856 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742006_1182
2019-11-13 14:49:36,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-11-13 14:49:36,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-13 14:49:38,906 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-13 14:49:41,043 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-11-13 14:49:41,694 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-11-13 14:49:41,694 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-11-13 14:49:41,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-11-13 14:49:41,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-11-13 14:49:41,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-11-13 14:49:41,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-11-13 14:49:41,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-11-13 14:49:42,325 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-11-13 14:49:42,343 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-11-13 14:49:42,394 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-11-13 14:49:42,407 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-11-13 14:49:42,407 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-11-13 14:49:42,407 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-11-13 14:49:42,530 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-11-13 14:49:42,595 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-11-13 14:49:42,595 INFO org.mortbay.log: jetty-6.1.26
2019-11-13 14:49:43,965 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-11-13 14:49:46,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-11-13 14:49:46,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-11-13 14:49:46,562 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-11-13 14:49:46,692 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-11-13 14:49:46,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-11-13 14:49:46,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-11-13 14:49:47,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-11-13 14:49:47,037 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-13 14:49:47,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2019-11-13 14:49:47,208 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-11-13 14:49:47,296 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-11-13 14:49:49,837 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2466@um2
2019-11-13 14:49:50,235 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-11-13 14:49:50,235 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-11-13 14:49:50,292 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-11-13 14:49:50,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-11-13 14:49:50,378 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-11-13 14:49:50,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-11-13 14:49:50,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-11-13 14:49:50,940 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-11-13 14:49:50,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-11-13 14:49:50,951 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-13 14:49:51,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 270ms
2019-11-13 14:49:51,233 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 292ms
2019-11-13 14:49:51,253 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-11-13 14:49:51,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 253ms
2019-11-13 14:49:51,512 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 279ms
2019-11-13 14:49:51,518 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1573666612518 with interval 21600000
2019-11-13 14:49:51,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-11-13 14:49:51,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2019-11-13 14:49:51,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-11-13 14:49:52,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=2600
2019-11-13 14:49:52,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000
2019-11-13 14:49:52,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x29bf3da89a,  containing 1 storage report(s), of which we sent 1. The reports had 240 total blocks and used 1 RPC(s). This took 25 msec to generate and 223 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-13 14:49:52,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-13 14:49:52,343 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-11-13 14:49:52,343 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-11-13 14:49:52,364 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-11-13 14:49:52,364 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-11-13 14:49:52,364 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-11-13 14:49:52,460 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-11-13 14:50:45,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742098_1274 src: /192.168.251.3:44172 dest: /192.168.251.4:50010
2019-11-13 14:50:45,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44172, dest: /192.168.251.4:50010, bytes: 2404428, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742098_1274, duration: 291710050
2019-11-13 14:50:45,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742098_1274, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:46,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742099_1275 src: /192.168.251.3:44176 dest: /192.168.251.4:50010
2019-11-13 14:50:46,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44176, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742099_1275, duration: 24345146
2019-11-13 14:50:46,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742099_1275, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:46,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742100_1276 src: /192.168.251.3:44180 dest: /192.168.251.4:50010
2019-11-13 14:50:46,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44180, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742100_1276, duration: 7762242
2019-11-13 14:50:46,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742100_1276, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:46,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742101_1277 src: /192.168.251.3:44184 dest: /192.168.251.4:50010
2019-11-13 14:50:46,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44184, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742101_1277, duration: 8296082
2019-11-13 14:50:46,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742101_1277, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:46,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742102_1278 src: /192.168.251.3:44188 dest: /192.168.251.4:50010
2019-11-13 14:50:46,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44188, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742102_1278, duration: 12524597
2019-11-13 14:50:46,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742102_1278, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:46,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742103_1279 src: /192.168.251.3:44192 dest: /192.168.251.4:50010
2019-11-13 14:50:46,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44192, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742103_1279, duration: 7855174
2019-11-13 14:50:46,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742103_1279, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:46,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742104_1280 src: /192.168.251.3:44196 dest: /192.168.251.4:50010
2019-11-13 14:50:46,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44196, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742104_1280, duration: 4243496
2019-11-13 14:50:46,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742104_1280, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:46,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742105_1281 src: /192.168.251.3:44200 dest: /192.168.251.4:50010
2019-11-13 14:50:46,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44200, dest: /192.168.251.4:50010, bytes: 5438457, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742105_1281, duration: 166767468
2019-11-13 14:50:46,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742105_1281, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:46,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742106_1282 src: /192.168.251.3:44204 dest: /192.168.251.4:50010
2019-11-13 14:50:46,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44204, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742106_1282, duration: 4975785
2019-11-13 14:50:46,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742106_1282, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742107_1283 src: /192.168.251.3:44208 dest: /192.168.251.4:50010
2019-11-13 14:50:47,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44208, dest: /192.168.251.4:50010, bytes: 4913, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742107_1283, duration: 7071227
2019-11-13 14:50:47,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742107_1283, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742108_1284 src: /192.168.251.3:44212 dest: /192.168.251.4:50010
2019-11-13 14:50:47,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44212, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742108_1284, duration: 20908346
2019-11-13 14:50:47,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742108_1284, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742109_1285 src: /192.168.251.3:44216 dest: /192.168.251.4:50010
2019-11-13 14:50:47,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44216, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742109_1285, duration: 6928480
2019-11-13 14:50:47,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742109_1285, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742110_1286 src: /192.168.251.3:44220 dest: /192.168.251.4:50010
2019-11-13 14:50:47,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44220, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742110_1286, duration: 7483377
2019-11-13 14:50:47,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742110_1286, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742111_1287 src: /192.168.251.3:44224 dest: /192.168.251.4:50010
2019-11-13 14:50:47,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44224, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742111_1287, duration: 6483301
2019-11-13 14:50:47,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742111_1287, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742112_1288 src: /192.168.251.3:44228 dest: /192.168.251.4:50010
2019-11-13 14:50:47,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44228, dest: /192.168.251.4:50010, bytes: 52091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742112_1288, duration: 6686524
2019-11-13 14:50:47,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742112_1288, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742113_1289 src: /192.168.251.3:44232 dest: /192.168.251.4:50010
2019-11-13 14:50:47,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44232, dest: /192.168.251.4:50010, bytes: 715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742113_1289, duration: 9999236
2019-11-13 14:50:47,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742113_1289, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742114_1290 src: /192.168.251.3:44236 dest: /192.168.251.4:50010
2019-11-13 14:50:47,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44236, dest: /192.168.251.4:50010, bytes: 6058, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742114_1290, duration: 4913130
2019-11-13 14:50:47,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742114_1290, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742115_1291 src: /192.168.251.3:44240 dest: /192.168.251.4:50010
2019-11-13 14:50:47,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44240, dest: /192.168.251.4:50010, bytes: 117110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742115_1291, duration: 14085136
2019-11-13 14:50:47,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742115_1291, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742116_1292 src: /192.168.251.3:44244 dest: /192.168.251.4:50010
2019-11-13 14:50:47,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44244, dest: /192.168.251.4:50010, bytes: 2031, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742116_1292, duration: 4522486
2019-11-13 14:50:47,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742116_1292, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742117_1293 src: /192.168.251.3:44248 dest: /192.168.251.4:50010
2019-11-13 14:50:47,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44248, dest: /192.168.251.4:50010, bytes: 2031, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742117_1293, duration: 6569990
2019-11-13 14:50:47,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742117_1293, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742118_1294 src: /192.168.251.3:44252 dest: /192.168.251.4:50010
2019-11-13 14:50:47,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44252, dest: /192.168.251.4:50010, bytes: 2039, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742118_1294, duration: 5369536
2019-11-13 14:50:47,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742118_1294, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742119_1295 src: /192.168.251.3:44256 dest: /192.168.251.4:50010
2019-11-13 14:50:47,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44256, dest: /192.168.251.4:50010, bytes: 2039, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742119_1295, duration: 4833827
2019-11-13 14:50:47,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742119_1295, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:47,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742120_1296 src: /192.168.251.3:44260 dest: /192.168.251.4:50010
2019-11-13 14:50:47,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44260, dest: /192.168.251.4:50010, bytes: 37022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742120_1296, duration: 7448308
2019-11-13 14:50:47,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742120_1296, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742121_1297 src: /192.168.251.3:44264 dest: /192.168.251.4:50010
2019-11-13 14:50:48,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44264, dest: /192.168.251.4:50010, bytes: 38982, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742121_1297, duration: 5160622
2019-11-13 14:50:48,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742121_1297, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742122_1298 src: /192.168.251.3:44268 dest: /192.168.251.4:50010
2019-11-13 14:50:48,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44268, dest: /192.168.251.4:50010, bytes: 240, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742122_1298, duration: 4535108
2019-11-13 14:50:48,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742122_1298, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742123_1299 src: /192.168.251.3:44272 dest: /192.168.251.4:50010
2019-11-13 14:50:48,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44272, dest: /192.168.251.4:50010, bytes: 13440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742123_1299, duration: 5007543
2019-11-13 14:50:48,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742123_1299, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742124_1300 src: /192.168.251.3:44276 dest: /192.168.251.4:50010
2019-11-13 14:50:48,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44276, dest: /192.168.251.4:50010, bytes: 2183240, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742124_1300, duration: 114042828
2019-11-13 14:50:48,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742124_1300, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742125_1301 src: /192.168.251.3:44280 dest: /192.168.251.4:50010
2019-11-13 14:50:48,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44280, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742125_1301, duration: 7881434
2019-11-13 14:50:48,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742125_1301, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742126_1302 src: /192.168.251.3:44284 dest: /192.168.251.4:50010
2019-11-13 14:50:48,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44284, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742126_1302, duration: 4290824
2019-11-13 14:50:48,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742126_1302, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742127_1303 src: /192.168.251.3:44288 dest: /192.168.251.4:50010
2019-11-13 14:50:48,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44288, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742127_1303, duration: 3792241
2019-11-13 14:50:48,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742127_1303, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742128_1304 src: /192.168.251.3:44292 dest: /192.168.251.4:50010
2019-11-13 14:50:48,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44292, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742128_1304, duration: 4226246
2019-11-13 14:50:48,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742128_1304, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742129_1305 src: /192.168.251.3:44296 dest: /192.168.251.4:50010
2019-11-13 14:50:48,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44296, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742129_1305, duration: 4714262
2019-11-13 14:50:48,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742129_1305, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742130_1306 src: /192.168.251.3:44300 dest: /192.168.251.4:50010
2019-11-13 14:50:48,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44300, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742130_1306, duration: 7147699
2019-11-13 14:50:48,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742130_1306, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:48,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742131_1307 src: /192.168.251.3:44304 dest: /192.168.251.4:50010
2019-11-13 14:50:49,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44304, dest: /192.168.251.4:50010, bytes: 2925431, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742131_1307, duration: 106867980
2019-11-13 14:50:49,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742131_1307, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:49,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742132_1308 src: /192.168.251.3:44308 dest: /192.168.251.4:50010
2019-11-13 14:50:49,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44308, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742132_1308, duration: 2872416
2019-11-13 14:50:49,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742132_1308, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:49,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742133_1309 src: /192.168.251.3:44312 dest: /192.168.251.4:50010
2019-11-13 14:50:49,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44312, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742133_1309, duration: 5058918
2019-11-13 14:50:49,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742133_1309, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:49,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742134_1310 src: /192.168.251.3:44316 dest: /192.168.251.4:50010
2019-11-13 14:50:49,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44316, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742134_1310, duration: 3634662
2019-11-13 14:50:49,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742134_1310, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:49,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742135_1311 src: /192.168.251.3:44320 dest: /192.168.251.4:50010
2019-11-13 14:50:49,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44320, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742135_1311, duration: 15193248
2019-11-13 14:50:49,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742135_1311, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:49,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742136_1312 src: /192.168.251.3:44324 dest: /192.168.251.4:50010
2019-11-13 14:50:49,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44324, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742136_1312, duration: 4576280
2019-11-13 14:50:49,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742136_1312, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:50:49,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742137_1313 src: /192.168.251.3:44328 dest: /192.168.251.4:50010
2019-11-13 14:50:49,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44328, dest: /192.168.251.4:50010, bytes: 702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1918550231_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742137_1313, duration: 4319872
2019-11-13 14:50:49,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742137_1313, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-11-13 14:56:42,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742138_1314 src: /192.168.251.3:44350 dest: /192.168.251.4:50010
2019-11-13 14:56:42,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:44350, dest: /192.168.251.4:50010, bytes: 2435123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_658878531_1, offset: 0, srvID: b6a7b098-41ae-4cde-9c76-1d8d664607d5, blockid: BP-278062902-192.168.182.3-1569688905540:blk_1073742138_1314, duration: 153373467
2019-11-13 14:56:42,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-278062902-192.168.182.3-1569688905540:blk_1073742138_1314, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2019-12-02 21:28:19,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-12-02 21:28:19,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-12-02 21:28:19,841 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-12-02 21:28:20,530 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-12-02 21:28:20,718 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-12-02 21:28:20,718 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-12-02 21:28:20,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-12-02 21:28:20,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-12-02 21:28:20,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-12-02 21:28:20,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-12-02 21:28:20,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-12-02 21:28:21,009 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-12-02 21:28:21,013 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-12-02 21:28:21,024 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-12-02 21:28:21,027 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-12-02 21:28:21,027 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-12-02 21:28:21,027 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-12-02 21:28:21,078 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-12-02 21:28:21,115 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-12-02 21:28:21,115 INFO org.mortbay.log: jetty-6.1.26
2019-12-02 21:28:21,623 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-12-02 21:28:22,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-12-02 21:28:22,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-12-02 21:28:22,229 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-12-02 21:28:22,250 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-12-02 21:28:22,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-12-02 21:28:22,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-12-02 21:28:22,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-12-02 21:28:22,355 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-12-02 21:28:22,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2019-12-02 21:28:22,415 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-12-02 21:28:22,415 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-12-02 21:28:23,927 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 3541@um2
2019-12-02 21:28:24,043 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-12-02 21:28:24,043 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-12-02 21:28:24,044 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-12-02 21:28:24,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-12-02 21:28:24,059 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-12-02 21:28:24,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-12-02 21:28:24,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-12-02 21:28:24,207 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-12-02 21:28:24,207 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-12-02 21:28:24,208 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-12-02 21:28:24,238 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 31ms
2019-12-02 21:28:24,239 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 31ms
2019-12-02 21:28:24,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-12-02 21:28:24,279 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 37ms
2019-12-02 21:28:24,279 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 40ms
2019-12-02 21:28:24,281 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1575326089281 with interval 21600000
2019-12-02 21:28:24,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-12-02 21:28:24,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2019-12-02 21:28:24,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-12-02 21:28:24,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=3339
2019-12-02 21:28:24,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000
2019-12-02 21:28:24,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x189bd955561,  containing 1 storage report(s), of which we sent 1. The reports had 281 total blocks and used 1 RPC(s). This took 4 msec to generate and 276 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-12-02 21:28:24,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-12-02 21:28:24,908 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-12-02 21:28:24,908 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-12-02 21:28:24,911 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-12-02 21:28:24,911 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-12-02 21:28:24,911 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-12-02 21:28:24,936 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-12-02 21:28:59,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742140_1316 src: /192.168.251.3:45078 dest: /192.168.251.4:50010
2019-12-02 21:28:59,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742200_1376 src: /192.168.251.3:45080 dest: /192.168.251.4:50010
2019-12-02 21:28:59,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742140_1316 src: /192.168.251.3:45078 dest: /192.168.251.4:50010 of size 2435123
2019-12-02 21:28:59,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742200_1376 src: /192.168.251.3:45080 dest: /192.168.251.4:50010 of size 3751306
2019-12-02 21:29:02,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742158_1334 src: /192.168.251.3:45084 dest: /192.168.251.4:50010
2019-12-02 21:29:02,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742142_1318 src: /192.168.251.3:45082 dest: /192.168.251.4:50010
2019-12-02 21:29:02,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742158_1334 src: /192.168.251.3:45084 dest: /192.168.251.4:50010 of size 2435123
2019-12-02 21:29:02,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742142_1318 src: /192.168.251.3:45082 dest: /192.168.251.4:50010 of size 2435123
2019-12-02 21:29:05,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742162_1338 src: /192.168.251.3:45090 dest: /192.168.251.4:50010
2019-12-02 21:29:05,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742160_1336 src: /192.168.251.3:45092 dest: /192.168.251.4:50010
2019-12-02 21:29:05,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742162_1338 src: /192.168.251.3:45090 dest: /192.168.251.4:50010 of size 36074
2019-12-02 21:29:05,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742160_1336 src: /192.168.251.3:45092 dest: /192.168.251.4:50010 of size 328
2019-12-02 21:29:08,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742179_1355 src: /192.168.251.3:45096 dest: /192.168.251.4:50010
2019-12-02 21:29:08,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742179_1355 src: /192.168.251.3:45096 dest: /192.168.251.4:50010 of size 327
2019-12-02 21:29:08,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742181_1357 src: /192.168.251.3:45098 dest: /192.168.251.4:50010
2019-12-02 21:29:08,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742181_1357 src: /192.168.251.3:45098 dest: /192.168.251.4:50010 of size 29276
2019-12-02 21:29:11,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742185_1361 src: /192.168.251.3:45102 dest: /192.168.251.4:50010
2019-12-02 21:29:11,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742185_1361 src: /192.168.251.3:45102 dest: /192.168.251.4:50010 of size 66
2019-12-02 21:29:11,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742187_1363 src: /192.168.251.3:45100 dest: /192.168.251.4:50010
2019-12-02 21:29:11,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742187_1363 src: /192.168.251.3:45100 dest: /192.168.251.4:50010 of size 3751306
2019-12-02 21:29:14,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742193_1369 src: /192.168.251.3:45106 dest: /192.168.251.4:50010
2019-12-02 21:29:14,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742193_1369 src: /192.168.251.3:45106 dest: /192.168.251.4:50010 of size 123637
2019-12-02 21:29:14,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742194_1370 src: /192.168.251.3:45108 dest: /192.168.251.4:50010
2019-12-02 21:29:14,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742194_1370 src: /192.168.251.3:45108 dest: /192.168.251.4:50010 of size 73
2019-12-02 21:29:17,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742199_1375 src: /192.168.251.3:45116 dest: /192.168.251.4:50010
2019-12-02 21:29:17,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742198_1374 src: /192.168.251.3:45114 dest: /192.168.251.4:50010
2019-12-02 21:29:17,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742198_1374 src: /192.168.251.3:45114 dest: /192.168.251.4:50010 of size 15
2019-12-02 21:29:17,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742199_1375 src: /192.168.251.3:45116 dest: /192.168.251.4:50010 of size 4043
2019-12-02 21:30:06,578 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742193_1369 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir1/blk_1073742193 for deletion
2019-12-02 21:30:06,582 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742187_1363 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir1/blk_1073742187 for deletion
2019-12-02 21:30:06,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073742193_1369 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir1/blk_1073742193
2019-12-02 21:30:06,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073742187_1363 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir1/blk_1073742187
2019-12-02 21:31:53,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742164_1340 src: /192.168.251.3:45148 dest: /192.168.251.4:50010
2019-12-02 21:31:53,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742175_1351 src: /192.168.251.3:45150 dest: /192.168.251.4:50010
2019-12-02 21:31:53,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742164_1340 src: /192.168.251.3:45148 dest: /192.168.251.4:50010 of size 108620
2019-12-02 21:31:53,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742175_1351 src: /192.168.251.3:45150 dest: /192.168.251.4:50010 of size 2435123
2019-12-02 21:31:56,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742183_1359 src: /192.168.251.3:45152 dest: /192.168.251.4:50010
2019-12-02 21:31:56,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742184_1360 src: /192.168.251.3:45154 dest: /192.168.251.4:50010
2019-12-02 21:31:56,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742183_1359 src: /192.168.251.3:45152 dest: /192.168.251.4:50010 of size 108625
2019-12-02 21:31:56,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742184_1360 src: /192.168.251.3:45154 dest: /192.168.251.4:50010 of size 2435123
2019-12-02 21:31:59,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742196_1372 src: /192.168.251.3:45156 dest: /192.168.251.4:50010
2019-12-02 21:31:59,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742196_1372 src: /192.168.251.3:45156 dest: /192.168.251.4:50010 of size 130
2019-12-02 21:31:59,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-278062902-192.168.182.3-1569688905540:blk_1073742197_1373 src: /192.168.251.3:45158 dest: /192.168.251.4:50010
2019-12-02 21:31:59,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-278062902-192.168.182.3-1569688905540:blk_1073742197_1373 src: /192.168.251.3:45158 dest: /192.168.251.4:50010 of size 3751306
2019-12-02 21:32:00,233 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742175_1351
2019-12-02 21:32:07,441 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073742184_1360
2019-12-02 23:34:49,312 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-278062902-192.168.182.3-1569688905540 Total blocks: 299, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-12-02 23:59:48,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9cccfea3b7d,  containing 1 storage report(s), of which we sent 1. The reports had 299 total blocks and used 1 RPC(s). This took 0 msec to generate and 11 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-12-02 23:59:48,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-12-02 23:59:51,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742138_1314 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir1/blk_1073742138 for deletion
2019-12-02 23:59:51,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-278062902-192.168.182.3-1569688905540 blk_1073742138_1314 file /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540/current/finalized/subdir0/subdir1/blk_1073742138
2019-12-03 05:34:49,361 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-278062902-192.168.182.3-1569688905540 Total blocks: 298, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-12-03 05:59:47,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1d715b8ffd3b,  containing 1 storage report(s), of which we sent 1. The reports had 298 total blocks and used 1 RPC(s). This took 1 msec to generate and 872 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-12-03 05:59:47,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-12-03 11:59:46,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x227b84fb1b9a,  containing 1 storage report(s), of which we sent 1. The reports had 298 total blocks and used 1 RPC(s). This took 1 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-12-03 11:59:46,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-12-14 17:51:22,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2019-12-14 17:51:22,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-12-14 17:51:23,001 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-12-14 17:51:24,054 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-12-14 17:51:24,338 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-12-14 17:51:24,339 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-12-14 17:51:24,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2019-12-14 17:51:24,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-12-14 17:51:24,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-12-14 17:51:24,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2019-12-14 17:51:24,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2019-12-14 17:51:24,605 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-12-14 17:51:24,625 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-12-14 17:51:24,657 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-12-14 17:51:24,661 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-12-14 17:51:24,661 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-12-14 17:51:24,662 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-12-14 17:51:24,707 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-12-14 17:51:24,725 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2019-12-14 17:51:24,725 INFO org.mortbay.log: jetty-6.1.26
2019-12-14 17:51:25,468 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2019-12-14 17:51:26,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2019-12-14 17:51:26,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-12-14 17:51:26,472 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2019-12-14 17:51:26,515 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-12-14 17:51:26,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-12-14 17:51:26,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-12-14 17:51:26,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-12-14 17:51:26,716 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-12-14 17:51:26,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2019-12-14 17:51:26,747 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-12-14 17:51:26,749 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-12-14 17:51:27,534 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2716@um2
2019-12-14 17:51:27,807 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-278062902-192.168.182.3-1569688905540
2019-12-14 17:51:27,808 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-278062902-192.168.182.3-1569688905540
2019-12-14 17:51:27,809 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2019-12-14 17:51:27,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=811552738;bpid=BP-278062902-192.168.182.3-1569688905540;lv=-56;nsInfo=lv=-60;cid=CID-52b20ea5-265a-4894-9ae4-b6de307aaed4;nsid=811552738;c=0;bpid=BP-278062902-192.168.182.3-1569688905540;dnuuid=b6a7b098-41ae-4cde-9c76-1d8d664607d5
2019-12-14 17:51:27,868 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2019-12-14 17:51:27,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2019-12-14 17:51:27,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2019-12-14 17:51:28,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-12-14 17:51:28,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-278062902-192.168.182.3-1569688905540
2019-12-14 17:51:28,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-12-14 17:51:28,279 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-278062902-192.168.182.3-1569688905540 on /orgz/data2/current: 93ms
2019-12-14 17:51:28,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-278062902-192.168.182.3-1569688905540: 105ms
2019-12-14 17:51:28,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current...
2019-12-14 17:51:28,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-278062902-192.168.182.3-1569688905540 on volume /orgz/data2/current: 119ms
2019-12-14 17:51:28,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 122ms
2019-12-14 17:51:28,405 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1576344273405 with interval 21600000
2019-12-14 17:51:28,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2019-12-14 17:51:28,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2019-12-14 17:51:28,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-12-14 17:51:28,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=3481
2019-12-14 17:51:28,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-278062902-192.168.182.3-1569688905540 (Datanode Uuid b6a7b098-41ae-4cde-9c76-1d8d664607d5) service to um1/192.168.251.3:9000
2019-12-14 17:51:28,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x16506cd827,  containing 1 storage report(s), of which we sent 1. The reports had 298 total blocks and used 1 RPC(s). This took 12 msec to generate and 87 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-12-14 17:51:28,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-278062902-192.168.182.3-1569688905540
2019-12-14 17:51:28,855 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2019-12-14 17:51:28,855 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-12-14 17:51:28,859 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2019-12-14 17:51:28,871 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2019-12-14 17:51:28,871 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-278062902-192.168.182.3-1569688905540
2019-12-14 17:51:28,904 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-278062902-192.168.182.3-1569688905540 to blockPoolScannerMap, new size=1
2019-12-14 17:51:33,568 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-278062902-192.168.182.3-1569688905540:blk_1073741965_1141
2019-12-14 18:24:33,442 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-278062902-192.168.182.3-1569688905540 Total blocks: 298, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2020-01-06 11:37:16,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-01-06 11:37:16,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-06 11:37:17,286 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-06 11:37:17,887 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-01-06 11:37:18,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-01-06 11:37:18,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-01-06 11:37:18,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-01-06 11:37:18,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-01-06 11:37:18,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-01-06 11:37:18,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-01-06 11:37:18,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-01-06 11:37:18,225 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-01-06 11:37:18,228 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-01-06 11:37:18,235 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-01-06 11:37:18,238 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-01-06 11:37:18,238 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-01-06 11:37:18,238 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-01-06 11:37:18,268 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-01-06 11:37:18,270 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-01-06 11:37:18,270 INFO org.mortbay.log: jetty-6.1.26
2020-01-06 11:37:18,566 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-01-06 11:37:19,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-01-06 11:37:19,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-01-06 11:37:19,116 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-01-06 11:37:19,132 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-01-06 11:37:19,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-01-06 11:37:19,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-01-06 11:37:19,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-01-06 11:37:19,209 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-06 11:37:19,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-01-06 11:37:19,240 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-01-06 11:37:19,243 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-01-06 11:37:19,600 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 3466@um2
2020-01-06 11:37:19,601 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /orgz/data2 is not formatted for BP-1099568443-192.168.251.3-1578306934316
2020-01-06 11:37:19,601 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2020-01-06 11:37:19,673 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-01-06 11:37:19,673 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-01-06 11:37:19,674 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316 is not formatted for BP-1099568443-192.168.251.3-1578306934316
2020-01-06 11:37:19,674 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2020-01-06 11:37:19,674 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1099568443-192.168.251.3-1578306934316 directory /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current
2020-01-06 11:37:19,676 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-01-06 11:37:19,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=null
2020-01-06 11:37:19,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-01-06 11:37:19,693 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-06 11:37:19,729 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-01-06 11:37:19,729 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-01-06 11:37:19,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-01-06 11:37:19,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-06 11:37:19,739 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-06 11:37:19,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 22ms
2020-01-06 11:37:19,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 26ms
2020-01-06 11:37:19,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-06 11:37:19,763 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 0ms
2020-01-06 11:37:19,763 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2020-01-06 11:37:19,765 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1578317073765 with interval 21600000
2020-01-06 11:37:19,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-01-06 11:37:19,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-01-06 11:37:19,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-01-06 11:37:19,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=1
2020-01-06 11:37:19,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-01-06 11:37:19,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8c37e103c2,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 29 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-01-06 11:37:19,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-06 11:37:19,904 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-01-06 11:37:19,905 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-01-06 11:37:19,908 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-01-06 11:37:19,909 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-01-06 11:37:19,909 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-06 11:37:19,913 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-01-06 11:44:32,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741825_1001 src: /192.168.251.3:52652 dest: /192.168.251.4:50010
2020-01-06 11:44:32,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:52652, dest: /192.168.251.4:50010, bytes: 130, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766601173_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741825_1001, duration: 93849215
2020-01-06 11:44:32,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 11:44:40,088 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741825_1001
2020-01-06 16:31:48,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-01-06 16:31:48,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-06 16:31:49,210 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-06 16:31:49,699 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-01-06 16:31:49,869 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-01-06 16:31:49,869 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-01-06 16:31:49,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-01-06 16:31:49,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-01-06 16:31:49,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-01-06 16:31:49,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-01-06 16:31:49,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-01-06 16:31:50,025 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-01-06 16:31:50,030 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-01-06 16:31:50,037 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-01-06 16:31:50,049 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-01-06 16:31:50,049 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-01-06 16:31:50,049 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-01-06 16:31:50,063 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-01-06 16:31:50,065 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-01-06 16:31:50,065 INFO org.mortbay.log: jetty-6.1.26
2020-01-06 16:31:50,352 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-01-06 16:31:50,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-01-06 16:31:50,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-01-06 16:31:50,891 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-01-06 16:31:50,908 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-01-06 16:31:50,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-01-06 16:31:50,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-01-06 16:31:50,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-01-06 16:31:50,998 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-06 16:31:51,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-01-06 16:31:51,023 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-01-06 16:31:51,024 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-01-06 16:31:51,366 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2398@um2
2020-01-06 16:31:51,438 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-01-06 16:31:51,438 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-01-06 16:31:51,440 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-01-06 16:31:51,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-01-06 16:31:51,452 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-06 16:31:51,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-01-06 16:31:51,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-01-06 16:31:51,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-01-06 16:31:51,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-06 16:31:51,522 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-06 16:31:51,547 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 25ms
2020-01-06 16:31:51,548 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 26ms
2020-01-06 16:31:51,548 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-06 16:31:51,550 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 2ms
2020-01-06 16:31:51,550 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2020-01-06 16:31:51,553 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1578336529553 with interval 21600000
2020-01-06 16:31:51,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-01-06 16:31:51,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-01-06 16:31:51,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-01-06 16:31:51,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=24
2020-01-06 16:31:51,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-01-06 16:31:51,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x19dfb9f6dbb,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 1 msec to generate and 59 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-01-06 16:31:51,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-06 16:31:51,687 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-01-06 16:31:51,687 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-01-06 16:31:51,690 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-01-06 16:31:51,690 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-01-06 16:31:51,694 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-06 16:31:51,698 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-01-06 16:46:23,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741826_1002 src: /192.168.251.3:42524 dest: /192.168.251.4:50010
2020-01-06 16:46:23,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42524, dest: /192.168.251.4:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_166176766_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741826_1002, duration: 100809028
2020-01-06 16:46:23,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:46:31,910 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741826_1002
2020-01-06 16:50:54,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741827_1003 src: /192.168.251.3:42570 dest: /192.168.251.4:50010
2020-01-06 16:50:54,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42570, dest: /192.168.251.4:50010, bytes: 3165, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_96702469_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741827_1003, duration: 29077189
2020-01-06 16:50:54,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:50:55,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741828_1004 src: /192.168.251.3:42576 dest: /192.168.251.4:50010
2020-01-06 16:50:56,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42576, dest: /192.168.251.4:50010, bytes: 32441258, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_96702469_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741828_1004, duration: 419592180
2020-01-06 16:50:56,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:50:56,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741829_1005 src: /192.168.251.3:42580 dest: /192.168.251.4:50010
2020-01-06 16:50:56,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42580, dest: /192.168.251.4:50010, bytes: 224, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_96702469_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741829_1005, duration: 1708123
2020-01-06 16:50:56,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:50:56,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741830_1006 src: /192.168.251.3:42584 dest: /192.168.251.4:50010
2020-01-06 16:50:56,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42584, dest: /192.168.251.4:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_96702469_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741830_1006, duration: 1605407
2020-01-06 16:50:56,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:50:56,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741831_1007 src: /192.168.251.3:42588 dest: /192.168.251.4:50010
2020-01-06 16:50:56,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42588, dest: /192.168.251.4:50010, bytes: 235227, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_96702469_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741831_1007, duration: 9538367
2020-01-06 16:50:56,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:51:02,189 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741827_1003
2020-01-06 16:52:10,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2020-01-06 16:52:10,704 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741827_1003 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741827
2020-01-06 16:52:50,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741832_1008 src: /192.168.251.3:42724 dest: /192.168.251.4:50010
2020-01-06 16:52:50,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42724, dest: /192.168.251.4:50010, bytes: 1725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1006619873_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741832_1008, duration: 56645965
2020-01-06 16:52:50,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:52:50,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741833_1009 src: /192.168.251.3:42728 dest: /192.168.251.4:50010
2020-01-06 16:52:50,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42728, dest: /192.168.251.4:50010, bytes: 75, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1006619873_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741833_1009, duration: 9371160
2020-01-06 16:52:50,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:52:55,695 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2020-01-06 16:52:55,696 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741833_1009 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741833
2020-01-06 16:52:57,272 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741832_1008
2020-01-06 16:57:08,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741834_1010 src: /192.168.251.3:42774 dest: /192.168.251.4:50010
2020-01-06 16:57:08,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42774, dest: /192.168.251.4:50010, bytes: 66, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_162183773_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741834_1010, duration: 74869512
2020-01-06 16:57:08,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:57:08,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741835_1011 src: /192.168.251.3:42778 dest: /192.168.251.4:50010
2020-01-06 16:57:08,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42778, dest: /192.168.251.4:50010, bytes: 4043, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_162183773_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741835_1011, duration: 2318400
2020-01-06 16:57:08,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:57:08,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741836_1012 src: /192.168.251.3:42782 dest: /192.168.251.4:50010
2020-01-06 16:57:08,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42782, dest: /192.168.251.4:50010, bytes: 3048, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_162183773_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741836_1012, duration: 3193243
2020-01-06 16:57:08,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:57:08,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741837_1013 src: /192.168.251.3:42786 dest: /192.168.251.4:50010
2020-01-06 16:57:08,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42786, dest: /192.168.251.4:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_162183773_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741837_1013, duration: 1989673
2020-01-06 16:57:08,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:57:08,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741838_1014 src: /192.168.251.3:42790 dest: /192.168.251.4:50010
2020-01-06 16:57:08,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42790, dest: /192.168.251.4:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_162183773_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741838_1014, duration: 1532850
2020-01-06 16:57:08,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:57:08,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741839_1015 src: /192.168.251.3:42794 dest: /192.168.251.4:50010
2020-01-06 16:57:08,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:42794, dest: /192.168.251.4:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_162183773_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741839_1015, duration: 6000900
2020-01-06 16:57:08,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-06 16:57:17,462 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741839_1015
2020-01-06 16:57:17,464 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741837_1013
2020-01-11 17:01:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-01-11 17:01:14,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-11 17:01:15,094 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:01:16,012 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-01-11 17:01:16,282 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-01-11 17:01:16,283 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-01-11 17:01:16,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-01-11 17:01:16,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-01-11 17:01:16,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-01-11 17:01:16,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-01-11 17:01:16,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-01-11 17:01:16,562 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-01-11 17:01:16,571 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-01-11 17:01:16,583 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-01-11 17:01:16,599 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-01-11 17:01:16,599 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-01-11 17:01:16,599 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-01-11 17:01:16,628 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-01-11 17:01:16,631 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-01-11 17:01:16,631 INFO org.mortbay.log: jetty-6.1.26
2020-01-11 17:01:17,204 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-01-11 17:01:18,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-01-11 17:01:18,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-01-11 17:01:18,337 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-01-11 17:01:18,361 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-01-11 17:01:18,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-01-11 17:01:18,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-01-11 17:01:18,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-01-11 17:01:18,510 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:01:18,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-01-11 17:01:18,576 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-01-11 17:01:18,606 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-01-11 17:01:19,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-01-11 17:01:21,238 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.251.3:9000
2020-01-11 17:01:26,991 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 18192@um2
2020-01-11 17:01:27,261 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:01:27,261 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:01:27,262 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-01-11 17:01:27,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-01-11 17:01:27,331 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:01:27,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-01-11 17:01:27,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-01-11 17:01:27,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-01-11 17:01:27,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:01:27,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-11 17:01:27,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 56ms
2020-01-11 17:01:27,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 71ms
2020-01-11 17:01:27,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-11 17:01:27,699 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 19ms
2020-01-11 17:01:27,699 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 22ms
2020-01-11 17:01:27,703 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1578771446703 with interval 21600000
2020-01-11 17:01:27,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-01-11 17:01:27,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-01-11 17:01:27,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-01-11 17:01:28,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=210
2020-01-11 17:01:28,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-01-11 17:01:28,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4be60a1c49,  containing 1 storage report(s), of which we sent 1. The reports had 13 total blocks and used 1 RPC(s). This took 2 msec to generate and 200 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-01-11 17:01:28,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:01:28,732 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-01-11 17:01:28,732 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-01-11 17:01:28,736 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-01-11 17:01:28,736 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-01-11 17:01:28,737 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:01:28,758 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-01-11 17:01:32,790 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741825_1001
2020-01-11 17:09:45,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741840_1016 src: /192.168.251.3:58256 dest: /192.168.251.4:50010
2020-01-11 17:09:45,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58256, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-553425283_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741840_1016, duration: 170774292
2020-01-11 17:09:45,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:09:46,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741841_1017 src: /192.168.251.3:58260 dest: /192.168.251.4:50010
2020-01-11 17:09:46,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58260, dest: /192.168.251.4:50010, bytes: 108, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-553425283_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741841_1017, duration: 29454412
2020-01-11 17:09:46,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:09:46,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741842_1018 src: /192.168.251.3:58264 dest: /192.168.251.4:50010
2020-01-11 17:09:46,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58264, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-553425283_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741842_1018, duration: 9107669
2020-01-11 17:09:46,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:09:46,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741843_1019 src: /192.168.251.3:58268 dest: /192.168.251.4:50010
2020-01-11 17:09:46,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58268, dest: /192.168.251.4:50010, bytes: 89300, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-553425283_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741843_1019, duration: 22148096
2020-01-11 17:09:46,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:09:53,012 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741841_1017
2020-01-11 17:09:56,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741844_1020 src: /192.168.251.4:39982 dest: /192.168.251.4:50010
2020-01-11 17:09:56,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:39982, dest: /192.168.251.4:50010, bytes: 106206, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-596335564_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741844_1020, duration: 26155377
2020-01-11 17:09:56,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741844_1020, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:10:03,043 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741844_1020
2020-01-11 17:10:06,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741845_1021 src: /192.168.251.4:39990 dest: /192.168.251.4:50010
2020-01-11 17:10:12,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741846_1022 src: /192.168.251.4:40002 dest: /192.168.251.4:50010
2020-01-11 17:10:12,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40002, dest: /192.168.251.4:50010, bytes: 3072, op: HDFS_WRITE, cliID: DFSClient_attempt_1578758538571_0001_r_000000_0_-1408023534_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741846_1022, duration: 148923009
2020-01-11 17:10:12,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741846_1022, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:10:13,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:39990, dest: /192.168.251.4:50010, bytes: 33621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-596335564_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741845_1021, duration: 6753206116
2020-01-11 17:10:13,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741845_1021, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:10:13,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741847_1023 src: /192.168.251.4:40008 dest: /192.168.251.4:50010
2020-01-11 17:10:13,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40008, dest: /192.168.251.4:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-596335564_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741847_1023, duration: 21378026
2020-01-11 17:10:13,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741847_1023, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:10:13,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741848_1024 src: /192.168.251.4:40014 dest: /192.168.251.4:50010
2020-01-11 17:10:13,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40014, dest: /192.168.251.4:50010, bytes: 33621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-596335564_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741848_1024, duration: 9313953
2020-01-11 17:10:13,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741848_1024, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:10:13,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741849_1025 src: /192.168.251.4:40018 dest: /192.168.251.4:50010
2020-01-11 17:10:13,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40018, dest: /192.168.251.4:50010, bytes: 106206, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-596335564_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741849_1025, duration: 14503655
2020-01-11 17:10:13,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741849_1025, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:10:18,050 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741846_1022
2020-01-11 17:10:22,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2020-01-11 17:10:22,359 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2020-01-11 17:10:22,359 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2020-01-11 17:10:22,359 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741840_1016 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741840
2020-01-11 17:10:22,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741841_1017 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741841
2020-01-11 17:10:22,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741842_1018 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741842
2020-01-11 17:10:22,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2020-01-11 17:10:22,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2020-01-11 17:10:22,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2020-01-11 17:10:22,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741843_1019 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741843
2020-01-11 17:10:22,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741844_1020 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741844
2020-01-11 17:10:22,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741845_1021 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741845
2020-01-11 17:10:23,061 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741848_1024
2020-01-11 17:10:55,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2020-01-11 17:10:55,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741847_1023 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741847
2020-01-11 17:18:19,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741850_1026 src: /192.168.251.3:58378 dest: /192.168.251.4:50010
2020-01-11 17:18:19,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58378, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-318164587_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741850_1026, duration: 44953645
2020-01-11 17:18:19,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:18:20,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741851_1027 src: /192.168.251.3:58382 dest: /192.168.251.4:50010
2020-01-11 17:18:20,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58382, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-318164587_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741851_1027, duration: 7024320
2020-01-11 17:18:20,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:18:20,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741852_1028 src: /192.168.251.3:58386 dest: /192.168.251.4:50010
2020-01-11 17:18:20,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58386, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-318164587_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741852_1028, duration: 4223319
2020-01-11 17:18:20,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:18:20,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741853_1029 src: /192.168.251.3:58390 dest: /192.168.251.4:50010
2020-01-11 17:18:20,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58390, dest: /192.168.251.4:50010, bytes: 89303, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-318164587_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741853_1029, duration: 15653901
2020-01-11 17:18:20,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:18:25,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741854_1030 src: /192.168.251.4:40056 dest: /192.168.251.4:50010
2020-01-11 17:18:25,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40056, dest: /192.168.251.4:50010, bytes: 106209, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1775815154_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741854_1030, duration: 44842086
2020-01-11 17:18:25,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741854_1030, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:18:28,344 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741851_1027
2020-01-11 17:18:31,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741855_1031 src: /192.168.251.4:40072 dest: /192.168.251.4:50010
2020-01-11 17:18:33,348 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741854_1030
2020-01-11 17:18:38,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741856_1032 src: /192.168.251.3:58408 dest: /192.168.251.4:50010
2020-01-11 17:18:38,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58408, dest: /192.168.251.4:50010, bytes: 2925, op: HDFS_WRITE, cliID: DFSClient_attempt_1578758538571_0002_r_000000_0_-1781670785_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741856_1032, duration: 74654669
2020-01-11 17:18:38,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:18:38,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40072, dest: /192.168.251.4:50010, bytes: 33659, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1775815154_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741855_1031, duration: 7611215143
2020-01-11 17:18:38,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741855_1031, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:18:39,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741857_1033 src: /192.168.251.4:40080 dest: /192.168.251.4:50010
2020-01-11 17:18:39,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40080, dest: /192.168.251.4:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1775815154_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741857_1033, duration: 6470765
2020-01-11 17:18:39,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741857_1033, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:18:39,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741858_1034 src: /192.168.251.4:40086 dest: /192.168.251.4:50010
2020-01-11 17:18:39,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40086, dest: /192.168.251.4:50010, bytes: 33659, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1775815154_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741858_1034, duration: 17391619
2020-01-11 17:18:39,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741858_1034, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:18:39,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741859_1035 src: /192.168.251.4:40090 dest: /192.168.251.4:50010
2020-01-11 17:18:39,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40090, dest: /192.168.251.4:50010, bytes: 106209, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1775815154_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741859_1035, duration: 12115812
2020-01-11 17:18:39,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741859_1035, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:18:43,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741850_1026 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741850
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741851_1027 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741851
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741852_1028 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741852
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2020-01-11 17:18:43,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741853_1029 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741853
2020-01-11 17:18:43,695 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741854_1030 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741854
2020-01-11 17:18:43,695 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741855_1031 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741855
2020-01-11 17:18:48,360 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741856_1032
2020-01-11 17:20:07,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2020-01-11 17:20:07,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741857_1033 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741857
2020-01-11 17:23:26,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741860_1036 src: /192.168.251.3:58628 dest: /192.168.251.4:50010
2020-01-11 17:23:26,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58628, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1045570799_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741860_1036, duration: 44894206
2020-01-11 17:23:26,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:23:26,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741861_1037 src: /192.168.251.3:58632 dest: /192.168.251.4:50010
2020-01-11 17:23:26,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58632, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1045570799_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741861_1037, duration: 9583828
2020-01-11 17:23:26,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:23:26,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741862_1038 src: /192.168.251.3:58636 dest: /192.168.251.4:50010
2020-01-11 17:23:26,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58636, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1045570799_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741862_1038, duration: 6052677
2020-01-11 17:23:26,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:23:26,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741863_1039 src: /192.168.251.3:58640 dest: /192.168.251.4:50010
2020-01-11 17:23:26,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58640, dest: /192.168.251.4:50010, bytes: 89302, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1045570799_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741863_1039, duration: 5182088
2020-01-11 17:23:26,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:23:31,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741864_1040 src: /192.168.251.4:40120 dest: /192.168.251.4:50010
2020-01-11 17:23:31,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40120, dest: /192.168.251.4:50010, bytes: 106208, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2131320151_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741864_1040, duration: 34998808
2020-01-11 17:23:31,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741864_1040, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:23:37,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741865_1041 src: /192.168.251.4:40134 dest: /192.168.251.4:50010
2020-01-11 17:23:43,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741866_1042 src: /192.168.251.4:40148 dest: /192.168.251.4:50010
2020-01-11 17:23:43,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40148, dest: /192.168.251.4:50010, bytes: 1479, op: HDFS_WRITE, cliID: DFSClient_attempt_1578758538571_0003_r_000001_0_-1773968035_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741866_1042, duration: 74336640
2020-01-11 17:23:43,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741866_1042, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:23:44,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741867_1043 src: /192.168.251.3:58658 dest: /192.168.251.4:50010
2020-01-11 17:23:44,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58658, dest: /192.168.251.4:50010, bytes: 1446, op: HDFS_WRITE, cliID: DFSClient_attempt_1578758538571_0003_r_000000_0_1325905154_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741867_1043, duration: 32146098
2020-01-11 17:23:44,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:23:44,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40134, dest: /192.168.251.4:50010, bytes: 41198, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2131320151_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741865_1041, duration: 7678947504
2020-01-11 17:23:44,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741865_1041, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:23:44,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741868_1044 src: /192.168.251.4:40156 dest: /192.168.251.4:50010
2020-01-11 17:23:44,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40156, dest: /192.168.251.4:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2131320151_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741868_1044, duration: 7122665
2020-01-11 17:23:44,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741868_1044, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:23:44,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741869_1045 src: /192.168.251.4:40162 dest: /192.168.251.4:50010
2020-01-11 17:23:44,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40162, dest: /192.168.251.4:50010, bytes: 41198, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2131320151_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741869_1045, duration: 6405364
2020-01-11 17:23:44,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741869_1045, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:23:44,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741870_1046 src: /192.168.251.4:40166 dest: /192.168.251.4:50010
2020-01-11 17:23:44,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40166, dest: /192.168.251.4:50010, bytes: 106208, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2131320151_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741870_1046, duration: 8365316
2020-01-11 17:23:44,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741870_1046, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:23:50,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2020-01-11 17:23:50,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2020-01-11 17:23:50,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2020-01-11 17:23:50,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2020-01-11 17:23:50,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2020-01-11 17:23:50,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2020-01-11 17:23:50,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741860_1036 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741860
2020-01-11 17:23:50,049 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741861_1037 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741861
2020-01-11 17:23:50,049 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741862_1038 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741862
2020-01-11 17:23:50,049 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741863_1039 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741863
2020-01-11 17:23:50,049 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741864_1040 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741864
2020-01-11 17:23:50,049 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741865_1041 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741865
2020-01-11 17:26:05,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2020-01-11 17:26:05,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741868_1044 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741868
2020-01-11 17:32:32,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741871_1047 src: /192.168.251.3:58774 dest: /192.168.251.4:50010
2020-01-11 17:32:32,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58774, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_634327498_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741871_1047, duration: 43826483
2020-01-11 17:32:32,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:32:32,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741872_1048 src: /192.168.251.3:58778 dest: /192.168.251.4:50010
2020-01-11 17:32:32,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58778, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_634327498_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741872_1048, duration: 3260459
2020-01-11 17:32:32,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:32:32,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741873_1049 src: /192.168.251.3:58782 dest: /192.168.251.4:50010
2020-01-11 17:32:32,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58782, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_634327498_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741873_1049, duration: 4393236
2020-01-11 17:32:32,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:32:32,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741874_1050 src: /192.168.251.3:58786 dest: /192.168.251.4:50010
2020-01-11 17:32:32,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:58786, dest: /192.168.251.4:50010, bytes: 89285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_634327498_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741874_1050, duration: 13285803
2020-01-11 17:32:32,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:38:24,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741875_1051 src: /192.168.251.4:40220 dest: /192.168.251.4:50010
2020-01-11 17:38:24,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40220, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1740837265_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741875_1051, duration: 115998516
2020-01-11 17:38:24,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741875_1051, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:38:25,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741876_1052 src: /192.168.251.4:40224 dest: /192.168.251.4:50010
2020-01-11 17:38:25,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40224, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1740837265_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741876_1052, duration: 18290072
2020-01-11 17:38:25,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741876_1052, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:38:25,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741877_1053 src: /192.168.251.4:40228 dest: /192.168.251.4:50010
2020-01-11 17:38:25,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40228, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1740837265_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741877_1053, duration: 8211821
2020-01-11 17:38:25,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741877_1053, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:38:25,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741878_1054 src: /192.168.251.4:40232 dest: /192.168.251.4:50010
2020-01-11 17:38:25,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40232, dest: /192.168.251.4:50010, bytes: 89255, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1740837265_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741878_1054, duration: 16411504
2020-01-11 17:38:25,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741878_1054, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:38:33,885 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741877_1053
2020-01-11 17:38:42,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741879_1055 src: /192.168.251.3:59062 dest: /192.168.251.4:50010
2020-01-11 17:38:42,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59062, dest: /192.168.251.4:50010, bytes: 31153, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1918143527_93, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741879_1055, duration: 79097728
2020-01-11 17:38:42,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:38:48,894 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741879_1055
2020-01-11 17:40:13,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741880_1056 src: /192.168.251.3:59142 dest: /192.168.251.4:50010
2020-01-11 17:40:13,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59142, dest: /192.168.251.4:50010, bytes: 15350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_399352746_134, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741880_1056, duration: 17321189
2020-01-11 17:40:13,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:40:18,938 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741880_1056
2020-01-11 17:41:13,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741881_1057 src: /192.168.251.4:40258 dest: /192.168.251.4:50010
2020-01-11 17:41:13,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40258, dest: /192.168.251.4:50010, bytes: 1369, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1012451210_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741881_1057, duration: 29966472
2020-01-11 17:41:13,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741881_1057, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:41:18,968 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741881_1057
2020-01-11 17:43:57,187 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.251.4"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2020-01-11 17:43:59,649 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2020-01-11 17:43:59,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.251.4
************************************************************/
2020-01-11 17:46:52,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-01-11 17:46:52,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-11 17:46:52,581 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:46:52,951 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-01-11 17:46:53,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-01-11 17:46:53,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-01-11 17:46:53,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-01-11 17:46:53,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-01-11 17:46:53,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-01-11 17:46:53,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-01-11 17:46:53,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-01-11 17:46:53,175 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-01-11 17:46:53,179 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-01-11 17:46:53,190 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-01-11 17:46:53,192 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-01-11 17:46:53,193 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-01-11 17:46:53,193 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-01-11 17:46:53,213 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-01-11 17:46:53,216 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-01-11 17:46:53,216 INFO org.mortbay.log: jetty-6.1.26
2020-01-11 17:46:53,441 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-01-11 17:46:53,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-01-11 17:46:53,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-01-11 17:46:53,795 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-01-11 17:46:53,809 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-01-11 17:46:53,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-01-11 17:46:53,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-01-11 17:46:53,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-01-11 17:46:53,858 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:46:53,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-01-11 17:46:53,879 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-01-11 17:46:53,881 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-01-11 17:46:54,249 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 21739@um2
2020-01-11 17:46:54,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:46:54,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:46:54,303 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-01-11 17:46:54,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-01-11 17:46:54,312 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:46:54,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-01-11 17:46:54,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-01-11 17:46:54,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-01-11 17:46:54,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:46:54,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-11 17:46:54,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current: 34455552
2020-01-11 17:46:54,382 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 9ms
2020-01-11 17:46:54,382 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 9ms
2020-01-11 17:46:54,382 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-11 17:46:54,388 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 5ms
2020-01-11 17:46:54,388 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2020-01-11 17:46:54,390 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1578774475390 with interval 21600000
2020-01-11 17:46:54,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-01-11 17:46:54,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-01-11 17:46:54,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-01-11 17:46:54,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=578
2020-01-11 17:46:54,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-01-11 17:46:54,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2c69a7c0ea9,  containing 1 storage report(s), of which we sent 1. The reports had 34 total blocks and used 1 RPC(s). This took 2 msec to generate and 92 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-01-11 17:46:54,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:46:54,647 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-01-11 17:46:54,647 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-01-11 17:46:54,648 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-01-11 17:46:54,648 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-01-11 17:46:54,648 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:46:54,654 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-01-11 17:46:59,493 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741873_1049
2020-01-11 17:51:05,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741882_1058 src: /192.168.251.4:40304 dest: /192.168.251.4:50010
2020-01-11 17:51:05,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40304, dest: /192.168.251.4:50010, bytes: 1352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1546074364_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741882_1058, duration: 94062941
2020-01-11 17:51:05,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741882_1058, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:51:14,830 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741882_1058
2020-01-11 17:52:37,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741883_1059 src: /192.168.251.4:40318 dest: /192.168.251.4:50010
2020-01-11 17:52:37,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40318, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_786066414_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741883_1059, duration: 49066159
2020-01-11 17:52:37,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741883_1059, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:52:37,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741884_1060 src: /192.168.251.4:40322 dest: /192.168.251.4:50010
2020-01-11 17:52:37,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40322, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_786066414_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741884_1060, duration: 7755686
2020-01-11 17:52:37,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741884_1060, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:52:37,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741885_1061 src: /192.168.251.4:40326 dest: /192.168.251.4:50010
2020-01-11 17:52:37,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40326, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_786066414_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741885_1061, duration: 6958684
2020-01-11 17:52:37,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741885_1061, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:52:37,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741886_1062 src: /192.168.251.4:40330 dest: /192.168.251.4:50010
2020-01-11 17:52:37,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40330, dest: /192.168.251.4:50010, bytes: 89256, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_786066414_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741886_1062, duration: 10783007
2020-01-11 17:52:37,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741886_1062, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 17:55:17,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741887_1063 src: /192.168.251.3:59476 dest: /192.168.251.4:50010
2020-01-11 17:55:17,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59476, dest: /192.168.251.4:50010, bytes: 19142, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-683135505_93, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741887_1063, duration: 61428514
2020-01-11 17:55:17,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 17:56:13,146 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "um2/192.168.251.4"; destination host is: "um1":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2020-01-11 17:56:16,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-01-11 17:56:17,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-01-11 17:56:18,006 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2020-01-11 17:56:18,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.251.4
************************************************************/
2020-01-11 17:58:41,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-01-11 17:58:41,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-11 17:58:42,252 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:58:42,579 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-01-11 17:58:42,695 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-01-11 17:58:42,695 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-01-11 17:58:42,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-01-11 17:58:42,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-01-11 17:58:42,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-01-11 17:58:42,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-01-11 17:58:42,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-01-11 17:58:42,810 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-01-11 17:58:42,813 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-01-11 17:58:42,822 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-01-11 17:58:42,824 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-01-11 17:58:42,825 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-01-11 17:58:42,825 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-01-11 17:58:42,840 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-01-11 17:58:42,843 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-01-11 17:58:42,843 INFO org.mortbay.log: jetty-6.1.26
2020-01-11 17:58:43,099 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-01-11 17:58:43,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-01-11 17:58:43,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-01-11 17:58:43,529 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-01-11 17:58:43,548 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-01-11 17:58:43,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-01-11 17:58:43,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-01-11 17:58:43,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-01-11 17:58:43,613 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:58:43,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-01-11 17:58:43,623 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-01-11 17:58:43,656 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-01-11 17:58:44,022 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 23178@um2
2020-01-11 17:58:44,103 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:58:44,104 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:58:44,104 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-01-11 17:58:44,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-01-11 17:58:44,114 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 17:58:44,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-01-11 17:58:44,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-01-11 17:58:44,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-01-11 17:58:44,177 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:58:44,178 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-11 17:58:44,189 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current: 34861348
2020-01-11 17:58:44,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 13ms
2020-01-11 17:58:44,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 14ms
2020-01-11 17:58:44,192 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-11 17:58:44,206 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 15ms
2020-01-11 17:58:44,206 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 15ms
2020-01-11 17:58:44,213 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1578775315213 with interval 21600000
2020-01-11 17:58:44,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-01-11 17:58:44,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-01-11 17:58:44,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-01-11 17:58:44,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=639
2020-01-11 17:58:44,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-01-11 17:58:44,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x36bdc78af28,  containing 1 storage report(s), of which we sent 1. The reports had 40 total blocks and used 1 RPC(s). This took 1 msec to generate and 73 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-01-11 17:58:44,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:58:44,403 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-01-11 17:58:44,403 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-01-11 17:58:44,404 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-01-11 17:58:44,405 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-01-11 17:58:44,405 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 17:58:44,409 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-01-11 17:58:49,278 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741832_1008
2020-01-11 18:00:25,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741888_1064 src: /192.168.251.3:59520 dest: /192.168.251.4:50010
2020-01-11 18:00:25,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59520, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1107317307_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741888_1064, duration: 78675107
2020-01-11 18:00:25,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741888_1064, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:00:25,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741889_1065 src: /192.168.251.3:59524 dest: /192.168.251.4:50010
2020-01-11 18:00:25,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59524, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1107317307_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741889_1065, duration: 6394885
2020-01-11 18:00:25,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741889_1065, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:00:25,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741890_1066 src: /192.168.251.3:59528 dest: /192.168.251.4:50010
2020-01-11 18:00:25,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59528, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1107317307_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741890_1066, duration: 5716904
2020-01-11 18:00:25,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741890_1066, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:00:26,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741891_1067 src: /192.168.251.3:59532 dest: /192.168.251.4:50010
2020-01-11 18:00:26,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59532, dest: /192.168.251.4:50010, bytes: 89254, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1107317307_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741891_1067, duration: 17475434
2020-01-11 18:00:26,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741891_1067, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:00:32,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741892_1068 src: /192.168.251.4:40386 dest: /192.168.251.4:50010
2020-01-11 18:00:32,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40386, dest: /192.168.251.4:50010, bytes: 106160, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-579446958_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741892_1068, duration: 46050776
2020-01-11 18:00:32,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741892_1068, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:00:34,438 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741890_1066
2020-01-11 18:00:39,522 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741892_1068
2020-01-11 18:00:40,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741893_1069 src: /192.168.251.4:40400 dest: /192.168.251.4:50010
2020-01-11 18:00:46,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741894_1070 src: /192.168.251.4:40412 dest: /192.168.251.4:50010
2020-01-11 18:00:46,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40412, dest: /192.168.251.4:50010, bytes: 1369, op: HDFS_WRITE, cliID: DFSClient_attempt_1578761968676_0001_r_000000_0_-1253446039_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741894_1070, duration: 77019783
2020-01-11 18:00:46,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741894_1070, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:00:47,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40400, dest: /192.168.251.4:50010, bytes: 33655, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-579446958_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741893_1069, duration: 7000780685
2020-01-11 18:00:47,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741893_1069, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:00:47,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741895_1071 src: /192.168.251.4:40418 dest: /192.168.251.4:50010
2020-01-11 18:00:47,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40418, dest: /192.168.251.4:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-579446958_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741895_1071, duration: 13640605
2020-01-11 18:00:47,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741895_1071, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:00:47,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741896_1072 src: /192.168.251.4:40424 dest: /192.168.251.4:50010
2020-01-11 18:00:47,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40424, dest: /192.168.251.4:50010, bytes: 33655, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-579446958_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741896_1072, duration: 21616668
2020-01-11 18:00:47,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741896_1072, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:00:47,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741897_1073 src: /192.168.251.4:40428 dest: /192.168.251.4:50010
2020-01-11 18:00:47,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40428, dest: /192.168.251.4:50010, bytes: 106160, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-579446958_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741897_1073, duration: 25716250
2020-01-11 18:00:47,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741897_1073, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:00:53,364 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2020-01-11 18:00:53,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2020-01-11 18:00:53,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2020-01-11 18:00:53,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2020-01-11 18:00:53,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2020-01-11 18:00:53,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2020-01-11 18:00:53,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741888_1064 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741888
2020-01-11 18:00:53,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741889_1065 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741889
2020-01-11 18:00:53,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741890_1066 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741890
2020-01-11 18:00:53,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741891_1067 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741891
2020-01-11 18:00:53,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741892_1068 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741892
2020-01-11 18:00:53,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741893_1069 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741893
2020-01-11 18:00:54,556 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741896_1072
2020-01-11 18:00:55,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741898_1074 src: /192.168.251.4:40438 dest: /192.168.251.4:50010
2020-01-11 18:00:55,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40438, dest: /192.168.251.4:50010, bytes: 45514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-733261612_91, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741898_1074, duration: 32360697
2020-01-11 18:00:55,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741898_1074, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:01:04,573 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741898_1074
2020-01-11 18:01:38,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2020-01-11 18:01:38,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741895_1071 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741895
2020-01-11 18:10:44,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741899_1075 src: /192.168.251.3:59666 dest: /192.168.251.4:50010
2020-01-11 18:10:44,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59666, dest: /192.168.251.4:50010, bytes: 3164, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741899_1075, duration: 32630021
2020-01-11 18:10:44,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741899_1075, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:10:45,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741900_1076 src: /192.168.251.3:59672 dest: /192.168.251.4:50010
2020-01-11 18:10:45,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59672, dest: /192.168.251.4:50010, bytes: 32441258, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741900_1076, duration: 491476962
2020-01-11 18:10:45,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741900_1076, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:10:45,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741901_1077 src: /192.168.251.3:59676 dest: /192.168.251.4:50010
2020-01-11 18:10:46,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59676, dest: /192.168.251.4:50010, bytes: 224, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741901_1077, duration: 4201498
2020-01-11 18:10:46,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741901_1077, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:10:46,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741902_1078 src: /192.168.251.3:59680 dest: /192.168.251.4:50010
2020-01-11 18:10:46,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59680, dest: /192.168.251.4:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741902_1078, duration: 5214810
2020-01-11 18:10:46,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741902_1078, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:10:46,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741903_1079 src: /192.168.251.3:59684 dest: /192.168.251.4:50010
2020-01-11 18:10:46,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59684, dest: /192.168.251.4:50010, bytes: 235265, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741903_1079, duration: 8606371
2020-01-11 18:10:46,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741903_1079, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:10:50,051 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741899_1075
2020-01-11 18:10:52,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741904_1080 src: /192.168.251.4:40482 dest: /192.168.251.4:50010
2020-01-11 18:10:52,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40482, dest: /192.168.251.4:50010, bytes: 269125, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-163894652_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741904_1080, duration: 70922139
2020-01-11 18:10:52,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741904_1080, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:10:55,199 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741903_1079
2020-01-11 18:11:00,247 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741904_1080
2020-01-11 18:11:07,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741905_1081 src: /192.168.251.3:59706 dest: /192.168.251.4:50010
2020-01-11 18:11:07,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59706, dest: /192.168.251.4:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_attempt_1578761968676_0002_m_000000_0_1589544404_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741905_1081, duration: 72978023
2020-01-11 18:11:07,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741905_1081, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:11:07,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741906_1082 src: /192.168.251.3:59710 dest: /192.168.251.4:50010
2020-01-11 18:11:07,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59710, dest: /192.168.251.4:50010, bytes: 74, op: HDFS_WRITE, cliID: DFSClient_attempt_1578761968676_0002_m_000000_0_1589544404_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741906_1082, duration: 13797936
2020-01-11 18:11:07,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741906_1082, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:11:08,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741907_1083 src: /192.168.251.4:40492 dest: /192.168.251.4:50010
2020-01-11 18:11:08,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40492, dest: /192.168.251.4:50010, bytes: 22898, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-163894652_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741907_1083, duration: 71272695
2020-01-11 18:11:08,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741907_1083, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:11:08,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741908_1084 src: /192.168.251.4:40496 dest: /192.168.251.4:50010
2020-01-11 18:11:08,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40496, dest: /192.168.251.4:50010, bytes: 375, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-163894652_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741908_1084, duration: 20230273
2020-01-11 18:11:08,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741908_1084, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:11:08,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741909_1085 src: /192.168.251.4:40502 dest: /192.168.251.4:50010
2020-01-11 18:11:08,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40502, dest: /192.168.251.4:50010, bytes: 22898, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-163894652_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741909_1085, duration: 24955654
2020-01-11 18:11:08,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741909_1085, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:11:08,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741910_1086 src: /192.168.251.4:40506 dest: /192.168.251.4:50010
2020-01-11 18:11:08,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40506, dest: /192.168.251.4:50010, bytes: 269125, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-163894652_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741910_1086, duration: 36282513
2020-01-11 18:11:08,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741910_1086, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:11:14,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741904_1080 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741904 for deletion
2020-01-11 18:11:14,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741900_1076 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741900 for deletion
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741904_1080 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741904
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741906_1082 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741906
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741907_1083 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741907
2020-01-11 18:11:14,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741899_1075 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741899
2020-01-11 18:11:14,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741900_1076 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741900
2020-01-11 18:11:14,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741901_1077 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741901
2020-01-11 18:11:14,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741902_1078 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741902
2020-01-11 18:11:14,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741903_1079 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741903
2020-01-11 18:11:15,255 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741908_1084
2020-01-11 18:11:16,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741911_1087 src: /192.168.251.4:40514 dest: /192.168.251.4:50010
2020-01-11 18:11:16,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40514, dest: /192.168.251.4:50010, bytes: 26346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-798720222_150, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741911_1087, duration: 5970959
2020-01-11 18:11:16,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741911_1087, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:11:16,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741912_1088 src: /192.168.251.3:59720 dest: /192.168.251.4:50010
2020-01-11 18:11:16,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59720, dest: /192.168.251.4:50010, bytes: 40732, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-989412670_93, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741912_1088, duration: 25117373
2020-01-11 18:11:16,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741912_1088, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:11:25,260 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741912_1088
2020-01-11 18:11:32,822 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741908_1084 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741908 for deletion
2020-01-11 18:11:32,823 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741908_1084 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741908
2020-01-11 18:14:09,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741913_1089 src: /192.168.251.3:59736 dest: /192.168.251.4:50010
2020-01-11 18:14:09,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59736, dest: /192.168.251.4:50010, bytes: 3160, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741913_1089, duration: 35547154
2020-01-11 18:14:09,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741913_1089, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:14:09,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741914_1090 src: /192.168.251.3:59742 dest: /192.168.251.4:50010
2020-01-11 18:14:10,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59742, dest: /192.168.251.4:50010, bytes: 32441258, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741914_1090, duration: 399715256
2020-01-11 18:14:10,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741914_1090, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:14:10,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741915_1091 src: /192.168.251.3:59746 dest: /192.168.251.4:50010
2020-01-11 18:14:10,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59746, dest: /192.168.251.4:50010, bytes: 224, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741915_1091, duration: 13401243
2020-01-11 18:14:10,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741915_1091, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:14:10,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741916_1092 src: /192.168.251.3:59750 dest: /192.168.251.4:50010
2020-01-11 18:14:10,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59750, dest: /192.168.251.4:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741916_1092, duration: 6028368
2020-01-11 18:14:10,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741916_1092, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:14:10,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741917_1093 src: /192.168.251.3:59754 dest: /192.168.251.4:50010
2020-01-11 18:14:10,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59754, dest: /192.168.251.4:50010, bytes: 235175, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-752514962_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741917_1093, duration: 9024760
2020-01-11 18:14:10,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741917_1093, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:14:16,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741918_1094 src: /192.168.251.4:40542 dest: /192.168.251.4:50010
2020-01-11 18:14:16,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40542, dest: /192.168.251.4:50010, bytes: 269035, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1929927233_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741918_1094, duration: 32238158
2020-01-11 18:14:16,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741918_1094, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:14:22,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741919_1095 src: /192.168.251.3:59776 dest: /192.168.251.4:50010
2020-01-11 18:14:22,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59776, dest: /192.168.251.4:50010, bytes: 1725, op: HDFS_WRITE, cliID: DFSClient_attempt_1578761968676_0003_m_000000_0_-1303789206_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741919_1095, duration: 67066865
2020-01-11 18:14:22,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741919_1095, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:14:22,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741920_1096 src: /192.168.251.3:59780 dest: /192.168.251.4:50010
2020-01-11 18:14:22,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59780, dest: /192.168.251.4:50010, bytes: 74, op: HDFS_WRITE, cliID: DFSClient_attempt_1578761968676_0003_m_000000_0_-1303789206_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741920_1096, duration: 6243222
2020-01-11 18:14:22,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741920_1096, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:14:23,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741921_1097 src: /192.168.251.4:40550 dest: /192.168.251.4:50010
2020-01-11 18:14:23,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40550, dest: /192.168.251.4:50010, bytes: 22894, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1929927233_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741921_1097, duration: 125837760
2020-01-11 18:14:23,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741921_1097, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:14:23,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741922_1098 src: /192.168.251.4:40554 dest: /192.168.251.4:50010
2020-01-11 18:14:23,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40554, dest: /192.168.251.4:50010, bytes: 374, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1929927233_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741922_1098, duration: 12427459
2020-01-11 18:14:23,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741922_1098, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:14:23,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741923_1099 src: /192.168.251.4:40560 dest: /192.168.251.4:50010
2020-01-11 18:14:23,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40560, dest: /192.168.251.4:50010, bytes: 22894, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1929927233_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741923_1099, duration: 6264090
2020-01-11 18:14:23,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741923_1099, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:14:23,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741924_1100 src: /192.168.251.4:40564 dest: /192.168.251.4:50010
2020-01-11 18:14:23,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40564, dest: /192.168.251.4:50010, bytes: 269035, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1929927233_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741924_1100, duration: 9997790
2020-01-11 18:14:23,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741924_1100, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:14:27,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2020-01-11 18:14:27,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2020-01-11 18:14:27,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2020-01-11 18:14:27,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2020-01-11 18:14:27,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2020-01-11 18:14:27,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741918_1094 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741918 for deletion
2020-01-11 18:14:27,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741921_1097 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741921
2020-01-11 18:14:27,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741914_1090 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741914
2020-01-11 18:14:27,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741915_1091 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741915
2020-01-11 18:14:27,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741916_1092 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741916
2020-01-11 18:14:27,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741917_1093 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741917
2020-01-11 18:14:27,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741918_1094 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741918
2020-01-11 18:14:30,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741925_1101 src: /192.168.251.3:59794 dest: /192.168.251.4:50010
2020-01-11 18:14:30,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:59794, dest: /192.168.251.4:50010, bytes: 40867, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1521467091_129, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741925_1101, duration: 3897036
2020-01-11 18:14:30,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741925_1101, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:14:31,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741926_1102 src: /192.168.251.4:40572 dest: /192.168.251.4:50010
2020-01-11 18:14:31,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:40572, dest: /192.168.251.4:50010, bytes: 25553, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1039773185_190, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741926_1102, duration: 5020055
2020-01-11 18:14:31,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741926_1102, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:14:33,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2020-01-11 18:14:33,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741922_1098 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741922 for deletion
2020-01-11 18:14:33,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2020-01-11 18:14:33,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741920_1096 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741920
2020-01-11 18:14:33,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741922_1098 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741922
2020-01-11 18:14:33,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741913_1089 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741913
2020-01-11 18:33:20,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-01-11 18:33:20,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-11 18:33:21,393 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 18:33:22,308 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-01-11 18:33:22,589 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-01-11 18:33:22,590 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-01-11 18:33:22,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-01-11 18:33:22,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-01-11 18:33:22,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-01-11 18:33:22,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-01-11 18:33:22,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-01-11 18:33:22,794 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-01-11 18:33:22,797 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-01-11 18:33:22,806 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-01-11 18:33:22,808 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-01-11 18:33:22,808 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-01-11 18:33:22,808 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-01-11 18:33:22,827 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-01-11 18:33:22,830 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-01-11 18:33:22,830 INFO org.mortbay.log: jetty-6.1.26
2020-01-11 18:33:23,123 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-01-11 18:33:23,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-01-11 18:33:23,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-01-11 18:33:23,721 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-01-11 18:33:23,743 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-01-11 18:33:23,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-01-11 18:33:23,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-01-11 18:33:23,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-01-11 18:33:23,886 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 18:33:23,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-01-11 18:33:23,923 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-01-11 18:33:23,934 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-01-11 18:33:24,536 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2901@um2
2020-01-11 18:33:24,633 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-01-11 18:33:24,633 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-01-11 18:33:24,635 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-01-11 18:33:24,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-01-11 18:33:24,654 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-11 18:33:24,683 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-01-11 18:33:24,683 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-01-11 18:33:24,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-01-11 18:33:24,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 18:33:24,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-11 18:33:24,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 39ms
2020-01-11 18:33:24,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 40ms
2020-01-11 18:33:24,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-11 18:33:24,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 16ms
2020-01-11 18:33:24,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 17ms
2020-01-11 18:33:24,804 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1578776304804 with interval 21600000
2020-01-11 18:33:24,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-01-11 18:33:24,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-01-11 18:33:24,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-01-11 18:33:25,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=978
2020-01-11 18:33:25,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-01-11 18:33:25,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd47775f75a,  containing 1 storage report(s), of which we sent 1. The reports had 54 total blocks and used 1 RPC(s). This took 2 msec to generate and 122 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-01-11 18:33:25,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 18:33:25,196 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-01-11 18:33:25,196 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-01-11 18:33:25,198 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-01-11 18:33:25,198 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-01-11 18:33:25,199 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-11 18:33:25,206 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-01-11 18:33:29,871 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741923_1099
2020-01-11 18:33:29,886 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741879_1055
2020-01-11 18:36:59,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741927_1103 src: /192.168.251.4:58420 dest: /192.168.251.4:50010
2020-01-11 18:36:59,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58420, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2087868275_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741927_1103, duration: 133728037
2020-01-11 18:36:59,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741927_1103, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:00,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741928_1104 src: /192.168.251.4:58424 dest: /192.168.251.4:50010
2020-01-11 18:37:00,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58424, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2087868275_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741928_1104, duration: 17592759
2020-01-11 18:37:00,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741928_1104, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:00,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741929_1105 src: /192.168.251.4:58428 dest: /192.168.251.4:50010
2020-01-11 18:37:00,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58428, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2087868275_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741929_1105, duration: 11300473
2020-01-11 18:37:00,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741929_1105, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:00,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741930_1106 src: /192.168.251.4:58432 dest: /192.168.251.4:50010
2020-01-11 18:37:00,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58432, dest: /192.168.251.4:50010, bytes: 89339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2087868275_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741930_1106, duration: 16991077
2020-01-11 18:37:00,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741930_1106, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:05,117 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741927_1103
2020-01-11 18:37:10,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741931_1107 src: /192.168.251.4:58454 dest: /192.168.251.4:50010
2020-01-11 18:37:10,133 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741929_1105
2020-01-11 18:37:10,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58454, dest: /192.168.251.4:50010, bytes: 106269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-247221388_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741931_1107, duration: 54920772
2020-01-11 18:37:10,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741931_1107, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:15,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741932_1108 src: /192.168.251.4:58470 dest: /192.168.251.4:50010
2020-01-11 18:37:20,145 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741931_1107
2020-01-11 18:37:24,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741933_1109 src: /192.168.251.3:34084 dest: /192.168.251.4:50010
2020-01-11 18:37:24,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34084, dest: /192.168.251.4:50010, bytes: 2925, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0001_r_000000_0_620665789_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741933_1109, duration: 66916342
2020-01-11 18:37:24,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741933_1109, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:37:24,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58470, dest: /192.168.251.4:50010, bytes: 33659, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-247221388_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741932_1108, duration: 9214822239
2020-01-11 18:37:24,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741932_1108, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:24,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741934_1110 src: /192.168.251.4:58478 dest: /192.168.251.4:50010
2020-01-11 18:37:25,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58478, dest: /192.168.251.4:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-247221388_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741934_1110, duration: 10397962
2020-01-11 18:37:25,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741934_1110, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:25,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741935_1111 src: /192.168.251.4:58484 dest: /192.168.251.4:50010
2020-01-11 18:37:25,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58484, dest: /192.168.251.4:50010, bytes: 33659, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-247221388_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741935_1111, duration: 6396061
2020-01-11 18:37:25,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741935_1111, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:25,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741936_1112 src: /192.168.251.4:58488 dest: /192.168.251.4:50010
2020-01-11 18:37:25,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58488, dest: /192.168.251.4:50010, bytes: 106269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-247221388_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741936_1112, duration: 11098299
2020-01-11 18:37:25,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741936_1112, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:28,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2020-01-11 18:37:28,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741928_1104 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741928 for deletion
2020-01-11 18:37:28,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741929_1105 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741929 for deletion
2020-01-11 18:37:28,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741930_1106 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741930 for deletion
2020-01-11 18:37:28,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2020-01-11 18:37:28,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2020-01-11 18:37:28,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741927_1103 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741927
2020-01-11 18:37:28,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741928_1104 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741928
2020-01-11 18:37:28,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741929_1105 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741929
2020-01-11 18:37:28,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741930_1106 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741930
2020-01-11 18:37:28,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741931_1107 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741931
2020-01-11 18:37:28,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741932_1108 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741932
2020-01-11 18:37:30,158 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741934_1110
2020-01-11 18:37:32,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741937_1113 src: /192.168.251.3:34090 dest: /192.168.251.4:50010
2020-01-11 18:37:32,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34090, dest: /192.168.251.4:50010, bytes: 8880, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1892164839_79, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741937_1113, duration: 49155943
2020-01-11 18:37:32,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741937_1113, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:37:32,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741938_1114 src: /192.168.251.4:58498 dest: /192.168.251.4:50010
2020-01-11 18:37:32,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58498, dest: /192.168.251.4:50010, bytes: 37476, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1727373303_79, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741938_1114, duration: 116742671
2020-01-11 18:37:32,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741938_1114, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:35,172 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741936_1112
2020-01-11 18:37:40,192 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741938_1114
2020-01-11 18:37:41,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741939_1115 src: /192.168.251.4:58506 dest: /192.168.251.4:50010
2020-01-11 18:37:41,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58506, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_91721303_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741939_1115, duration: 109047003
2020-01-11 18:37:41,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741939_1115, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:41,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741940_1116 src: /192.168.251.4:58510 dest: /192.168.251.4:50010
2020-01-11 18:37:41,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58510, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_91721303_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741940_1116, duration: 18730806
2020-01-11 18:37:41,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741940_1116, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:41,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741941_1117 src: /192.168.251.4:58514 dest: /192.168.251.4:50010
2020-01-11 18:37:41,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58514, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_91721303_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741941_1117, duration: 5927722
2020-01-11 18:37:41,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741941_1117, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:41,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741942_1118 src: /192.168.251.4:58518 dest: /192.168.251.4:50010
2020-01-11 18:37:41,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58518, dest: /192.168.251.4:50010, bytes: 89339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_91721303_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741942_1118, duration: 15484107
2020-01-11 18:37:41,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741942_1118, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:47,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741943_1119 src: /192.168.251.4:58542 dest: /192.168.251.4:50010
2020-01-11 18:37:47,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58542, dest: /192.168.251.4:50010, bytes: 106269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_989320367_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741943_1119, duration: 28765458
2020-01-11 18:37:47,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741943_1119, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:50,207 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741939_1115
2020-01-11 18:37:53,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741944_1120 src: /192.168.251.4:58550 dest: /192.168.251.4:50010
2020-01-11 18:37:55,214 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741943_1119
2020-01-11 18:37:59,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741945_1121 src: /192.168.251.3:34116 dest: /192.168.251.4:50010
2020-01-11 18:37:59,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34116, dest: /192.168.251.4:50010, bytes: 2925, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0002_r_000000_0_-317716138_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741945_1121, duration: 40061407
2020-01-11 18:37:59,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741945_1121, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:37:59,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58550, dest: /192.168.251.4:50010, bytes: 33646, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_989320367_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741944_1120, duration: 6493409310
2020-01-11 18:37:59,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741944_1120, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:37:59,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741946_1122 src: /192.168.251.4:58558 dest: /192.168.251.4:50010
2020-01-11 18:38:00,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58558, dest: /192.168.251.4:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_989320367_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741946_1122, duration: 12657089
2020-01-11 18:38:00,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741946_1122, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:00,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741947_1123 src: /192.168.251.4:58564 dest: /192.168.251.4:50010
2020-01-11 18:38:00,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58564, dest: /192.168.251.4:50010, bytes: 33646, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_989320367_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741947_1123, duration: 30794017
2020-01-11 18:38:00,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741947_1123, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:00,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741948_1124 src: /192.168.251.4:58568 dest: /192.168.251.4:50010
2020-01-11 18:38:00,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58568, dest: /192.168.251.4:50010, bytes: 106269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_989320367_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741948_1124, duration: 36021875
2020-01-11 18:38:00,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741948_1124, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741939_1115 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741939 for deletion
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741940_1116 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741940 for deletion
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741939_1115 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741939
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741941_1117 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741941 for deletion
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741940_1116 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741940
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741942_1118 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741942 for deletion
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741941_1117 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741941
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741943_1119 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741943 for deletion
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741942_1118 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741942
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741944_1120 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741944 for deletion
2020-01-11 18:38:04,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741943_1119 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741943
2020-01-11 18:38:04,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741944_1120 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741944
2020-01-11 18:38:05,223 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741947_1123
2020-01-11 18:38:07,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741949_1125 src: /192.168.251.3:34122 dest: /192.168.251.4:50010
2020-01-11 18:38:07,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34122, dest: /192.168.251.4:50010, bytes: 14820, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-254598213_79, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741949_1125, duration: 7334202
2020-01-11 18:38:07,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741949_1125, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:38:07,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741950_1126 src: /192.168.251.4:58576 dest: /192.168.251.4:50010
2020-01-11 18:38:07,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58576, dest: /192.168.251.4:50010, bytes: 35599, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_197758367_79, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741950_1126, duration: 6850416
2020-01-11 18:38:07,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741950_1126, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:15,230 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741949_1125
2020-01-11 18:38:15,231 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741950_1126
2020-01-11 18:38:31,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741951_1127 src: /192.168.251.4:58588 dest: /192.168.251.4:50010
2020-01-11 18:38:31,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58588, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995069259_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741951_1127, duration: 43773401
2020-01-11 18:38:31,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741951_1127, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:31,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741952_1128 src: /192.168.251.4:58592 dest: /192.168.251.4:50010
2020-01-11 18:38:31,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58592, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995069259_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741952_1128, duration: 12520986
2020-01-11 18:38:31,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741952_1128, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:31,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741953_1129 src: /192.168.251.4:58596 dest: /192.168.251.4:50010
2020-01-11 18:38:31,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58596, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995069259_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741953_1129, duration: 8413663
2020-01-11 18:38:31,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741953_1129, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:31,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741954_1130 src: /192.168.251.4:58600 dest: /192.168.251.4:50010
2020-01-11 18:38:31,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58600, dest: /192.168.251.4:50010, bytes: 89330, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995069259_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741954_1130, duration: 7403481
2020-01-11 18:38:31,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741954_1130, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:39,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741955_1131 src: /192.168.251.3:34142 dest: /192.168.251.4:50010
2020-01-11 18:38:39,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34142, dest: /192.168.251.4:50010, bytes: 106126, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1714505138_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741955_1131, duration: 37667710
2020-01-11 18:38:39,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741955_1131, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:38:40,255 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741952_1128
2020-01-11 18:38:40,256 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741954_1130
2020-01-11 18:38:45,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741956_1132 src: /192.168.251.3:34156 dest: /192.168.251.4:50010
2020-01-11 18:38:45,267 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741955_1131
2020-01-11 18:38:50,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741957_1133 src: /192.168.251.4:58624 dest: /192.168.251.4:50010
2020-01-11 18:38:50,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58624, dest: /192.168.251.4:50010, bytes: 2925, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0003_r_000000_0_833157326_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741957_1133, duration: 33011092
2020-01-11 18:38:50,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741957_1133, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:51,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34156, dest: /192.168.251.4:50010, bytes: 33650, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1714505138_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741956_1132, duration: 6139528380
2020-01-11 18:38:51,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741956_1132, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:38:51,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741958_1134 src: /192.168.251.3:34164 dest: /192.168.251.4:50010
2020-01-11 18:38:51,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34164, dest: /192.168.251.4:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1714505138_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741958_1134, duration: 4351660
2020-01-11 18:38:51,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741958_1134, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:38:51,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741959_1135 src: /192.168.251.3:34170 dest: /192.168.251.4:50010
2020-01-11 18:38:51,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34170, dest: /192.168.251.4:50010, bytes: 33650, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1714505138_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741959_1135, duration: 6531170
2020-01-11 18:38:51,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741959_1135, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:38:51,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741960_1136 src: /192.168.251.3:34174 dest: /192.168.251.4:50010
2020-01-11 18:38:51,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34174, dest: /192.168.251.4:50010, bytes: 106126, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1714505138_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741960_1136, duration: 7462830
2020-01-11 18:38:51,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741960_1136, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741952_1128 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741952 for deletion
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741953_1129 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741953 for deletion
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741954_1130 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741954 for deletion
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741955_1131 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741955 for deletion
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741956_1132 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741956 for deletion
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741951_1127 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741951 for deletion
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741952_1128 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741952
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741953_1129 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741953
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741954_1130 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741954
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741955_1131 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741955
2020-01-11 18:38:55,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741956_1132 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741956
2020-01-11 18:38:55,285 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741951_1127 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741951
2020-01-11 18:38:58,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741961_1137 src: /192.168.251.4:58632 dest: /192.168.251.4:50010
2020-01-11 18:38:58,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58632, dest: /192.168.251.4:50010, bytes: 6129, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403195738_79, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741961_1137, duration: 14547556
2020-01-11 18:38:58,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741961_1137, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:38:58,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741962_1138 src: /192.168.251.3:34180 dest: /192.168.251.4:50010
2020-01-11 18:38:58,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34180, dest: /192.168.251.4:50010, bytes: 41967, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_936077017_79, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741962_1138, duration: 6135342
2020-01-11 18:38:58,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741962_1138, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:39:00,277 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741958_1134
2020-01-11 18:39:00,277 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741957_1133
2020-01-11 18:39:05,291 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741961_1137
2020-01-11 18:39:05,292 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741962_1138
2020-01-11 18:39:40,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741963_1139 src: /192.168.251.4:58642 dest: /192.168.251.4:50010
2020-01-11 18:39:40,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58642, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1604795033_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741963_1139, duration: 57311673
2020-01-11 18:39:40,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741963_1139, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:39:40,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741964_1140 src: /192.168.251.4:58646 dest: /192.168.251.4:50010
2020-01-11 18:39:40,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58646, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1604795033_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741964_1140, duration: 6687079
2020-01-11 18:39:40,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741964_1140, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:39:40,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741965_1141 src: /192.168.251.4:58650 dest: /192.168.251.4:50010
2020-01-11 18:39:40,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58650, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1604795033_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741965_1141, duration: 9442228
2020-01-11 18:39:40,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741965_1141, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:39:40,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741966_1142 src: /192.168.251.4:58654 dest: /192.168.251.4:50010
2020-01-11 18:39:40,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58654, dest: /192.168.251.4:50010, bytes: 89327, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1604795033_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741966_1142, duration: 11649921
2020-01-11 18:39:40,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741966_1142, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:39:45,328 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741963_1139
2020-01-11 18:39:45,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741967_1143 src: /192.168.251.3:34202 dest: /192.168.251.4:50010
2020-01-11 18:39:46,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34202, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1717475582_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741967_1143, duration: 138088267
2020-01-11 18:39:46,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741967_1143, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:39:46,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741968_1144 src: /192.168.251.3:34206 dest: /192.168.251.4:50010
2020-01-11 18:39:46,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34206, dest: /192.168.251.4:50010, bytes: 111, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1717475582_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741968_1144, duration: 9435247
2020-01-11 18:39:46,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741968_1144, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:39:46,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741969_1145 src: /192.168.251.3:34210 dest: /192.168.251.4:50010
2020-01-11 18:39:46,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34210, dest: /192.168.251.4:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1717475582_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741969_1145, duration: 4144978
2020-01-11 18:39:46,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741969_1145, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:39:47,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741970_1146 src: /192.168.251.3:34214 dest: /192.168.251.4:50010
2020-01-11 18:39:47,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34214, dest: /192.168.251.4:50010, bytes: 89360, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1717475582_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741970_1146, duration: 26415403
2020-01-11 18:39:47,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741970_1146, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:39:50,340 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741966_1142
2020-01-11 18:39:50,340 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741965_1141
2020-01-11 18:39:54,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741971_1147 src: /192.168.251.3:34232 dest: /192.168.251.4:50010
2020-01-11 18:39:54,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34232, dest: /192.168.251.4:50010, bytes: 106124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1351112490_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741971_1147, duration: 91114664
2020-01-11 18:39:54,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741971_1147, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:39:55,346 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741969_1145
2020-01-11 18:39:55,348 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741967_1143
2020-01-11 18:40:00,359 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741971_1147
2020-01-11 18:40:00,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741972_1148 src: /192.168.251.3:34250 dest: /192.168.251.4:50010
2020-01-11 18:40:00,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34250, dest: /192.168.251.4:50010, bytes: 106156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_66951179_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741972_1148, duration: 71393517
2020-01-11 18:40:00,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741972_1148, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:03,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741973_1149 src: /192.168.251.3:34262 dest: /192.168.251.4:50010
2020-01-11 18:40:09,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741974_1150 src: /192.168.251.3:34276 dest: /192.168.251.4:50010
2020-01-11 18:40:10,370 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741972_1148
2020-01-11 18:40:12,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741975_1151 src: /192.168.251.3:34286 dest: /192.168.251.4:50010
2020-01-11 18:40:12,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34286, dest: /192.168.251.4:50010, bytes: 2925, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0004_r_000000_0_-1475751708_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741975_1151, duration: 112956932
2020-01-11 18:40:12,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741975_1151, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:12,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34262, dest: /192.168.251.4:50010, bytes: 33650, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1351112490_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741973_1149, duration: 9846054784
2020-01-11 18:40:12,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741973_1149, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:12,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741976_1152 src: /192.168.251.3:34294 dest: /192.168.251.4:50010
2020-01-11 18:40:12,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34294, dest: /192.168.251.4:50010, bytes: 342, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1351112490_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741976_1152, duration: 4457148
2020-01-11 18:40:12,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741976_1152, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:13,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741977_1153 src: /192.168.251.3:34300 dest: /192.168.251.4:50010
2020-01-11 18:40:13,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34300, dest: /192.168.251.4:50010, bytes: 33650, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1351112490_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741977_1153, duration: 3614423
2020-01-11 18:40:13,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741977_1153, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:13,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741978_1154 src: /192.168.251.3:34304 dest: /192.168.251.4:50010
2020-01-11 18:40:13,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34304, dest: /192.168.251.4:50010, bytes: 106124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1351112490_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741978_1154, duration: 16114403
2020-01-11 18:40:13,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741978_1154, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:16,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741971_1147 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741971 for deletion
2020-01-11 18:40:16,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741973_1149 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741973 for deletion
2020-01-11 18:40:16,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2020-01-11 18:40:16,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741964_1140 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741964 for deletion
2020-01-11 18:40:16,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2020-01-11 18:40:16,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741966_1142 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741966 for deletion
2020-01-11 18:40:16,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741971_1147 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741971
2020-01-11 18:40:16,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741973_1149 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741973
2020-01-11 18:40:16,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741963_1139 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741963
2020-01-11 18:40:16,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741964_1140 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741964
2020-01-11 18:40:16,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741965_1141 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741965
2020-01-11 18:40:16,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741966_1142 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741966
2020-01-11 18:40:17,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741979_1155 src: /192.168.251.3:34322 dest: /192.168.251.4:50010
2020-01-11 18:40:17,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34322, dest: /192.168.251.4:50010, bytes: 2925, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0005_r_000000_0_-1674427437_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741979_1155, duration: 39330868
2020-01-11 18:40:17,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741979_1155, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:18,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34276, dest: /192.168.251.4:50010, bytes: 33640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_66951179_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741974_1150, duration: 8452360669
2020-01-11 18:40:18,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741974_1150, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:18,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741980_1156 src: /192.168.251.3:34328 dest: /192.168.251.4:50010
2020-01-11 18:40:18,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34328, dest: /192.168.251.4:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_66951179_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741980_1156, duration: 4002441
2020-01-11 18:40:18,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741980_1156, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:18,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741981_1157 src: /192.168.251.3:34334 dest: /192.168.251.4:50010
2020-01-11 18:40:18,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34334, dest: /192.168.251.4:50010, bytes: 33640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_66951179_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741981_1157, duration: 7672467
2020-01-11 18:40:18,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741981_1157, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:18,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741982_1158 src: /192.168.251.3:34338 dest: /192.168.251.4:50010
2020-01-11 18:40:18,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34338, dest: /192.168.251.4:50010, bytes: 106156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_66951179_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741982_1158, duration: 5331978
2020-01-11 18:40:18,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741982_1158, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:19,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741958_1134 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741958 for deletion
2020-01-11 18:40:19,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741976_1152 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741976 for deletion
2020-01-11 18:40:19,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741946_1122 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741946 for deletion
2020-01-11 18:40:19,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741958_1134 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741958
2020-01-11 18:40:19,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2020-01-11 18:40:19,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741976_1152 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741976
2020-01-11 18:40:19,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741946_1122 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741946
2020-01-11 18:40:19,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741934_1110 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741934
2020-01-11 18:40:20,393 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741978_1154
2020-01-11 18:40:20,393 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741975_1151
2020-01-11 18:40:20,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741983_1159 src: /192.168.251.3:34344 dest: /192.168.251.4:50010
2020-01-11 18:40:20,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34344, dest: /192.168.251.4:50010, bytes: 50466, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1015917336_79, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741983_1159, duration: 4761689
2020-01-11 18:40:20,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741983_1159, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741968_1144 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741968 for deletion
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741969_1145 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741969 for deletion
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741970_1146 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741970 for deletion
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741972_1148 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741972 for deletion
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741968_1144 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741968
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741974_1150 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741974 for deletion
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741967_1143 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741967 for deletion
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741969_1145 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741969
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741970_1146 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741970
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741972_1148 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741972
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741974_1150 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741974
2020-01-11 18:40:22,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741967_1143 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741967
2020-01-11 18:40:25,398 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741981_1157
2020-01-11 18:40:25,399 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741980_1156
2020-01-11 18:40:25,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741984_1160 src: /192.168.251.3:34350 dest: /192.168.251.4:50010
2020-01-11 18:40:25,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34350, dest: /192.168.251.4:50010, bytes: 48735, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2064599573_216, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741984_1160, duration: 9416397
2020-01-11 18:40:25,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741984_1160, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:40:30,405 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741983_1159
2020-01-11 18:40:35,412 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741984_1160
2020-01-11 18:41:25,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741980_1156 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741980 for deletion
2020-01-11 18:41:25,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741980_1156 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741980
2020-01-11 18:45:17,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741985_1161 src: /192.168.251.3:34380 dest: /192.168.251.4:50010
2020-01-11 18:45:17,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34380, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741985_1161, duration: 44367433
2020-01-11 18:45:17,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741985_1161, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741986_1162 src: /192.168.251.3:34384 dest: /192.168.251.4:50010
2020-01-11 18:45:17,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34384, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741986_1162, duration: 3175029
2020-01-11 18:45:17,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741986_1162, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741987_1163 src: /192.168.251.3:34388 dest: /192.168.251.4:50010
2020-01-11 18:45:17,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34388, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741987_1163, duration: 2938429
2020-01-11 18:45:17,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741987_1163, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741988_1164 src: /192.168.251.3:34392 dest: /192.168.251.4:50010
2020-01-11 18:45:17,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34392, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741988_1164, duration: 8266454
2020-01-11 18:45:17,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741988_1164, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741989_1165 src: /192.168.251.3:34396 dest: /192.168.251.4:50010
2020-01-11 18:45:17,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34396, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741989_1165, duration: 6980402
2020-01-11 18:45:17,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741989_1165, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741990_1166 src: /192.168.251.3:34400 dest: /192.168.251.4:50010
2020-01-11 18:45:17,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34400, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741990_1166, duration: 3981120
2020-01-11 18:45:17,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741990_1166, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741991_1167 src: /192.168.251.3:34404 dest: /192.168.251.4:50010
2020-01-11 18:45:17,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34404, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741991_1167, duration: 2889653
2020-01-11 18:45:17,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741991_1167, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741992_1168 src: /192.168.251.3:34408 dest: /192.168.251.4:50010
2020-01-11 18:45:17,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34408, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741992_1168, duration: 7489442
2020-01-11 18:45:17,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741992_1168, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741993_1169 src: /192.168.251.3:34412 dest: /192.168.251.4:50010
2020-01-11 18:45:17,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34412, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741993_1169, duration: 5416640
2020-01-11 18:45:17,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741993_1169, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:17,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741994_1170 src: /192.168.251.3:34416 dest: /192.168.251.4:50010
2020-01-11 18:45:17,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34416, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741994_1170, duration: 5501906
2020-01-11 18:45:17,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741994_1170, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:18,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741995_1171 src: /192.168.251.3:34422 dest: /192.168.251.4:50010
2020-01-11 18:45:18,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34422, dest: /192.168.251.4:50010, bytes: 1514166, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741995_1171, duration: 31634733
2020-01-11 18:45:18,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741995_1171, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:18,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741996_1172 src: /192.168.251.3:34426 dest: /192.168.251.4:50010
2020-01-11 18:45:18,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34426, dest: /192.168.251.4:50010, bytes: 1177, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741996_1172, duration: 4114125
2020-01-11 18:45:18,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741996_1172, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:18,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741997_1173 src: /192.168.251.3:34430 dest: /192.168.251.4:50010
2020-01-11 18:45:18,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34430, dest: /192.168.251.4:50010, bytes: 135, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741997_1173, duration: 2424102
2020-01-11 18:45:18,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741997_1173, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:19,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741998_1174 src: /192.168.251.3:34434 dest: /192.168.251.4:50010
2020-01-11 18:45:19,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34434, dest: /192.168.251.4:50010, bytes: 89499, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1802138384_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741998_1174, duration: 21782997
2020-01-11 18:45:19,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741998_1174, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:45:25,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073741999_1175 src: /192.168.251.4:58698 dest: /192.168.251.4:50010
2020-01-11 18:45:25,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58698, dest: /192.168.251.4:50010, bytes: 106453, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655596264_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073741999_1175, duration: 45035488
2020-01-11 18:45:25,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073741999_1175, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:45:25,520 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741989_1165
2020-01-11 18:45:30,524 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741999_1175
2020-01-11 18:45:40,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742000_1176 src: /192.168.251.4:58736 dest: /192.168.251.4:50010
2020-01-11 18:45:40,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742001_1177 src: /192.168.251.4:58738 dest: /192.168.251.4:50010
2020-01-11 18:45:41,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742002_1178 src: /192.168.251.4:58744 dest: /192.168.251.4:50010
2020-01-11 18:45:41,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742003_1179 src: /192.168.251.4:58748 dest: /192.168.251.4:50010
2020-01-11 18:45:46,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58738, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000002_0_-1983943612_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742001_1177, duration: 5238194443
2020-01-11 18:45:46,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742001_1177, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:45:47,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742004_1180 src: /192.168.251.4:58756 dest: /192.168.251.4:50010
2020-01-11 18:45:47,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58748, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000001_0_112256474_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742003_1179, duration: 5748484151
2020-01-11 18:45:47,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742003_1179, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:45:48,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58736, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000000_0_1094439944_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742000_1176, duration: 7609858359
2020-01-11 18:45:48,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742000_1176, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:45:48,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58744, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000003_0_-424671937_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742002_1178, duration: 7145083587
2020-01-11 18:45:48,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742002_1178, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:45:59,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742005_1181 src: /192.168.251.4:58786 dest: /192.168.251.4:50010
2020-01-11 18:46:02,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742006_1182 src: /192.168.251.4:58794 dest: /192.168.251.4:50010
2020-01-11 18:46:03,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58786, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000004_0_1110727211_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742005_1181, duration: 3367162637
2020-01-11 18:46:03,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742005_1181, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:05,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742007_1183 src: /192.168.251.4:58808 dest: /192.168.251.4:50010
2020-01-11 18:46:05,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742008_1184 src: /192.168.251.4:58810 dest: /192.168.251.4:50010
2020-01-11 18:46:06,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58794, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000005_0_1448498998_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742006_1182, duration: 3499997520
2020-01-11 18:46:06,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742006_1182, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:09,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58810, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000007_0_-646122788_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742008_1184, duration: 3728085259
2020-01-11 18:46:09,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742008_1184, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:09,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58808, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000006_0_-2090965212_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742007_1183, duration: 4272257296
2020-01-11 18:46:09,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742007_1183, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:16,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742009_1185 src: /192.168.251.4:58840 dest: /192.168.251.4:50010
2020-01-11 18:46:18,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742010_1186 src: /192.168.251.4:58848 dest: /192.168.251.4:50010
2020-01-11 18:46:18,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58840, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000008_0_358762584_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742009_1185, duration: 2022158099
2020-01-11 18:46:18,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742009_1185, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:19,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58848, dest: /192.168.251.4:50010, bytes: 104857600, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_m_000009_0_-1320158704_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742010_1186, duration: 1759290242
2020-01-11 18:46:19,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742010_1186, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:20,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742011_1187 src: /192.168.251.4:58862 dest: /192.168.251.4:50010
2020-01-11 18:46:21,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58862, dest: /192.168.251.4:50010, bytes: 78, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0006_r_000000_0_755857937_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742011_1187, duration: 76873946
2020-01-11 18:46:21,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742011_1187, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:21,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58756, dest: /192.168.251.4:50010, bytes: 92880, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655596264_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742004_1180, duration: 34181574769
2020-01-11 18:46:21,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742004_1180, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:21,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742012_1188 src: /192.168.251.4:58870 dest: /192.168.251.4:50010
2020-01-11 18:46:21,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58870, dest: /192.168.251.4:50010, bytes: 389, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655596264_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742012_1188, duration: 5789268
2020-01-11 18:46:21,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742012_1188, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:21,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742013_1189 src: /192.168.251.4:58876 dest: /192.168.251.4:50010
2020-01-11 18:46:21,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58876, dest: /192.168.251.4:50010, bytes: 92880, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655596264_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742013_1189, duration: 4462972
2020-01-11 18:46:21,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742013_1189, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:21,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742014_1190 src: /192.168.251.4:58880 dest: /192.168.251.4:50010
2020-01-11 18:46:21,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58880, dest: /192.168.251.4:50010, bytes: 106453, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655596264_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742014_1190, duration: 8599373
2020-01-11 18:46:21,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742014_1190, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:28,627 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742004_1180 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742004 for deletion
2020-01-11 18:46:28,629 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741995_1171 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741995 for deletion
2020-01-11 18:46:28,629 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741996_1172 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741996 for deletion
2020-01-11 18:46:28,629 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741997_1173 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741997 for deletion
2020-01-11 18:46:28,629 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741998_1174 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741998 for deletion
2020-01-11 18:46:28,629 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741999_1175 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741999 for deletion
2020-01-11 18:46:28,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742004_1180 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742004
2020-01-11 18:46:28,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741995_1171 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741995
2020-01-11 18:46:28,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741996_1172 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741996
2020-01-11 18:46:28,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741997_1173 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741997
2020-01-11 18:46:28,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741998_1174 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741998
2020-01-11 18:46:28,631 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741999_1175 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741999
2020-01-11 18:46:29,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742015_1191 src: /192.168.251.4:58888 dest: /192.168.251.4:50010
2020-01-11 18:46:29,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58888, dest: /192.168.251.4:50010, bytes: 160512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1182196424_191, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742015_1191, duration: 6687184
2020-01-11 18:46:29,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742015_1191, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:46:52,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742016_1192 src: /192.168.251.3:34468 dest: /192.168.251.4:50010
2020-01-11 18:46:52,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34468, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742016_1192, duration: 43400972
2020-01-11 18:46:52,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742016_1192, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742017_1193 src: /192.168.251.3:34472 dest: /192.168.251.4:50010
2020-01-11 18:46:52,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34472, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742017_1193, duration: 9520178
2020-01-11 18:46:52,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742017_1193, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742018_1194 src: /192.168.251.3:34476 dest: /192.168.251.4:50010
2020-01-11 18:46:52,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34476, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742018_1194, duration: 2972523
2020-01-11 18:46:52,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742018_1194, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742019_1195 src: /192.168.251.3:34480 dest: /192.168.251.4:50010
2020-01-11 18:46:52,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34480, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742019_1195, duration: 2462445
2020-01-11 18:46:52,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742019_1195, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742020_1196 src: /192.168.251.3:34484 dest: /192.168.251.4:50010
2020-01-11 18:46:52,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34484, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742020_1196, duration: 13398432
2020-01-11 18:46:52,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742020_1196, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742021_1197 src: /192.168.251.3:34488 dest: /192.168.251.4:50010
2020-01-11 18:46:52,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34488, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742021_1197, duration: 2914982
2020-01-11 18:46:52,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742021_1197, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742022_1198 src: /192.168.251.3:34492 dest: /192.168.251.4:50010
2020-01-11 18:46:52,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34492, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742022_1198, duration: 2241380
2020-01-11 18:46:52,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742022_1198, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742023_1199 src: /192.168.251.3:34496 dest: /192.168.251.4:50010
2020-01-11 18:46:52,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34496, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742023_1199, duration: 3407518
2020-01-11 18:46:52,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742023_1199, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742024_1200 src: /192.168.251.3:34500 dest: /192.168.251.4:50010
2020-01-11 18:46:52,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34500, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742024_1200, duration: 3755241
2020-01-11 18:46:52,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742024_1200, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:52,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742025_1201 src: /192.168.251.3:34504 dest: /192.168.251.4:50010
2020-01-11 18:46:52,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34504, dest: /192.168.251.4:50010, bytes: 112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742025_1201, duration: 17731514
2020-01-11 18:46:52,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742025_1201, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:53,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742026_1202 src: /192.168.251.3:34510 dest: /192.168.251.4:50010
2020-01-11 18:46:53,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34510, dest: /192.168.251.4:50010, bytes: 1514166, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742026_1202, duration: 16467459
2020-01-11 18:46:53,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742026_1202, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:53,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742027_1203 src: /192.168.251.3:34514 dest: /192.168.251.4:50010
2020-01-11 18:46:53,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34514, dest: /192.168.251.4:50010, bytes: 1177, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742027_1203, duration: 13148075
2020-01-11 18:46:53,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742027_1203, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:53,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742028_1204 src: /192.168.251.3:34518 dest: /192.168.251.4:50010
2020-01-11 18:46:53,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34518, dest: /192.168.251.4:50010, bytes: 135, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742028_1204, duration: 4516021
2020-01-11 18:46:53,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742028_1204, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:53,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742029_1205 src: /192.168.251.3:34522 dest: /192.168.251.4:50010
2020-01-11 18:46:53,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34522, dest: /192.168.251.4:50010, bytes: 89497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_872080702_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742029_1205, duration: 20931579
2020-01-11 18:46:53,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742029_1205, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:46:55,645 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741985_1161 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741985 for deletion
2020-01-11 18:46:55,645 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741986_1162 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741986 for deletion
2020-01-11 18:46:55,645 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741987_1163 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741987 for deletion
2020-01-11 18:46:55,645 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741988_1164 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741988 for deletion
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741989_1165 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741989 for deletion
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741990_1166 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741990 for deletion
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741991_1167 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741991 for deletion
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741992_1168 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741992 for deletion
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741993_1169 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741993 for deletion
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741994_1170 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741994 for deletion
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741985_1161 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741985
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741986_1162 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741986
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741987_1163 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741987
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741988_1164 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741988
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741989_1165 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741989
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741990_1166 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741990
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741991_1167 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741991
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741992_1168 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741992
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741993_1169 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741993
2020-01-11 18:46:55,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073741994_1170 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073741994
2020-01-11 18:46:59,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742030_1206 src: /192.168.251.4:58912 dest: /192.168.251.4:50010
2020-01-11 18:46:59,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58912, dest: /192.168.251.4:50010, bytes: 106451, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-367718330_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742030_1206, duration: 27908511
2020-01-11 18:46:59,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742030_1206, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:47:17,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742031_1207 src: /192.168.251.4:58930 dest: /192.168.251.4:50010
2020-01-11 18:47:36,140 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073742002_1178
2020-01-11 18:47:41,145 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073742011_1187
2020-01-11 18:47:42,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742032_1208 src: /192.168.251.3:34636 dest: /192.168.251.4:50010
2020-01-11 18:47:42,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34636, dest: /192.168.251.4:50010, bytes: 78, op: HDFS_WRITE, cliID: DFSClient_attempt_1578764061429_0007_r_000000_0_-794855128_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742032_1208, duration: 40550026
2020-01-11 18:47:42,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742032_1208, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:47:42,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58930, dest: /192.168.251.4:50010, bytes: 91840, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-367718330_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742031_1207, duration: 25229086573
2020-01-11 18:47:42,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742031_1207, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:47:42,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742033_1209 src: /192.168.251.4:58972 dest: /192.168.251.4:50010
2020-01-11 18:47:42,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58972, dest: /192.168.251.4:50010, bytes: 389, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-367718330_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742033_1209, duration: 7631837
2020-01-11 18:47:42,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742033_1209, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:47:42,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742034_1210 src: /192.168.251.4:58978 dest: /192.168.251.4:50010
2020-01-11 18:47:42,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58978, dest: /192.168.251.4:50010, bytes: 91840, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-367718330_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742034_1210, duration: 8202593
2020-01-11 18:47:42,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742034_1210, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:47:42,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742035_1211 src: /192.168.251.4:58982 dest: /192.168.251.4:50010
2020-01-11 18:47:42,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58982, dest: /192.168.251.4:50010, bytes: 106451, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-367718330_1, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742035_1211, duration: 5248312
2020-01-11 18:47:42,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742035_1211, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742026_1202 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742026 for deletion
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742027_1203 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742027 for deletion
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742028_1204 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742028 for deletion
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742029_1205 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742029 for deletion
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742030_1206 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742030 for deletion
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742031_1207 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742031 for deletion
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742026_1202 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742026
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742027_1203 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742027
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742028_1204 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742028
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742029_1205 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742029
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742030_1206 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742030
2020-01-11 18:47:46,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742031_1207 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742031
2020-01-11 18:47:49,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742036_1212 src: /192.168.251.3:34646 dest: /192.168.251.4:50010
2020-01-11 18:47:49,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:34646, dest: /192.168.251.4:50010, bytes: 73482, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-584651225_290, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742036_1212, duration: 6600997
2020-01-11 18:47:49,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742036_1212, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-01-11 18:47:50,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742037_1213 src: /192.168.251.4:58990 dest: /192.168.251.4:50010
2020-01-11 18:47:50,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:58990, dest: /192.168.251.4:50010, bytes: 100912, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_167664282_191, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742037_1213, duration: 8149588
2020-01-11 18:47:50,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742037_1213, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-01-11 18:47:51,164 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073742035_1211
2020-01-11 18:47:56,173 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073742036_1212
2020-01-11 18:49:19,740 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742033_1209 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742033 for deletion
2020-01-11 18:49:19,741 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742012_1188 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742012 for deletion
2020-01-11 18:49:19,742 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742033_1209 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742033
2020-01-11 18:49:19,742 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1099568443-192.168.251.3-1578306934316 blk_1073742012_1188 file /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/finalized/subdir0/subdir0/blk_1073742012
2020-01-18 17:19:12,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-01-18 17:19:12,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-18 17:19:12,858 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-18 17:19:13,498 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-01-18 17:19:13,762 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-01-18 17:19:13,762 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-01-18 17:19:13,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-01-18 17:19:13,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-01-18 17:19:13,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-01-18 17:19:13,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-01-18 17:19:13,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-01-18 17:19:14,105 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-01-18 17:19:14,113 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-01-18 17:19:14,136 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-01-18 17:19:14,141 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-01-18 17:19:14,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-01-18 17:19:14,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-01-18 17:19:14,175 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-01-18 17:19:14,181 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-01-18 17:19:14,181 INFO org.mortbay.log: jetty-6.1.26
2020-01-18 17:19:14,804 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-01-18 17:19:15,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-01-18 17:19:15,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-01-18 17:19:15,765 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-01-18 17:19:15,800 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-01-18 17:19:15,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-01-18 17:19:15,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-01-18 17:19:15,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-01-18 17:19:15,960 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-18 17:19:15,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-01-18 17:19:16,002 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-01-18 17:19:16,048 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-01-18 17:19:17,015 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2823@um2
2020-01-18 17:19:17,078 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-01-18 17:19:17,078 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-01-18 17:19:17,080 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-01-18 17:19:17,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-01-18 17:19:17,096 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-18 17:19:17,127 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-01-18 17:19:17,128 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-01-18 17:19:17,169 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-01-18 17:19:17,170 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-18 17:19:17,172 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-18 17:19:17,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 62ms
2020-01-18 17:19:17,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 64ms
2020-01-18 17:19:17,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-18 17:19:17,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 17ms
2020-01-18 17:19:17,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 18ms
2020-01-18 17:19:17,257 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1579379014257 with interval 21600000
2020-01-18 17:19:17,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-01-18 17:19:17,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-01-18 17:19:17,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-01-18 17:19:17,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=1805
2020-01-18 17:19:17,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-01-18 17:19:17,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3142937f7d,  containing 1 storage report(s), of which we sent 1. The reports had 106 total blocks and used 1 RPC(s). This took 6 msec to generate and 87 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-01-18 17:19:17,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-18 17:19:17,663 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-01-18 17:19:17,663 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-01-18 17:19:17,666 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-01-18 17:19:17,666 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-01-18 17:19:17,666 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-18 17:19:17,678 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-01-18 17:21:01,633 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073742009_1185
2020-01-24 16:13:14,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-01-24 16:13:14,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-01-24 16:13:14,693 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-24 16:13:15,609 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-01-24 16:13:15,923 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-01-24 16:13:15,923 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-01-24 16:13:15,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-01-24 16:13:15,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-01-24 16:13:16,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-01-24 16:13:16,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-01-24 16:13:16,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-01-24 16:13:16,204 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-01-24 16:13:16,213 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-01-24 16:13:16,236 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-01-24 16:13:16,240 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-01-24 16:13:16,240 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-01-24 16:13:16,240 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-01-24 16:13:16,283 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-01-24 16:13:16,296 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-01-24 16:13:16,296 INFO org.mortbay.log: jetty-6.1.26
2020-01-24 16:13:16,983 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-01-24 16:13:17,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-01-24 16:13:17,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-01-24 16:13:18,025 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-01-24 16:13:18,047 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-01-24 16:13:18,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-01-24 16:13:18,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-01-24 16:13:18,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-01-24 16:13:18,191 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-24 16:13:18,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-01-24 16:13:18,222 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-01-24 16:13:18,244 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-01-24 16:13:19,582 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2741@um2
2020-01-24 16:13:19,763 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-01-24 16:13:19,763 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-01-24 16:13:19,767 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-01-24 16:13:19,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-01-24 16:13:19,813 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-01-24 16:13:19,905 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-01-24 16:13:19,905 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-01-24 16:13:19,950 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-01-24 16:13:19,950 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-24 16:13:19,951 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-24 16:13:19,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 39ms
2020-01-24 16:13:19,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 40ms
2020-01-24 16:13:19,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-01-24 16:13:20,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 18ms
2020-01-24 16:13:20,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 20ms
2020-01-24 16:13:20,013 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1579895787013 with interval 21600000
2020-01-24 16:13:20,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-01-24 16:13:20,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-01-24 16:13:20,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-01-24 16:13:20,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=1830
2020-01-24 16:13:20,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-01-24 16:13:20,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x18e66e44ff,  containing 1 storage report(s), of which we sent 1. The reports had 106 total blocks and used 1 RPC(s). This took 5 msec to generate and 188 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-01-24 16:13:20,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-24 16:13:20,628 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-01-24 16:13:20,628 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-01-24 16:13:20,630 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-01-24 16:13:20,631 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-01-24 16:13:20,631 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-01-24 16:13:20,642 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-01-24 16:13:25,133 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1099568443-192.168.251.3-1578306934316:blk_1073741859_1035
2020-01-24 16:26:43,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1099568443-192.168.251.3-1578306934316:blk_1073742038_1214 src: /192.168.251.3:53664 dest: /192.168.251.4:50010
2020-01-24 16:26:43,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:53664, dest: /192.168.251.4:50010, bytes: 27, op: HDFS_WRITE, cliID: DFSClient_attempt_20200124162641_0017_m_000000_211_-1686129524_45, offset: 0, srvID: 80e34f7f-2071-4269-afc5-9a34cc2a1204, blockid: BP-1099568443-192.168.251.3-1578306934316:blk_1073742038_1214, duration: 110092627
2020-01-24 16:26:43,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1099568443-192.168.251.3-1578306934316:blk_1073742038_1214, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-02 16:37:21,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-02-02 16:37:21,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-02-02 16:37:22,695 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-02 16:37:24,092 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-02-02 16:37:24,397 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-02-02 16:37:24,397 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-02-02 16:37:24,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-02-02 16:37:24,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-02-02 16:37:24,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-02-02 16:37:24,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-02-02 16:37:24,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-02-02 16:37:24,851 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-02-02 16:37:24,867 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-02-02 16:37:24,878 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-02-02 16:37:24,881 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-02-02 16:37:24,881 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-02-02 16:37:24,881 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-02-02 16:37:24,915 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-02-02 16:37:24,918 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-02-02 16:37:24,918 INFO org.mortbay.log: jetty-6.1.26
2020-02-02 16:37:25,369 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-02-02 16:37:26,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-02-02 16:37:26,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-02-02 16:37:26,254 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-02-02 16:37:26,315 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-02-02 16:37:26,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-02-02 16:37:26,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-02-02 16:37:26,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-02-02 16:37:26,703 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-02 16:37:26,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-02-02 16:37:26,811 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-02-02 16:37:26,816 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-02-02 16:37:29,819 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: um1/192.168.251.3:9000
2020-02-02 16:37:35,665 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 2742@um2
2020-02-02 16:37:35,836 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1099568443-192.168.251.3-1578306934316
2020-02-02 16:37:35,837 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316
2020-02-02 16:37:35,846 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-02-02 16:37:35,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1121220742;bpid=BP-1099568443-192.168.251.3-1578306934316;lv=-56;nsInfo=lv=-60;cid=CID-d40beff8-3514-4fb0-91dd-06918f0ab065;nsid=1121220742;c=0;bpid=BP-1099568443-192.168.251.3-1578306934316;dnuuid=80e34f7f-2071-4269-afc5-9a34cc2a1204
2020-02-02 16:37:35,898 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-02 16:37:35,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-02-02 16:37:35,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-02-02 16:37:36,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-02-02 16:37:36,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1099568443-192.168.251.3-1578306934316
2020-02-02 16:37:36,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-02-02 16:37:36,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1099568443-192.168.251.3-1578306934316 on /orgz/data2/current: 62ms
2020-02-02 16:37:36,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1099568443-192.168.251.3-1578306934316: 77ms
2020-02-02 16:37:36,350 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current...
2020-02-02 16:37:36,425 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1099568443-192.168.251.3-1578306934316 on volume /orgz/data2/current: 75ms
2020-02-02 16:37:36,429 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 91ms
2020-02-02 16:37:36,445 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1580662249445 with interval 21600000
2020-02-02 16:37:36,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-02-02 16:37:36,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-02-02 16:37:36,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-02-02 16:37:37,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=1894
2020-02-02 16:37:37,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1099568443-192.168.251.3-1578306934316 (Datanode Uuid 80e34f7f-2071-4269-afc5-9a34cc2a1204) service to um1/192.168.251.3:9000
2020-02-02 16:37:37,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x27d206fbc1,  containing 1 storage report(s), of which we sent 1. The reports had 107 total blocks and used 1 RPC(s). This took 9 msec to generate and 337 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-02-02 16:37:37,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1099568443-192.168.251.3-1578306934316
2020-02-02 16:37:37,528 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-02-02 16:37:37,537 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-02-02 16:37:37,541 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-02-02 16:37:37,541 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-02-02 16:37:37,542 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1099568443-192.168.251.3-1578306934316
2020-02-02 16:37:37,604 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1099568443-192.168.251.3-1578306934316 to blockPoolScannerMap, new size=1
2020-02-02 17:35:00,748 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "um2/192.168.251.4"; destination host is: "um1":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2020-02-02 17:35:04,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-02 17:35:05,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-02 17:35:05,829 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2020-02-02 17:35:05,831 WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Failed to write dfsUsed to /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/dfsUsed
java.io.FileNotFoundException: /orgz/data2/current/BP-1099568443-192.168.251.3-1578306934316/current/dfsUsed (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:162)
	at java.io.FileWriter.<init>(FileWriter.java:90)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.saveDfsUsed(BlockPoolSlice.java:236)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice$1.run(BlockPoolSlice.java:144)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2020-02-02 17:35:05,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at um2/192.168.251.4
************************************************************/
2020-02-02 17:40:04,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-02-02 17:40:04,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-02-02 17:40:05,284 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-02 17:40:06,006 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-02-02 17:40:06,169 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-02-02 17:40:06,169 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-02-02 17:40:06,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-02-02 17:40:06,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-02-02 17:40:06,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-02-02 17:40:06,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-02-02 17:40:06,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-02-02 17:40:06,338 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-02-02 17:40:06,344 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-02-02 17:40:06,361 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-02-02 17:40:06,365 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-02-02 17:40:06,366 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-02-02 17:40:06,366 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-02-02 17:40:06,392 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-02-02 17:40:06,397 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-02-02 17:40:06,397 INFO org.mortbay.log: jetty-6.1.26
2020-02-02 17:40:06,851 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-02-02 17:40:07,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-02-02 17:40:07,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-02-02 17:40:07,549 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-02-02 17:40:07,627 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-02-02 17:40:07,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-02-02 17:40:07,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-02-02 17:40:07,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-02-02 17:40:07,732 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-02 17:40:07,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-02-02 17:40:07,748 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-02-02 17:40:07,750 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-02-02 17:40:08,797 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 21681@um2
2020-02-02 17:40:08,799 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /orgz/data2 is not formatted for BP-1217694175-192.168.251.3-1580661368826
2020-02-02 17:40:08,799 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2020-02-02 17:40:08,882 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1217694175-192.168.251.3-1580661368826
2020-02-02 17:40:08,882 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826
2020-02-02 17:40:08,882 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826 is not formatted for BP-1217694175-192.168.251.3-1580661368826
2020-02-02 17:40:08,883 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2020-02-02 17:40:08,883 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1217694175-192.168.251.3-1580661368826 directory /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current
2020-02-02 17:40:08,885 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-02-02 17:40:08,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1764777453;bpid=BP-1217694175-192.168.251.3-1580661368826;lv=-56;nsInfo=lv=-60;cid=CID-fa1ec920-c1b6-432b-9ffe-7f9053fe4aea;nsid=1764777453;c=0;bpid=BP-1217694175-192.168.251.3-1580661368826;dnuuid=null
2020-02-02 17:40:08,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID ff408c34-7225-4a3e-8e46-7f52a838f4f6
2020-02-02 17:40:08,898 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-02 17:40:08,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-02-02 17:40:08,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-02-02 17:40:08,928 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-02-02 17:40:08,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-02 17:40:08,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-02-02 17:40:08,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1217694175-192.168.251.3-1580661368826 on /orgz/data2/current: 14ms
2020-02-02 17:40:08,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1217694175-192.168.251.3-1580661368826: 18ms
2020-02-02 17:40:08,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-02-02 17:40:08,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current: 0ms
2020-02-02 17:40:08,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2020-02-02 17:40:08,951 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1580670801951 with interval 21600000
2020-02-02 17:40:08,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-02-02 17:40:09,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-02-02 17:40:09,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-02-02 17:40:09,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=1
2020-02-02 17:40:09,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000
2020-02-02 17:40:09,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3916b4d1d71,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 68 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-02-02 17:40:09,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-02 17:40:09,316 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-02-02 17:40:09,316 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-02-02 17:40:09,319 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-02-02 17:40:09,319 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-02-02 17:40:09,320 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-02 17:40:09,325 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1217694175-192.168.251.3-1580661368826 to blockPoolScannerMap, new size=1
2020-02-02 17:52:24,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741825_1001 src: /192.168.251.3:51418 dest: /192.168.251.4:50010
2020-02-02 17:52:24,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:51418, dest: /192.168.251.4:50010, bytes: 170, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_660205225_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741825_1001, duration: 164863350
2020-02-02 17:52:24,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-02 17:52:29,734 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741825_1001
2020-02-02 20:52:35,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from um1/192.168.251.3:9000 with active state
2020-02-02 20:52:35,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-02-02 20:52:35,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000 successfully registered with NN
2020-02-02 20:52:35,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x96bc01c201f,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-02-02 20:52:35,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-06 08:13:04,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-02-06 08:13:04,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-02-06 08:13:05,506 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-06 08:13:06,403 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-02-06 08:13:06,633 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-02-06 08:13:06,633 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-02-06 08:13:06,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-02-06 08:13:06,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-02-06 08:13:06,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-02-06 08:13:06,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-02-06 08:13:06,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-02-06 08:13:07,158 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-02-06 08:13:07,175 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-02-06 08:13:07,196 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-02-06 08:13:07,204 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-02-06 08:13:07,204 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-02-06 08:13:07,205 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-02-06 08:13:07,238 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-02-06 08:13:07,244 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-02-06 08:13:07,244 INFO org.mortbay.log: jetty-6.1.26
2020-02-06 08:13:07,801 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-02-06 08:13:08,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-02-06 08:13:08,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-02-06 08:13:08,706 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-02-06 08:13:08,727 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-02-06 08:13:08,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-02-06 08:13:08,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-02-06 08:13:08,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-02-06 08:13:08,864 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-06 08:13:08,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-02-06 08:13:08,889 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-02-06 08:13:08,913 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-02-06 08:13:09,910 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 13579@um2
2020-02-06 08:13:10,075 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1217694175-192.168.251.3-1580661368826
2020-02-06 08:13:10,076 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826
2020-02-06 08:13:10,077 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-02-06 08:13:10,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1764777453;bpid=BP-1217694175-192.168.251.3-1580661368826;lv=-56;nsInfo=lv=-60;cid=CID-fa1ec920-c1b6-432b-9ffe-7f9053fe4aea;nsid=1764777453;c=0;bpid=BP-1217694175-192.168.251.3-1580661368826;dnuuid=ff408c34-7225-4a3e-8e46-7f52a838f4f6
2020-02-06 08:13:10,100 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-06 08:13:10,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-02-06 08:13:10,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-02-06 08:13:10,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-02-06 08:13:10,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-06 08:13:10,243 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-02-06 08:13:10,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1217694175-192.168.251.3-1580661368826 on /orgz/data2/current: 32ms
2020-02-06 08:13:10,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1217694175-192.168.251.3-1580661368826: 34ms
2020-02-06 08:13:10,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-02-06 08:13:10,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current: 5ms
2020-02-06 08:13:10,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2020-02-06 08:13:10,288 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1580976682288 with interval 21600000
2020-02-06 08:13:10,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-02-06 08:13:10,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-02-06 08:13:10,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-02-06 08:13:10,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=34
2020-02-06 08:13:10,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000
2020-02-06 08:13:11,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1332e7fdf95,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 3 msec to generate and 259 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-02-06 08:13:11,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-06 08:13:11,187 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-02-06 08:13:11,187 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-02-06 08:13:11,197 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-02-06 08:13:11,197 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-02-06 08:13:11,198 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-06 08:13:11,209 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1217694175-192.168.251.3-1580661368826 to blockPoolScannerMap, new size=1
2020-02-15 14:57:59,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-02-15 14:57:59,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-02-15 14:58:00,281 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-15 14:58:01,156 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-02-15 14:58:01,344 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-02-15 14:58:01,344 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-02-15 14:58:01,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-02-15 14:58:01,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-02-15 14:58:01,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-02-15 14:58:01,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-02-15 14:58:01,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-02-15 14:58:01,605 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-02-15 14:58:01,620 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-02-15 14:58:01,631 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-02-15 14:58:01,634 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-02-15 14:58:01,634 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-02-15 14:58:01,634 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-02-15 14:58:01,654 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-02-15 14:58:01,657 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-02-15 14:58:01,657 INFO org.mortbay.log: jetty-6.1.26
2020-02-15 14:58:02,116 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-02-15 14:58:02,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-02-15 14:58:02,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-02-15 14:58:02,848 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-02-15 14:58:02,900 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-02-15 14:58:02,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-02-15 14:58:03,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-02-15 14:58:03,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-02-15 14:58:03,200 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-15 14:58:03,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-02-15 14:58:03,232 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-02-15 14:58:03,256 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-02-15 14:58:04,660 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 3133@um2
2020-02-15 14:58:04,851 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1217694175-192.168.251.3-1580661368826
2020-02-15 14:58:04,851 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826
2020-02-15 14:58:04,852 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-02-15 14:58:04,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1764777453;bpid=BP-1217694175-192.168.251.3-1580661368826;lv=-56;nsInfo=lv=-60;cid=CID-fa1ec920-c1b6-432b-9ffe-7f9053fe4aea;nsid=1764777453;c=0;bpid=BP-1217694175-192.168.251.3-1580661368826;dnuuid=ff408c34-7225-4a3e-8e46-7f52a838f4f6
2020-02-15 14:58:04,880 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-15 14:58:04,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-02-15 14:58:04,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-02-15 14:58:05,046 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-02-15 14:58:05,047 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-15 14:58:05,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-02-15 14:58:05,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1217694175-192.168.251.3-1580661368826 on /orgz/data2/current: 87ms
2020-02-15 14:58:05,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1217694175-192.168.251.3-1580661368826: 89ms
2020-02-15 14:58:05,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-02-15 14:58:05,144 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current: 7ms
2020-02-15 14:58:05,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2020-02-15 14:58:05,150 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1581782032150 with interval 21600000
2020-02-15 14:58:05,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-02-15 14:58:05,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-02-15 14:58:05,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-02-15 14:58:05,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=41
2020-02-15 14:58:05,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000
2020-02-15 14:58:05,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xbb9cb62d12,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 3 msec to generate and 143 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-02-15 14:58:05,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-15 14:58:05,782 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-02-15 14:58:05,782 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-02-15 14:58:05,786 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-02-15 14:58:05,786 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-02-15 14:58:05,787 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-15 14:58:05,797 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1217694175-192.168.251.3-1580661368826 to blockPoolScannerMap, new size=1
2020-02-15 15:06:41,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741826_1002 src: /192.168.251.3:49310 dest: /192.168.251.4:50010
2020-02-15 15:06:42,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49310, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-358494170_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741826_1002, duration: 454771685
2020-02-15 15:06:42,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:06:42,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741827_1003 src: /192.168.251.3:49314 dest: /192.168.251.4:50010
2020-02-15 15:06:42,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49314, dest: /192.168.251.4:50010, bytes: 106, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-358494170_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741827_1003, duration: 6642049
2020-02-15 15:06:42,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:06:43,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741828_1004 src: /192.168.251.3:49318 dest: /192.168.251.4:50010
2020-02-15 15:06:43,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49318, dest: /192.168.251.4:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-358494170_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741828_1004, duration: 29630439
2020-02-15 15:06:43,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:06:43,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741829_1005 src: /192.168.251.3:49322 dest: /192.168.251.4:50010
2020-02-15 15:06:43,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49322, dest: /192.168.251.4:50010, bytes: 89249, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-358494170_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741829_1005, duration: 24079674
2020-02-15 15:06:43,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:06:50,531 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741826_1002
2020-02-15 15:06:50,540 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741829_1005
2020-02-15 15:06:50,549 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741827_1003
2020-02-15 15:06:59,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741830_1006 src: /192.168.251.4:42346 dest: /192.168.251.4:50010
2020-02-15 15:06:59,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42346, dest: /192.168.251.4:50010, bytes: 106155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695550698_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741830_1006, duration: 56733897
2020-02-15 15:06:59,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:07:05,566 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741830_1006
2020-02-15 15:07:11,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741831_1007 src: /192.168.251.4:42356 dest: /192.168.251.4:50010
2020-02-15 15:07:20,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741832_1008 src: /192.168.251.4:42370 dest: /192.168.251.4:50010
2020-02-15 15:07:20,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42370, dest: /192.168.251.4:50010, bytes: 184, op: HDFS_WRITE, cliID: DFSClient_attempt_1581775135161_0001_r_000000_0_1955378153_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741832_1008, duration: 139631182
2020-02-15 15:07:20,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741832_1008, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:07:20,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42356, dest: /192.168.251.4:50010, bytes: 33603, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695550698_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741831_1007, duration: 9639363899
2020-02-15 15:07:20,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:07:20,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741833_1009 src: /192.168.251.4:42376 dest: /192.168.251.4:50010
2020-02-15 15:07:20,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42376, dest: /192.168.251.4:50010, bytes: 346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695550698_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741833_1009, duration: 14279146
2020-02-15 15:07:20,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741833_1009, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:07:20,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741834_1010 src: /192.168.251.4:42382 dest: /192.168.251.4:50010
2020-02-15 15:07:20,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42382, dest: /192.168.251.4:50010, bytes: 33603, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695550698_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741834_1010, duration: 33250869
2020-02-15 15:07:20,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741834_1010, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:07:21,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741835_1011 src: /192.168.251.4:42386 dest: /192.168.251.4:50010
2020-02-15 15:07:21,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42386, dest: /192.168.251.4:50010, bytes: 106155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695550698_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741835_1011, duration: 17099338
2020-02-15 15:07:21,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741835_1011, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:07:25,604 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741832_1008
2020-02-15 15:07:26,970 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2020-02-15 15:07:26,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2020-02-15 15:07:26,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2020-02-15 15:07:26,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2020-02-15 15:07:26,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2020-02-15 15:07:26,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741826_1002 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741826
2020-02-15 15:07:26,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2020-02-15 15:07:26,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741827_1003 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741827
2020-02-15 15:07:26,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741828_1004 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741828
2020-02-15 15:07:26,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741829_1005 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741829
2020-02-15 15:07:26,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741830_1006 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741830
2020-02-15 15:07:26,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741831_1007 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741831
2020-02-15 15:07:28,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741836_1012 src: /192.168.251.3:49354 dest: /192.168.251.4:50010
2020-02-15 15:07:28,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49354, dest: /192.168.251.4:50010, bytes: 4783, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1631673121_79, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741836_1012, duration: 98832562
2020-02-15 15:07:28,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:07:28,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741837_1013 src: /192.168.251.4:42394 dest: /192.168.251.4:50010
2020-02-15 15:07:29,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42394, dest: /192.168.251.4:50010, bytes: 42669, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464093392_80, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741837_1013, duration: 57987833
2020-02-15 15:07:29,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741837_1013, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:07:30,623 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741834_1010
2020-02-15 15:07:30,627 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741835_1011
2020-02-15 15:07:35,633 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741836_1012
2020-02-15 15:07:35,634 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741837_1013
2020-02-15 15:10:07,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741838_1014 src: /192.168.251.3:49364 dest: /192.168.251.4:50010
2020-02-15 15:10:07,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49364, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-369132124_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741838_1014, duration: 91828737
2020-02-15 15:10:07,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:10:07,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741839_1015 src: /192.168.251.3:49368 dest: /192.168.251.4:50010
2020-02-15 15:10:07,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49368, dest: /192.168.251.4:50010, bytes: 106, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-369132124_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741839_1015, duration: 16867457
2020-02-15 15:10:07,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:10:07,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741840_1016 src: /192.168.251.3:49372 dest: /192.168.251.4:50010
2020-02-15 15:10:07,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49372, dest: /192.168.251.4:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-369132124_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741840_1016, duration: 12253289
2020-02-15 15:10:07,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:10:07,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741841_1017 src: /192.168.251.3:49376 dest: /192.168.251.4:50010
2020-02-15 15:10:07,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49376, dest: /192.168.251.4:50010, bytes: 89349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-369132124_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741841_1017, duration: 13028805
2020-02-15 15:10:07,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:10:15,752 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741840_1016
2020-02-15 15:10:15,754 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741839_1015
2020-02-15 15:10:17,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741842_1018 src: /192.168.251.4:42424 dest: /192.168.251.4:50010
2020-02-15 15:10:17,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42424, dest: /192.168.251.4:50010, bytes: 106146, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1086375970_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741842_1018, duration: 69150459
2020-02-15 15:10:17,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741842_1018, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:10:25,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741843_1019 src: /192.168.251.4:42432 dest: /192.168.251.4:50010
2020-02-15 15:10:25,786 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741842_1018
2020-02-15 15:10:33,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741844_1020 src: /192.168.251.3:49414 dest: /192.168.251.4:50010
2020-02-15 15:10:33,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49414, dest: /192.168.251.4:50010, bytes: 184, op: HDFS_WRITE, cliID: DFSClient_attempt_1581775135161_0002_r_000000_0_-2112043851_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741844_1020, duration: 58773253
2020-02-15 15:10:33,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:10:33,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42432, dest: /192.168.251.4:50010, bytes: 33576, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1086375970_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741843_1019, duration: 8248141376
2020-02-15 15:10:33,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741843_1019, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:10:33,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741845_1021 src: /192.168.251.4:42440 dest: /192.168.251.4:50010
2020-02-15 15:10:33,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42440, dest: /192.168.251.4:50010, bytes: 342, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1086375970_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741845_1021, duration: 10906672
2020-02-15 15:10:33,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741845_1021, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:10:34,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741846_1022 src: /192.168.251.4:42446 dest: /192.168.251.4:50010
2020-02-15 15:10:34,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42446, dest: /192.168.251.4:50010, bytes: 33576, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1086375970_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741846_1022, duration: 17707691
2020-02-15 15:10:34,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741846_1022, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:10:34,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741847_1023 src: /192.168.251.4:42450 dest: /192.168.251.4:50010
2020-02-15 15:10:34,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42450, dest: /192.168.251.4:50010, bytes: 106146, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1086375970_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741847_1023, duration: 12419806
2020-02-15 15:10:34,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741847_1023, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:10:39,218 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2020-02-15 15:10:39,219 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2020-02-15 15:10:39,219 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741840_1016 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741840
2020-02-15 15:10:39,219 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2020-02-15 15:10:39,219 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2020-02-15 15:10:39,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741841_1017 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741841
2020-02-15 15:10:39,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2020-02-15 15:10:39,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741842_1018 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741842
2020-02-15 15:10:39,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2020-02-15 15:10:39,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741843_1019 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741843
2020-02-15 15:10:39,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741838_1014 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741838
2020-02-15 15:10:39,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741839_1015 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741839
2020-02-15 15:10:40,795 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741846_1022
2020-02-15 15:10:40,796 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741845_1021
2020-02-15 15:10:40,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741848_1024 src: /192.168.251.3:49420 dest: /192.168.251.4:50010
2020-02-15 15:10:40,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49420, dest: /192.168.251.4:50010, bytes: 12041, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1886020545_116, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741848_1024, duration: 4945703
2020-02-15 15:10:40,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:10:41,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741849_1025 src: /192.168.251.4:42458 dest: /192.168.251.4:50010
2020-02-15 15:10:41,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42458, dest: /192.168.251.4:50010, bytes: 35098, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_263940102_130, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741849_1025, duration: 12368431
2020-02-15 15:10:41,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741849_1025, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:10:50,814 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741849_1025
2020-02-15 15:10:50,815 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741848_1024
2020-02-15 15:11:33,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741850_1026 src: /192.168.251.3:49428 dest: /192.168.251.4:50010
2020-02-15 15:11:33,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49428, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-216831419_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741850_1026, duration: 94113079
2020-02-15 15:11:33,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:11:33,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741851_1027 src: /192.168.251.3:49432 dest: /192.168.251.4:50010
2020-02-15 15:11:33,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49432, dest: /192.168.251.4:50010, bytes: 106, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-216831419_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741851_1027, duration: 32039124
2020-02-15 15:11:33,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:11:33,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741852_1028 src: /192.168.251.3:49436 dest: /192.168.251.4:50010
2020-02-15 15:11:33,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49436, dest: /192.168.251.4:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-216831419_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741852_1028, duration: 8368538
2020-02-15 15:11:33,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:11:33,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741853_1029 src: /192.168.251.3:49440 dest: /192.168.251.4:50010
2020-02-15 15:11:33,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49440, dest: /192.168.251.4:50010, bytes: 89349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-216831419_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741853_1029, duration: 31057478
2020-02-15 15:11:33,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:11:37,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741854_1030 src: /192.168.251.3:49448 dest: /192.168.251.4:50010
2020-02-15 15:11:38,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49448, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105035321_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741854_1030, duration: 221014114
2020-02-15 15:11:38,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:11:38,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741855_1031 src: /192.168.251.3:49452 dest: /192.168.251.4:50010
2020-02-15 15:11:38,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49452, dest: /192.168.251.4:50010, bytes: 106, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105035321_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741855_1031, duration: 11415600
2020-02-15 15:11:38,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:11:38,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741856_1032 src: /192.168.251.3:49456 dest: /192.168.251.4:50010
2020-02-15 15:11:38,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49456, dest: /192.168.251.4:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105035321_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741856_1032, duration: 19819719
2020-02-15 15:11:38,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:11:38,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741857_1033 src: /192.168.251.3:49460 dest: /192.168.251.4:50010
2020-02-15 15:11:38,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49460, dest: /192.168.251.4:50010, bytes: 89352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105035321_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741857_1033, duration: 35066278
2020-02-15 15:11:38,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 15:11:40,863 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741852_1028
2020-02-15 15:11:40,874 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741853_1029
2020-02-15 15:11:45,949 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741856_1032
2020-02-15 15:11:45,962 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741854_1030
2020-02-15 15:11:48,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741858_1034 src: /192.168.251.4:42494 dest: /192.168.251.4:50010
2020-02-15 15:11:48,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42494, dest: /192.168.251.4:50010, bytes: 106146, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1017775754_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741858_1034, duration: 83283318
2020-02-15 15:11:48,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741858_1034, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:11:55,983 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741858_1034
2020-02-15 15:11:56,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741859_1035 src: /192.168.251.4:42506 dest: /192.168.251.4:50010
2020-02-15 15:11:56,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42506, dest: /192.168.251.4:50010, bytes: 106148, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695277236_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741859_1035, duration: 214629069
2020-02-15 15:11:56,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741859_1035, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:02,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741860_1036 src: /192.168.251.4:42520 dest: /192.168.251.4:50010
2020-02-15 15:12:06,031 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741859_1035
2020-02-15 15:12:08,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741861_1037 src: /192.168.251.4:42536 dest: /192.168.251.4:50010
2020-02-15 15:12:12,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741862_1038 src: /192.168.251.4:42548 dest: /192.168.251.4:50010
2020-02-15 15:12:12,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42548, dest: /192.168.251.4:50010, bytes: 184, op: HDFS_WRITE, cliID: DFSClient_attempt_1581775135161_0003_r_000000_0_1749574085_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741862_1038, duration: 114019876
2020-02-15 15:12:12,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741862_1038, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:13,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42520, dest: /192.168.251.4:50010, bytes: 33607, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1017775754_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741860_1036, duration: 11707770283
2020-02-15 15:12:13,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741860_1036, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:13,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741863_1039 src: /192.168.251.4:42558 dest: /192.168.251.4:50010
2020-02-15 15:12:13,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42558, dest: /192.168.251.4:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1017775754_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741863_1039, duration: 26509013
2020-02-15 15:12:13,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741863_1039, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:14,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741864_1040 src: /192.168.251.4:42564 dest: /192.168.251.4:50010
2020-02-15 15:12:14,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42564, dest: /192.168.251.4:50010, bytes: 33607, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1017775754_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741864_1040, duration: 28242021
2020-02-15 15:12:14,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741864_1040, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:14,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741865_1041 src: /192.168.251.4:42568 dest: /192.168.251.4:50010
2020-02-15 15:12:14,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42568, dest: /192.168.251.4:50010, bytes: 106146, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1017775754_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741865_1041, duration: 27300408
2020-02-15 15:12:14,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741865_1041, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:18,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2020-02-15 15:12:18,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2020-02-15 15:12:18,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2020-02-15 15:12:18,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2020-02-15 15:12:18,285 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2020-02-15 15:12:18,285 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2020-02-15 15:12:18,285 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741858_1034 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741858
2020-02-15 15:12:18,285 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741860_1036 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741860
2020-02-15 15:12:18,286 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741850_1026 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741850
2020-02-15 15:12:18,286 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741851_1027 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741851
2020-02-15 15:12:18,286 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741852_1028 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741852
2020-02-15 15:12:18,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741853_1029 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741853
2020-02-15 15:12:18,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741866_1042 src: /192.168.251.4:42578 dest: /192.168.251.4:50010
2020-02-15 15:12:18,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42578, dest: /192.168.251.4:50010, bytes: 184, op: HDFS_WRITE, cliID: DFSClient_attempt_1581775135161_0004_r_000000_0_926503305_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741866_1042, duration: 81997962
2020-02-15 15:12:18,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741866_1042, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:19,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42536, dest: /192.168.251.4:50010, bytes: 33582, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695277236_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741861_1037, duration: 10290946268
2020-02-15 15:12:19,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741861_1037, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:19,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741867_1043 src: /192.168.251.4:42586 dest: /192.168.251.4:50010
2020-02-15 15:12:19,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42586, dest: /192.168.251.4:50010, bytes: 344, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695277236_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741867_1043, duration: 8694373
2020-02-15 15:12:19,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741867_1043, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:19,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741868_1044 src: /192.168.251.4:42592 dest: /192.168.251.4:50010
2020-02-15 15:12:19,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42592, dest: /192.168.251.4:50010, bytes: 33582, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695277236_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741868_1044, duration: 10230080
2020-02-15 15:12:19,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741868_1044, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:19,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741869_1045 src: /192.168.251.4:42596 dest: /192.168.251.4:50010
2020-02-15 15:12:19,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42596, dest: /192.168.251.4:50010, bytes: 106148, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-695277236_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741869_1045, duration: 16115707
2020-02-15 15:12:19,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741869_1045, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:21,061 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741865_1041
2020-02-15 15:12:21,062 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741863_1039
2020-02-15 15:12:21,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741870_1046 src: /192.168.251.4:42604 dest: /192.168.251.4:50010
2020-02-15 15:12:21,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42604, dest: /192.168.251.4:50010, bytes: 45631, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1119932849_130, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741870_1046, duration: 7300968
2020-02-15 15:12:21,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741870_1046, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:24,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2020-02-15 15:12:24,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2020-02-15 15:12:24,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2020-02-15 15:12:24,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741856_1032 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741856
2020-02-15 15:12:24,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2020-02-15 15:12:24,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2020-02-15 15:12:24,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2020-02-15 15:12:24,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741857_1033 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741857
2020-02-15 15:12:24,286 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741859_1035 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741859
2020-02-15 15:12:24,286 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741861_1037 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741861
2020-02-15 15:12:24,286 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741854_1030 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741854
2020-02-15 15:12:24,286 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741855_1031 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741855
2020-02-15 15:12:26,070 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741868_1044
2020-02-15 15:12:26,071 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741867_1043
2020-02-15 15:12:27,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741871_1047 src: /192.168.251.4:42612 dest: /192.168.251.4:50010
2020-02-15 15:12:27,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42612, dest: /192.168.251.4:50010, bytes: 45349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_107563038_191, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741871_1047, duration: 25835247
2020-02-15 15:12:27,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741871_1047, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 15:12:31,074 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741870_1046
2020-02-15 15:12:36,083 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741871_1047
2020-02-15 16:33:44,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741872_1048 src: /192.168.251.3:49506 dest: /192.168.251.4:50010
2020-02-15 16:33:44,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49506, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2143280786_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741872_1048, duration: 71963100
2020-02-15 16:33:44,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 16:33:49,130 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2020-02-15 16:33:49,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741872_1048 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741872
2020-02-15 16:34:08,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741873_1049 src: /192.168.251.3:49514 dest: /192.168.251.4:50010
2020-02-15 16:34:08,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49514, dest: /192.168.251.4:50010, bytes: 292710, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1624168446_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741873_1049, duration: 81920580
2020-02-15 16:34:08,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 16:34:08,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741874_1050 src: /192.168.251.3:49518 dest: /192.168.251.4:50010
2020-02-15 16:34:08,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49518, dest: /192.168.251.4:50010, bytes: 106, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1624168446_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741874_1050, duration: 15145169
2020-02-15 16:34:08,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 16:34:08,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741875_1051 src: /192.168.251.3:49522 dest: /192.168.251.4:50010
2020-02-15 16:34:08,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49522, dest: /192.168.251.4:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1624168446_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741875_1051, duration: 9306078
2020-02-15 16:34:08,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 16:34:09,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741876_1052 src: /192.168.251.3:49526 dest: /192.168.251.4:50010
2020-02-15 16:34:09,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49526, dest: /192.168.251.4:50010, bytes: 89351, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1624168446_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741876_1052, duration: 13979385
2020-02-15 16:34:09,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 16:34:14,305 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741874_1050
2020-02-15 16:34:14,306 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741876_1052
2020-02-15 16:34:15,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741877_1053 src: /192.168.251.4:42814 dest: /192.168.251.4:50010
2020-02-15 16:34:15,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42814, dest: /192.168.251.4:50010, bytes: 106147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_505438538_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741877_1053, duration: 291266347
2020-02-15 16:34:15,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741877_1053, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 16:34:20,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741878_1054 src: /192.168.251.4:42828 dest: /192.168.251.4:50010
2020-02-15 16:34:24,321 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741877_1053
2020-02-15 16:34:28,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741879_1055 src: /192.168.251.4:42842 dest: /192.168.251.4:50010
2020-02-15 16:34:28,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42842, dest: /192.168.251.4:50010, bytes: 94, op: HDFS_WRITE, cliID: DFSClient_attempt_1581775135161_0006_r_000001_0_1697063567_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741879_1055, duration: 73647265
2020-02-15 16:34:28,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741879_1055, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 16:34:29,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741880_1056 src: /192.168.251.3:49548 dest: /192.168.251.4:50010
2020-02-15 16:34:29,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49548, dest: /192.168.251.4:50010, bytes: 90, op: HDFS_WRITE, cliID: DFSClient_attempt_1581775135161_0006_r_000000_0_-1870463144_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741880_1056, duration: 55871533
2020-02-15 16:34:29,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 16:34:29,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42828, dest: /192.168.251.4:50010, bytes: 41091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_505438538_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741878_1054, duration: 9266374750
2020-02-15 16:34:29,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741878_1054, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 16:34:29,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741881_1057 src: /192.168.251.4:42850 dest: /192.168.251.4:50010
2020-02-15 16:34:29,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42850, dest: /192.168.251.4:50010, bytes: 344, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_505438538_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741881_1057, duration: 6165135
2020-02-15 16:34:29,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741881_1057, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 16:34:29,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741882_1058 src: /192.168.251.4:42856 dest: /192.168.251.4:50010
2020-02-15 16:34:29,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42856, dest: /192.168.251.4:50010, bytes: 41091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_505438538_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741882_1058, duration: 7284476
2020-02-15 16:34:29,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741882_1058, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 16:34:29,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741883_1059 src: /192.168.251.4:42860 dest: /192.168.251.4:50010
2020-02-15 16:34:29,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42860, dest: /192.168.251.4:50010, bytes: 106147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_505438538_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741883_1059, duration: 8464875
2020-02-15 16:34:29,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741883_1059, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 16:34:31,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2020-02-15 16:34:31,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2020-02-15 16:34:31,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2020-02-15 16:34:31,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2020-02-15 16:34:31,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741873_1049 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741873
2020-02-15 16:34:31,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2020-02-15 16:34:31,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741874_1050 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741874
2020-02-15 16:34:31,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2020-02-15 16:34:31,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741875_1051 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741875
2020-02-15 16:34:31,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741876_1052 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741876
2020-02-15 16:34:31,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741877_1053 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741877
2020-02-15 16:34:31,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1217694175-192.168.251.3-1580661368826 blk_1073741878_1054 file /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826/current/finalized/subdir0/subdir0/blk_1073741878
2020-02-15 16:34:36,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741884_1060 src: /192.168.251.3:49554 dest: /192.168.251.4:50010
2020-02-15 16:34:36,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:49554, dest: /192.168.251.4:50010, bytes: 7674, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1042959793_154, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741884_1060, duration: 3763091
2020-02-15 16:34:36,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-02-15 16:34:37,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741885_1061 src: /192.168.251.4:42868 dest: /192.168.251.4:50010
2020-02-15 16:34:37,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.4:42868, dest: /192.168.251.4:50010, bytes: 52598, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_770138624_265, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741885_1061, duration: 6498885
2020-02-15 16:34:37,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741885_1061, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2020-02-15 16:34:39,344 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741881_1057
2020-02-15 16:34:39,345 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741880_1056
2020-02-15 16:34:44,359 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741884_1060
2020-02-15 16:34:44,360 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741885_1061
2020-02-15 16:53:52,163 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1217694175-192.168.251.3-1580661368826 Total blocks: 30, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2020-02-15 16:59:53,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x76124851f9f,  containing 1 storage report(s), of which we sent 1. The reports had 30 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-02-15 16:59:53,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-15 18:46:24,766 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: No route to host; Host Details : local host is: "um2/192.168.251.4"; destination host is: "um1":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: No route to host
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1073)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)
2020-02-15 18:46:28,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:31,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:34,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:38,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:41,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:44,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:47,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:50,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:53,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:56,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:46:58,554 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:47:02,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:05,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:08,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:11,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:14,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:17,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:21,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:24,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:27,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:30,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:32,346 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:47:36,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:39,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:42,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:45,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:48,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:51,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:54,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:47:57,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:00,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:04,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:06,142 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:48:10,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:13,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:16,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:19,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:22,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:25,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:28,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:31,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:34,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:37,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:39,966 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:48:44,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:47,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:50,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:53,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:56,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:48:59,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:02,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:05,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:08,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:11,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:13,819 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:49:17,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:20,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:24,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:27,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:30,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:33,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:36,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:39,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:45,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:47,613 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:49:51,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:54,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:49:57,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:00,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:04,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:07,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:10,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:13,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:16,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:19,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:21,570 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:50:25,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:28,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:31,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:34,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:37,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:41,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:44,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:47,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:50,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:53,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:50:55,387 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:50:59,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:02,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:05,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:08,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:11,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:14,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:17,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:20,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:24,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:27,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:29,243 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:51:33,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:36,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:39,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:42,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:45,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:48,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:51,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:54,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:51:58,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:01,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:03,131 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:52:07,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:10,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:13,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:16,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:19,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:22,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:25,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:28,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:31,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:34,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:36,924 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:52:40,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:44,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:47,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:50,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:53,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:56,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:52:59,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:02,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:05,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:08,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:10,715 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:53:14,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:17,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:20,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:24,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:27,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:30,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:33,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:36,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:39,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:42,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:44,507 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:53:48,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:51,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:54,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:53:57,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:00,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:04,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:07,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:10,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:13,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:16,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:18,363 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:54:22,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:25,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:28,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:31,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:34,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:37,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:40,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:43,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:47,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:50,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:52,253 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:54:56,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:54:59,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:02,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:05,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:08,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:11,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:14,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:17,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:20,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:23,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:26,043 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:55:30,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:33,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:36,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:39,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:42,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:45,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:48,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:51,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:54,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:55:58,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:00,122 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:56:04,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:07,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:10,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:13,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:16,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:19,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:22,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:25,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:28,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:31,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:33,916 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:56:37,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:41,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:44,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:47,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:50,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:53,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:56,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:56:59,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:02,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:05,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:07,706 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:57:11,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:14,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:17,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:20,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:24,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:27,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:30,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:33,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:36,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:39,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:41,500 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:57:45,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:48,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:51,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:54,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:57:57,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:01,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:04,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:07,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:10,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:13,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:15,387 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:58:19,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:22,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:25,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:28,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:31,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:34,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:37,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:40,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:44,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:47,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:49,215 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:58:53,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:56,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:58:59,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:02,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:05,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:08,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:11,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:14,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:17,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:20,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:23,003 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 18:59:27,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:30,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:33,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:36,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:39,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:42,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:45,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:48,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:51,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:54,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 18:59:56,795 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:00:00,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:03,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:07,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:10,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:13,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:16,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:19,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:22,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:25,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:28,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:30,620 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:00:34,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:37,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:40,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:43,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:46,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:50,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:53,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:56,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:00:59,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:02,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:04,413 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:01:08,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:11,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:14,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:17,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:20,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:23,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:26,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:29,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:33,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:36,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:38,216 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:01:42,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:45,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:48,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:51,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:54,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:01:57,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:00,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:03,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:06,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:09,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:11,994 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:02:16,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:19,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:22,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:25,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:28,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:31,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:34,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:37,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:40,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:44,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:46,107 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:02:50,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:53,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:56,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:02:59,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:02,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:05,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:08,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:11,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:14,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:17,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:19,903 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:03:23,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:27,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:30,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:33,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:36,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:39,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:42,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:45,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:48,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:51,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:03:53,693 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:03:57,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:00,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:03,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:07,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:10,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:13,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:16,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:19,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:22,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:25,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:27,552 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:04:31,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:34,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:37,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:40,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:43,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:47,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:50,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:53,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:56,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:04:59,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:01,342 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:05:05,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:08,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:11,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:14,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:17,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:20,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:23,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:26,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:29,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:33,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:35,143 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:05:39,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:42,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:45,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:48,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:51,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:54,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:05:57,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:00,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:03,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:06,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:08,923 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:06:12,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:16,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:19,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:22,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:25,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:28,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:31,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:34,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:37,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:40,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:42,780 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:06:46,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:49,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:52,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:56,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:06:59,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:02,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:05,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:08,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:11,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:14,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:16,573 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:07:20,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:23,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:26,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:29,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:32,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:36,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:39,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:42,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:45,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:48,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:50,406 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:07:54,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:07:57,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:00,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:03,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:06,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:09,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:12,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:16,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:19,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:22,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:24,251 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:08:28,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:31,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:34,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:37,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:40,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:43,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:46,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:49,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:52,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:56,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:08:58,111 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:09:02,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:05,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:08,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:11,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:14,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:17,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:20,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:23,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:26,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:29,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:31,901 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:09:35,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:39,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:42,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:45,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:48,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:51,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:54,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:09:57,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:00,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:03,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:05,698 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:10:09,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:12,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:15,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:18,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:22,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:25,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:28,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:31,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:34,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:37,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:39,483 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:10:43,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:46,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:49,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:52,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:55,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:10:58,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:02,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:05,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:08,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:11,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:13,316 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:11:17,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:20,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:23,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:26,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:29,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:33,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:36,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:39,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:42,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:45,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:47,424 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:11:51,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:54,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:11:57,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:00,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:03,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:07,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:10,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:13,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:16,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:19,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:21,409 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:12:25,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:28,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:31,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:34,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:37,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:40,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:43,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:46,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:50,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:53,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:12:55,198 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:12:59,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:02,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:05,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:08,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:11,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:14,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:17,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:20,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:23,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:26,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:28,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:13:33,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:36,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:39,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:42,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:45,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:48,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:51,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:54,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:13:57,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:00,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:02,779 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:14:06,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:09,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:12,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:16,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:19,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:22,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:25,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:28,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:31,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:34,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:36,604 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:14:40,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:43,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:46,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:49,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:52,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:56,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:14:59,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:02,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:05,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:08,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:10,396 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:15:14,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:17,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:20,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:23,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:26,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:29,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:32,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:35,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:39,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:42,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:44,190 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:15:48,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:51,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:54,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:15:57,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:00,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:03,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:06,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:09,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:12,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:15,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:18,012 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:16:22,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:25,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:28,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:31,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:34,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:37,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:40,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:43,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:46,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:49,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:51,905 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:16:55,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:16:59,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:02,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:05,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:08,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:11,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:14,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:17,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:20,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:23,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:25,755 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:17:29,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:32,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:35,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:39,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:42,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:45,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:48,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:51,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:54,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:57,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:17:59,548 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:18:03,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:06,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:09,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:12,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:15,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:19,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:22,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:25,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:28,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:31,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:33,370 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:18:37,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:40,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:43,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:46,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:49,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:52,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:55,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:18:58,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:02,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:05,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:07,164 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:19:11,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:14,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:17,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:20,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:23,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:26,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:29,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:32,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:35,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:38,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:40,955 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:19:45,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:48,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:51,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:54,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:19:57,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:00,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:03,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:06,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:09,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:12,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:14,746 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:20:18,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:21,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:24,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:28,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:31,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:34,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:37,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:40,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:43,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:46,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:48,539 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:20:52,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:55,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:20:58,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:01,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:04,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:08,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:11,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:14,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:17,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:20,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:22,362 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:21:26,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:29,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:32,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:35,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:38,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:41,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:44,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:47,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:51,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:54,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:21:56,160 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:22:00,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:03,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:06,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:09,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:12,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:15,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:18,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:21,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:24,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:27,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:29,974 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:22:34,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:37,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:40,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:43,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:46,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:49,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:52,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:55,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:22:58,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:01,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:03,789 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:23:07,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:10,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:13,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:17,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:20,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:23,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:26,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:29,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:32,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:35,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:37,563 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:23:41,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:44,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:47,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:50,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:53,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:23:57,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:00,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:03,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:06,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:09,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:11,354 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:24:15,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:18,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:21,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:24,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:27,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:30,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:33,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:36,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:40,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:43,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:45,180 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:24:49,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:52,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:55,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:24:58,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:01,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:04,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:07,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:10,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:13,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:16,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:19,009 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:25:23,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:26,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:29,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:32,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:35,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:38,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:41,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:44,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:47,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:50,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:52,836 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:25:56,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:25:59,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:03,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:06,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:09,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:12,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:15,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:18,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:21,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:24,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:26,621 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:26:30,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:33,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:36,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:39,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:42,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:46,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:49,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:52,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:55,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:26:58,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:00,410 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:27:04,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:07,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:10,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:13,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:16,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:19,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:22,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:25,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:29,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:32,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:34,203 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:27:38,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:41,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:44,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:47,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:50,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:53,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:56,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:27:59,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:02,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:05,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:08,026 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:28:12,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:15,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:18,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:21,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:24,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:27,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:30,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:33,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:36,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:39,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:41,852 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:28:45,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:48,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:52,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:55,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:28:58,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:01,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:04,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:07,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:10,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:13,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:15,647 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:29:19,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:22,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:25,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:28,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:32,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:35,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:38,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:41,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:44,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:47,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:49,437 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:29:53,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:56,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:29:59,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:02,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:05,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:08,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:11,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:15,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:18,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:21,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:23,264 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:30:27,333 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:30,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:33,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:36,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:39,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:42,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:45,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:48,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:51,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:55,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:30:57,084 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:31:01,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:04,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:07,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:10,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:13,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:16,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:19,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:22,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:25,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:28,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:30,874 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:31:34,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:38,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:41,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:44,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:47,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:50,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:53,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:56,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:31:59,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:02,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:04,698 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:32:08,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:11,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:14,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:17,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:21,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:24,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:27,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:30,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:33,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:36,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:38,496 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:32:42,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:45,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:48,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:51,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:54,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:32:57,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:00,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:04,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:07,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:10,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:12,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:33:16,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:19,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:22,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:25,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:28,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:31,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:34,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:37,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:40,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:44,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:46,143 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:33:50,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:53,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:56,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:33:59,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:02,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:05,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:08,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:11,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:14,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:17,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:19,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:34:24,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:27,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:30,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:33,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:36,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:39,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:42,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:45,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:48,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:51,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:34:53,754 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:34:57,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:00,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:03,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:07,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:10,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:13,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:16,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:19,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:22,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:25,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:27,835 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:35:31,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:34,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:38,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:41,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:44,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:47,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:50,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:53,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:56,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:35:59,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:01,626 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:36:05,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:08,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:11,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:14,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:17,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:21,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:24,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:27,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:30,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:33,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:35,486 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:36:39,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:42,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:45,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:48,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:51,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:54,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:36:57,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:01,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:04,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:07,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:09,274 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:37:13,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:16,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:19,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:22,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:25,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:28,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:31,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:34,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:37,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:41,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:43,067 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:37:47,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:50,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:53,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:56,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:37:59,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:02,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:05,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:08,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:11,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:14,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:16,858 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:38:20,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:24,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:27,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:30,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:33,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:36,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:39,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:42,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:45,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:48,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:50,650 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:38:54,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:38:57,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:00,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:03,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:07,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:10,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:13,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:16,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:19,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:22,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:24,449 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:39:28,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:31,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:34,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:37,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:40,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:43,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:46,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:50,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:53,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:56,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:39:58,234 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:40:02,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:05,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:08,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:11,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:14,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:17,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:20,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:23,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:26,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:29,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:32,026 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:40:36,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:39,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:42,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:45,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:48,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:51,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:54,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:40:57,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:00,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:03,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:05,818 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:41:09,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:12,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:16,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:19,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:22,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:25,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:28,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:31,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:34,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:37,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:39,706 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:41:43,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:46,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:49,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:52,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:56,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:41:59,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:02,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:05,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:08,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:11,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:13,499 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:42:17,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:20,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:23,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:26,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:29,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:32,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:36,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:39,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:42,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:45,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:47,291 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:42:51,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:54,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:42:57,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:00,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:03,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:06,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:09,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:12,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:15,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:19,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:21,082 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:43:25,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:28,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:31,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:34,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:37,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:40,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:43,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:46,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:49,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:52,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:43:54,874 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:43:58,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:02,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:05,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:08,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:11,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:14,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:17,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:20,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:23,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:26,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:28,704 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:44:32,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:35,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:38,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:41,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:45,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:48,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:51,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:54,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:44:57,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:00,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:02,490 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:45:06,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:09,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:12,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:15,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:18,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:21,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:24,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:28,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:31,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:34,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:36,285 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:45:40,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:43,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:46,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:49,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:52,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:55,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:45:58,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:01,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:04,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:08,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:10,106 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:46:14,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:17,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:20,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:23,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:26,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:29,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:32,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:35,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:38,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:41,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:43,898 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:46:47,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:51,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:54,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:46:57,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:00,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:03,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:06,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:09,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:12,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:15,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:17,691 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:47:21,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:24,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:27,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:31,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:34,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:37,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:40,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:43,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:46,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:49,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:51,554 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:47:55,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:47:58,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:01,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:04,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:07,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:10,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:14,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:17,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:20,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:23,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:25,342 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:48:29,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:32,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:35,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:38,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:41,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:44,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:47,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:50,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:53,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:57,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:48:59,131 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:49:03,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:06,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:09,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:12,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:15,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:18,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:21,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:24,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:27,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:30,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:32,986 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:49:37,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:40,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:43,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:46,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:49,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:52,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:55,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:49:58,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:01,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:04,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:06,779 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:50:10,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:13,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:17,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:20,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:23,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:26,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:29,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:32,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:35,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:38,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:40,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:50:44,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:47,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:50,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:53,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:50:56,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:00,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:03,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:06,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:09,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:12,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:14,362 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:51:18,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:21,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:24,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:27,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:30,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:33,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:36,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:40,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:43,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:46,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:48,220 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:51:52,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:55,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:51:58,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:01,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:04,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:07,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:10,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:13,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:16,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:19,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:22,011 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:52:26,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:29,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:32,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:35,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:38,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:41,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:44,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:47,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:50,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:53,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:52:55,836 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
2020-02-15 19:52:59,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:02,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:06,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:09,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:12,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:15,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:18,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:21,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:24,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:27,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: um1/192.168.251.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2020-02-15 19:53:29,627 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  um2/192.168.251.4 to um1:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:617)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:715)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:889)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 8 more
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2020-02-23 17:14:08,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-02-23 17:14:09,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-02-23 17:14:10,386 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-23 17:14:11,647 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-02-23 17:14:11,911 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-02-23 17:14:11,912 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-02-23 17:14:11,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-02-23 17:14:11,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-02-23 17:14:12,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-02-23 17:14:12,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-02-23 17:14:12,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-02-23 17:14:12,381 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-02-23 17:14:12,394 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-02-23 17:14:12,412 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-02-23 17:14:12,418 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-02-23 17:14:12,418 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-02-23 17:14:12,418 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-02-23 17:14:12,462 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-02-23 17:14:12,467 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-02-23 17:14:12,467 INFO org.mortbay.log: jetty-6.1.26
2020-02-23 17:14:13,356 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-02-23 17:14:14,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-02-23 17:14:14,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-02-23 17:14:15,051 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-02-23 17:14:15,207 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-02-23 17:14:15,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-02-23 17:14:15,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-02-23 17:14:15,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-02-23 17:14:15,503 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-23 17:14:15,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-02-23 17:14:15,542 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-02-23 17:14:15,556 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-02-23 17:14:16,342 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 3042@um2
2020-02-23 17:14:16,476 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1217694175-192.168.251.3-1580661368826
2020-02-23 17:14:16,476 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826
2020-02-23 17:14:16,495 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-02-23 17:14:16,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1764777453;bpid=BP-1217694175-192.168.251.3-1580661368826;lv=-56;nsInfo=lv=-60;cid=CID-fa1ec920-c1b6-432b-9ffe-7f9053fe4aea;nsid=1764777453;c=0;bpid=BP-1217694175-192.168.251.3-1580661368826;dnuuid=ff408c34-7225-4a3e-8e46-7f52a838f4f6
2020-02-23 17:14:16,533 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-02-23 17:14:16,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-02-23 17:14:16,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-02-23 17:14:16,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-02-23 17:14:16,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-23 17:14:16,689 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-02-23 17:14:16,739 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1217694175-192.168.251.3-1580661368826 on /orgz/data2/current: 44ms
2020-02-23 17:14:16,739 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1217694175-192.168.251.3-1580661368826: 57ms
2020-02-23 17:14:16,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-02-23 17:14:16,760 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current: 15ms
2020-02-23 17:14:16,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 21ms
2020-02-23 17:14:16,765 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1582483572765 with interval 21600000
2020-02-23 17:14:16,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-02-23 17:14:16,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-02-23 17:14:16,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-02-23 17:14:16,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=578
2020-02-23 17:14:16,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000
2020-02-23 17:14:16,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3770bf32be,  containing 1 storage report(s), of which we sent 1. The reports had 30 total blocks and used 1 RPC(s). This took 1 msec to generate and 61 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-02-23 17:14:16,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-23 17:14:16,993 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-02-23 17:14:16,993 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-02-23 17:14:16,996 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-02-23 17:14:16,996 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-02-23 17:14:16,996 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1217694175-192.168.251.3-1580661368826
2020-02-23 17:14:17,014 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1217694175-192.168.251.3-1580661368826 to blockPoolScannerMap, new size=1
2020-02-23 17:14:21,844 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741864_1040
2020-03-04 06:18:28,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-03-04 06:18:28,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-03-04 06:18:28,736 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-03-04 06:18:29,347 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-03-04 06:18:29,561 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-03-04 06:18:29,561 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-03-04 06:18:29,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-03-04 06:18:29,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-03-04 06:18:29,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-03-04 06:18:29,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-03-04 06:18:29,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-03-04 06:18:30,719 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-03-04 06:18:30,726 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-03-04 06:18:30,743 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-03-04 06:18:30,748 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-03-04 06:18:30,748 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-03-04 06:18:30,748 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-03-04 06:18:30,776 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-03-04 06:18:30,781 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-03-04 06:18:30,781 INFO org.mortbay.log: jetty-6.1.26
2020-03-04 06:18:31,413 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-03-04 06:18:32,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-03-04 06:18:32,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-03-04 06:18:32,615 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-03-04 06:18:32,642 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-03-04 06:18:32,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-03-04 06:18:32,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-03-04 06:18:32,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-03-04 06:18:32,833 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-03-04 06:18:32,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-03-04 06:18:32,908 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-03-04 06:18:32,908 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-03-04 06:18:33,460 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 3966@um2
2020-03-04 06:18:33,599 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1217694175-192.168.251.3-1580661368826
2020-03-04 06:18:33,599 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826
2020-03-04 06:18:33,600 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-03-04 06:18:33,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1764777453;bpid=BP-1217694175-192.168.251.3-1580661368826;lv=-56;nsInfo=lv=-60;cid=CID-fa1ec920-c1b6-432b-9ffe-7f9053fe4aea;nsid=1764777453;c=0;bpid=BP-1217694175-192.168.251.3-1580661368826;dnuuid=ff408c34-7225-4a3e-8e46-7f52a838f4f6
2020-03-04 06:18:33,626 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-03-04 06:18:33,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-03-04 06:18:33,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-03-04 06:18:33,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-03-04 06:18:33,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1217694175-192.168.251.3-1580661368826
2020-03-04 06:18:33,779 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-03-04 06:18:33,830 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1217694175-192.168.251.3-1580661368826 on /orgz/data2/current: 48ms
2020-03-04 06:18:33,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1217694175-192.168.251.3-1580661368826: 60ms
2020-03-04 06:18:33,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-03-04 06:18:33,859 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current: 14ms
2020-03-04 06:18:33,861 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 23ms
2020-03-04 06:18:33,867 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1583305471867 with interval 21600000
2020-03-04 06:18:33,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-03-04 06:18:33,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-03-04 06:18:33,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-03-04 06:18:34,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=607
2020-03-04 06:18:34,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000
2020-03-04 06:18:34,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6f016ab1e9,  containing 1 storage report(s), of which we sent 1. The reports had 30 total blocks and used 1 RPC(s). This took 1 msec to generate and 45 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-03-04 06:18:34,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-03-04 06:18:34,104 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-03-04 06:18:34,104 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-03-04 06:18:34,107 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-03-04 06:18:34,107 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-03-04 06:18:34,107 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1217694175-192.168.251.3-1580661368826
2020-03-04 06:18:34,125 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1217694175-192.168.251.3-1580661368826 to blockPoolScannerMap, new size=1
2020-03-04 08:04:31,892 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1217694175-192.168.251.3-1580661368826 Total blocks: 30, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2020-03-04 08:21:08,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741886_1062 src: /192.168.251.3:46460 dest: /192.168.251.4:50010
2020-03-04 08:21:08,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:46460, dest: /192.168.251.4:50010, bytes: 5812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-159326489_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741886_1062, duration: 98322600
2020-03-04 08:21:08,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741886_1062, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-03-04 08:23:12,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741887_1063 src: /192.168.251.3:46470 dest: /192.168.251.4:50010
2020-03-04 08:23:12,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.251.3:46470, dest: /192.168.251.4:50010, bytes: 66, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_964200271_1, offset: 0, srvID: ff408c34-7225-4a3e-8e46-7f52a838f4f6, blockid: BP-1217694175-192.168.251.3-1580661368826:blk_1073741887_1063, duration: 70828817
2020-03-04 08:23:12,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1217694175-192.168.251.3-1580661368826:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2020-03-04 08:30:21,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7a023f2378a,  containing 1 storage report(s), of which we sent 1. The reports had 32 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-03-04 08:30:21,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-06-21 16:15:12,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = um2/192.168.251.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.5
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z
STARTUP_MSG:   java = 1.8.0_221
************************************************************/
2020-06-21 16:15:12,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-06-21 16:15:14,542 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-06-21 16:15:15,970 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-06-21 16:15:16,360 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-06-21 16:15:16,361 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2020-06-21 16:15:16,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is um2
2020-06-21 16:15:16,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2020-06-21 16:15:16,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2020-06-21 16:15:16,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2020-06-21 16:15:16,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2020-06-21 16:15:16,947 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-06-21 16:15:16,968 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2020-06-21 16:15:17,003 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-06-21 16:15:17,038 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-06-21 16:15:17,038 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-06-21 16:15:17,038 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-06-21 16:15:17,067 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-06-21 16:15:17,081 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2020-06-21 16:15:17,082 INFO org.mortbay.log: jetty-6.1.26
2020-06-21 16:15:18,084 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2020-06-21 16:15:19,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdu
2020-06-21 16:15:19,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2020-06-21 16:15:20,426 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-06-21 16:15:20,582 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2020-06-21 16:15:20,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2020-06-21 16:15:20,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2020-06-21 16:15:20,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2020-06-21 16:15:21,180 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-06-21 16:15:21,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to um1/192.168.251.3:9000 starting to offer service
2020-06-21 16:15:21,233 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2020-06-21 16:15:21,275 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-06-21 16:15:23,796 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /orgz/data2/in_use.lock acquired by nodename 3075@um2
2020-06-21 16:15:24,317 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1217694175-192.168.251.3-1580661368826
2020-06-21 16:15:24,318 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /orgz/data2/current/BP-1217694175-192.168.251.3-1580661368826
2020-06-21 16:15:24,319 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2020-06-21 16:15:24,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1764777453;bpid=BP-1217694175-192.168.251.3-1580661368826;lv=-56;nsInfo=lv=-60;cid=CID-fa1ec920-c1b6-432b-9ffe-7f9053fe4aea;nsid=1764777453;c=0;bpid=BP-1217694175-192.168.251.3-1580661368826;dnuuid=ff408c34-7225-4a3e-8e46-7f52a838f4f6
2020-06-21 16:15:24,398 WARN org.apache.hadoop.hdfs.server.common.Util: Path /orgz/data2 should be specified as a URI in configuration files. Please update hdfs configuration.
2020-06-21 16:15:24,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /orgz/data2/current
2020-06-21 16:15:24,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /orgz/data2/current, StorageType: DISK
2020-06-21 16:15:24,914 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2020-06-21 16:15:24,915 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1217694175-192.168.251.3-1580661368826
2020-06-21 16:15:24,925 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-06-21 16:15:25,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1217694175-192.168.251.3-1580661368826 on /orgz/data2/current: 124ms
2020-06-21 16:15:25,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1217694175-192.168.251.3-1580661368826: 141ms
2020-06-21 16:15:25,059 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current...
2020-06-21 16:15:25,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1217694175-192.168.251.3-1580661368826 on volume /orgz/data2/current: 60ms
2020-06-21 16:15:25,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 110ms
2020-06-21 16:15:25,211 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1592762309211 with interval 21600000
2020-06-21 16:15:25,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 beginning handshake with NN
2020-06-21 16:15:25,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid null) service to um1/192.168.251.3:9000 successfully registered with NN
2020-06-21 16:15:25,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode um1/192.168.251.3:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-06-21 16:15:25,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000 trying to claim ACTIVE state with txid=675
2020-06-21 16:15:25,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1217694175-192.168.251.3-1580661368826 (Datanode Uuid ff408c34-7225-4a3e-8e46-7f52a838f4f6) service to um1/192.168.251.3:9000
2020-06-21 16:15:25,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x90028088c2,  containing 1 storage report(s), of which we sent 1. The reports had 32 total blocks and used 1 RPC(s). This took 3 msec to generate and 88 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-06-21 16:15:25,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1217694175-192.168.251.3-1580661368826
2020-06-21 16:15:25,764 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2020-06-21 16:15:25,765 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-06-21 16:15:25,775 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2020-06-21 16:15:25,775 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2020-06-21 16:15:25,775 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1217694175-192.168.251.3-1580661368826
2020-06-21 16:15:25,807 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1217694175-192.168.251.3-1580661368826 to blockPoolScannerMap, new size=1
2020-06-21 16:15:30,344 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1217694175-192.168.251.3-1580661368826:blk_1073741879_1055
2020-06-21 16:15:57,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741889_1065 src: /192.168.251.3:55890 dest: /192.168.251.4:50010
2020-06-21 16:15:57,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741888_1064 src: /192.168.251.3:55888 dest: /192.168.251.4:50010
2020-06-21 16:15:57,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1217694175-192.168.251.3-1580661368826:blk_1073741889_1065 src: /192.168.251.3:55890 dest: /192.168.251.4:50010 of size 14978
2020-06-21 16:15:57,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1217694175-192.168.251.3-1580661368826:blk_1073741888_1064 src: /192.168.251.3:55888 dest: /192.168.251.4:50010 of size 84853
2020-06-21 16:16:00,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1217694175-192.168.251.3-1580661368826:blk_1073741890_1066 src: /192.168.251.3:55892 dest: /192.168.251.4:50010
2020-06-21 16:16:00,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1217694175-192.168.251.3-1580661368826:blk_1073741890_1066 src: /192.168.251.3:55892 dest: /192.168.251.4:50010 of size 1366
